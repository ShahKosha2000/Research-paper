TY  - JOUR
AU  - Zammit, M.
AU  - Voulgari, I.
AU  - Liapis, A.
AU  - Yannakakis, G.N.
TI  - Learn to Machine Learn via Games in the Classroom
PY  - 2022
T2  - Frontiers in Education
VL  - 7
C7  - 913530
DO  - 10.3389/feduc.2022.913530
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133513817&doi=10.3389%2ffeduc.2022.913530&partnerID=40&md5=e3d737573b6b7e74fd438037689b8cd4
AD  - Institute of Digital Games, University of Malta, Msida, Malta
AB  - Artificial Intelligence (AI) and Machine Learning (ML) algorithms are increasingly being adopted to create and filter online digital content viewed by audiences from diverse demographics. From an early age, children grow into habitual use of online services but are usually unaware of how such algorithms operate, or even of their presence. Design decisions and biases inherent in the ML algorithms or in the datasets they are trained on shape the everyday digital lives of present and future generations. It is therefore important to disseminate a general understanding of AI and ML, and the ethical concerns associated with their use. As a response, the digital game ArtBot was designed and developed to teach fundamental principles about AI and ML, and to promote critical thinking about their functionality and shortcomings in everyday digital life. The game is intended as a learning tool in primary and secondary school classrooms. To assess the effectiveness of the ArtBot game as a learning experience we collected data from over 2,000 players across different platforms focusing on the degree of usage, interface efficiency, learners' performance and user experience. The quantitative usage data collected within the game was complemented by over 160 survey responses from teachers and students during early pilots of ArtBot. The evaluation analysis performed in this paper gauges the usability and usefulness of the game, and identifies areas of the game design which need improvement. Copyright © 2022 Zammit, Voulgari, Liapis and Yannakakis.
KW  - artificial intelligence
KW  - digital literacy
KW  - educational games
KW  - game analytics
KW  - machine learning
KW  - reinforcement learning
KW  - serious games
KW  - supervised learning
PB  - Frontiers Media S.A.
SN  - 2504284X (ISSN)
LA  - English
J2  - Front. Educ.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: M. Zammit; Institute of Digital Games, University of Malta, Msida, Malta; email: marvin.zammit@um.edu.mt
ER  -

TY  - JOUR
AU  - KhalafAnsara, H.M.
AU  - Keighobadi, J.
TI  - Deep Reinforcement Learning with Immersion- and Invariance-based State Observer Control of Wave Energy Converters
PY  - 2024
T2  - International Journal of Engineering, Transactions B: Applications
VL  - 37
IS  - 6
SP  - 1085
EP  - 1097
DO  - 10.5829/ije.2024.37.06c.05
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188230678&doi=10.5829%2fije.2024.37.06c.05&partnerID=40&md5=da17231f25b89e22fc259afec94d4d1a
AD  - Faculty of Mechanical Engineering, University of Tabriz, East Azerbaijan, Iran
AB  - Composable life under the extensive global warming of the Earth encourages the progress of renewable energy devices and the adoption of new technologies, such as artificial intelligence. Regarding enormous potential of wave energy and its consistency, wave energy converter (WEC) plays vital role in uniform energy harvesting field. In this paper, the significant environmental changes in the ocean prompt us to propose an intelligent feedback control system to mitigate the impact of disturbances and variable wind effects on the efficacy of WECs. Deep reinforcement learning (DRL), as a powerful machine intelligence technique, is capable of identifying WECs as black-box models. Therefore, based on the DRL model, the disturbance and unmeasured state variables are simultaneously estimated in the extended state observer section. Leakage in identification data and real-time application requirements of limited number of layers in the deep neural networks are compensated by implementation of immersion and invariance-based extended state observer which improves coping with the unwanted exogenous noises as well. In the overall intelligent control system, the estimated parameters are inputted into the DRL as the actor-critic networks. The initial actor network is responsible for predicting the control action, while the subsequent critic network determines the decision criterion for evaluating the accuracy of the actor's estimated amount. Next, the output value of the critic stage is backpropagated through the layers to update the network weights. The simulation test results in MATLAB indicate the convergence of unmeasured parameters/states to the corresponding true values and the significance of newly designed intelligent DRL method. © 2024 The author(s). This is an open access article distributed under the terms of the Creative Commons Attribution (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, as long as the original authors and source are cited. No permission is required from the authors or the publishers.
KW  - Deep Reinforcement Learning
KW  - Extended State Observer
KW  - Immersion- and Invariance-based Control
KW  - Uniform Energy
KW  - Wave Energy Converter
KW  - Adaptive control systems
KW  - Deep neural networks
KW  - Energy harvesting
KW  - Feedback control
KW  - Global warming
KW  - Learning systems
KW  - Multilayer neural networks
KW  - State estimation
KW  - Wave energy conversion
KW  - Wind power
KW  - Critic network
KW  - Deep reinforcement learning
KW  - Energy
KW  - Extended state observer
KW  - Immersion and invariances
KW  - Immersion- and invariance-based control
KW  - Reinforcement learnings
KW  - State observers controls
KW  - Uniform energy
KW  - Wave energy converters
KW  - Reinforcement learning
PB  - Materials and Energy Research Center
SN  - 1728144X (ISSN)
LA  - English
J2  - Int. J. Eng. Trans. B Applic.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: J. Keighobadi; Faculty of Mechanical Engineering, University of Tabriz, East Azerbaijan, Iran; email: keighobadi@tabrizu.ac.ir
ER  -

TY  - CONF
AU  - Ugurlu, H.I.
AU  - Bardakci, D.
AU  - Pham, H.X.
AU  - Kayacan, E.
TI  - Guidance of Agricultural Ground Robots Team with an Aerial Vehicle: A Cost-Effective Solution
PY  - 2023
T2  - Europe ISR 2023 - International Symposium on Robotics, Proceedings
SP  - 9
EP  - 15
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184349776&partnerID=40&md5=8b76fd7eb18cf7d6329c4c07675adc75
AD  - Robotics Laboratory (Air Lab), The Department of Electrical and Computer Engineering, Aarhus University, Aarhus C, 8000, Denmark
AD  - Automatic Control Group, Department of Electrical Engineering and Information Technology, Paderborn University, Paderborn, Germany
AB  - Increasing the operational efficiency of agricultural machines is essential by the use of artificial intelligence (AI)-based navigation, planning, and control algorithms to handle the increasing demand for food production without compromising sustainability. In this study, an end-to-end path planning algorithm (AgroRL) is proposed for aerial-ground robots team collaboration. In the proposed solution, while main operations in the field are handled by the ground vehicle, the aerial robot is responsible for re-planning a collision-free trajectory for the ground robot when the robot faces an obstacle. Deep reinforcement learning is used for training the end-to-end policy for local re-planning of the aerial robot. The agent, informed by the global trajectory, generates local plans based on depth images. Variational autoencoders are also investigated for dimension reduction of the depth images in obstacle avoidance context to speed up deep reinforcement learning and alleviate the computational complexity of the policy network. The agriculture environment is developed in the Webots open-source robot simulator for training and testing purposes. The efficiency and efficacy of the end-to-end planner are evaluated over a number of cluttered field scenarios. The simulation experiments demonstrate a single aerial vehicle guiding multiple ground robots in agricultural operations. © VDE VERLAG GMBH Berlin Offenbach.
KW  - Agricultural robots
KW  - Agriculture
KW  - Air navigation
KW  - Antennas
KW  - Cost effectiveness
KW  - Deep learning
KW  - Efficiency
KW  - Ground vehicles
KW  - Motion planning
KW  - Robot programming
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial robots
KW  - Aerial vehicle
KW  - Cost-effective solutions
KW  - Depth image
KW  - End to end
KW  - Ground robot
KW  - Operational efficiencies
KW  - Re-planning
KW  - Reinforcement learnings
KW  - Robot teams
KW  - Reinforcement learning
PB  - VDE VERLAG GMBH
SN  - 978-380076141-8 (ISBN)
LA  - English
J2  - Eur. ISR - Int. Symp. Robot., Proc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 56th International Symposium on Robotics, ISR Europe 2023; Conference date: 26 September 2023 through 27 September 2023; Conference code: 196265
ER  -

TY  - JOUR
AU  - Triguero, I.
AU  - Molina, D.
AU  - Poyatos, J.
AU  - Del Ser, J.
AU  - Herrera, F.
TI  - General Purpose Artificial Intelligence Systems (GPAIS): Properties, definition, taxonomy, societal implications and responsible governance
PY  - 2024
T2  - Information Fusion
VL  - 103
C7  - 102135
DO  - 10.1016/j.inffus.2023.102135
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176561699&doi=10.1016%2fj.inffus.2023.102135&partnerID=40&md5=cc3bb320cd8d34f2a33b32f6e9fa1413
AD  - Department of Computer Science and Artificial Intelligence, Andalusian Research Institute in Data Science and Computational Intelligence (DaSCI), University of Granada, Granada, 18071, Spain
AD  - TECNALIA, Basque Research & Technology Alliance (BRTA), Derio, 48160, Spain
AD  - University of the Basque Country (UPV/EHU), Bilbao, 48013, Spain
AD  - School of Computer Science, University of Nottingham, Nottingham, NG8 1BB, United Kingdom
AB  - Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research. This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and ability based on several factors such as adaptation to new tasks, competence in domains not intentionally trained for, ability to learn from few data, or proactive acknowledgement of their own limitations. We then propose a taxonomy of approaches to realise GPAIS, describing research trends such as the use of AI techniques to improve another AI (commonly referred to as AI-powered AI) or (single) foundation models. As a prime example, we delve into generative AI (GenAI), aligning them with the terms and concepts presented in the taxonomy. Similarly, we explore the challenges and prospects of multi-modality, which involves fusing various types of data sources to expand the capabilities of GPAIS. Through the proposed definition and taxonomy, our aim is to facilitate research collaboration across different areas that are tackling general purpose tasks, as they share many common aspects. Finally, with the goal of providing a holistic view of GPAIS, we discuss the current state of GPAIS, its prospects, implications for our society, and the need for regulation and governance of GPAIS to ensure their responsible and trustworthy development. © 2023
KW  - AutoML
KW  - Few-shot learning
KW  - General-purpose AI
KW  - Generative AI
KW  - Large language models
KW  - Meta-learning
KW  - Neuroevolution
KW  - Reinforcement learning
KW  - Transfer learning
KW  - Learning systems
KW  - Taxonomies
KW  - Automl
KW  - Few-shot learning
KW  - General-purpose artificial intelligence
KW  - Generative artificial intelligence
KW  - Language model
KW  - Large language model
KW  - Metalearning
KW  - Neuro evolutions
KW  - Reinforcement learnings
KW  - Transfer learning
KW  - Reinforcement learning
PB  - Elsevier B.V.
SN  - 15662535 (ISSN)
LA  - English
J2  - Inf. Fusion
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: I. Triguero; Department of Computer Science and Artificial Intelligence, Andalusian Research Institute in Data Science and Computational Intelligence (DaSCI), University of Granada, Granada, 18071, Spain; email: triguero@decsai.ugr.es
ER  -

TY  - JOUR
AU  - Romano, M.F.
AU  - Shih, L.C.
AU  - Paschalidis, I.C.
AU  - Au, R.
AU  - Kolachalama, V.B.
TI  - Large Language Models in Neurology Research and Future Practice
PY  - 2023
T2  - Neurology
VL  - 101
IS  - 23
SP  - 1058
EP  - 1067
DO  - 10.1212/WNL.0000000000207967
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178668953&doi=10.1212%2fWNL.0000000000207967&partnerID=40&md5=9d703cf41f346807004a4c0416c7391a
AD  - Department of Medicine, Boston University Chobanian Avedisian School of Medicine, MA, United States
AD  - Department of Radiology and Biomedical Imaging, University of California, San Francisco, United States
AD  - Department of Neurology, Boston University Chobanian Avedisian School of Medicine, United States
AD  - Department of Electrical and Computer Engineering, Division of Systems Engineering, and Department of Biomedical Engineering, United States
AD  - Faculty of Computing and Data Sciences, Boston University, United States
AD  - Department of Anatomy and Neurobiology, The Framingham Heart Study, Boston University Chobanian Avedisian School of Medicine, United States
AD  - Department of Epidemiology, Boston University School of Public Health, Boston University Alzheimer's Disease Research Center, United States
AD  - Department of Computer Science, Boston University, MA, United States
AB  - Recent advancements in generative artificial intelligence, particularly using large language models (LLMs), are gaining increased public attention. We provide a perspective on the potential of LLMs to analyze enormous amounts of data from medical records and gain insights on specific topics in neurology. In addition, we explore use cases for LLMs, such as early diagnosis, supporting patient and caregivers, and acting as an assistant for clinicians. We point to the potential ethical and technical challenges raised by LLMs, such as concerns about privacy and data security, potential biases in the data for model training, and the need for careful validation of results. Researchers must consider these challenges and take steps to address them to ensure that their work is conducted in a safe and responsible manner. Despite these challenges, LLMs offer promising opportunities for improving care and treatment of various neurologic disorders.  © American Academy of Neurology.
KW  - Artificial Intelligence
KW  - Humans
KW  - Language
KW  - Medical Records
KW  - Neurology
KW  - Research Personnel
KW  - amnesia
KW  - aphasia
KW  - Article
KW  - artificial intelligence
KW  - artificial neural network
KW  - attention
KW  - caregiver
KW  - cognition
KW  - cognitive rehabilitation
KW  - deep learning
KW  - dementia
KW  - demographics
KW  - depression
KW  - early diagnosis
KW  - electroencephalography
KW  - electromyography
KW  - electronic health record
KW  - fatigue
KW  - hospitalization
KW  - human
KW  - information security
KW  - language processing
KW  - large language model
KW  - machine learning
KW  - medical information
KW  - medical record
KW  - natural language processing
KW  - nerve conduction
KW  - neurologic disease
KW  - neurologic examination
KW  - neurology
KW  - neuropathology
KW  - neuropsychological assessment
KW  - psychological well-being
KW  - reinforcement learning (machine learning)
KW  - sentiment analysis
KW  - training
KW  - traumatic brain injury
KW  - validation process
KW  - artificial intelligence
KW  - language
KW  - personnel
PB  - Lippincott Williams and Wilkins
SN  - 00283878 (ISSN)
C2  - 37816646
LA  - English
J2  - Neurology
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 3; Correspondence Address: V.B. Kolachalama; Department of Medicine, Boston University Chobanian Avedisian School of Medicine, United States; email: vkola@bu.edu; CODEN: NEURA
ER  -

TY  - JOUR
AU  - Fard, N.E.
AU  - Selmic, R.R.
AU  - Khorasani, K.
TI  - A Review of Techniques and Policies on Cybersecurity Using Artificial Intelligence and Reinforcement Learning Algorithms
PY  - 2023
T2  - IEEE Technology and Society Magazine
VL  - 42
IS  - 3
SP  - 57
EP  - 68
DO  - 10.1109/MTS.2023.3306540
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174392719&doi=10.1109%2fMTS.2023.3306540&partnerID=40&md5=56e207559e027a2cadc9ae63d27c9cab
AD  - Concordia University, Department of Electrical and Computer Engineering, Montreal, H3G 1M8, QC, Canada
AB  - Cybersecurity is a critical process that safeguards networks, systems, and applications against cyber-attacks, wherein digital information is targeted for unauthorized access, manipulation, or destruction. As attackers continually evolve their tactics, addressing cybersecurity challenges has become paramount, especially in sensitive domains like the military and defense industries. This article delves into the challenges that artificial intelligence (AI) faces in the military domain, specifically focusing on defense applications. We review AI algorithms relevant to defense, examining their potential applications and benefits: much of this study revolves around cybersecurity in defense applications, particularly within cyber-physical systems (CPS). We explore reinforcement learning (RL) and deep RL (DRL) algorithms in CPS, aiming to enhance understanding of the cybersecurity implications in this domain. In this context, we present RL and DRL algorithms employed in cyber-attacks and their potential threats and vulnerabilities. Furthermore, we discuss how RL and DRL algorithms can be effectively leveraged for cyber-attack detection and defense applications, providing usable insights into bolstering CPS cybersecurity. By addressing both technical aspects and ethical considerations, this article offers a comprehensive view of the challenges and opportunities surrounding cybersecurity in defense applications.  © 1982-2012 IEEE.
KW  - Computer crime
KW  - Crime
KW  - Cyber attacks
KW  - Cyber Physical System
KW  - Deep learning
KW  - Embedded systems
KW  - Learning algorithms
KW  - Military applications
KW  - Network security
KW  - Artificial intelligence learning
KW  - Cybe-physical systems
KW  - Cyber security
KW  - Cyber-attacks
KW  - Cyber-physical systems
KW  - Defence applications
KW  - Network applications
KW  - Network systems
KW  - Reinforcement learning algorithms
KW  - Reinforcement learnings
KW  - Reinforcement learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 02780097 (ISSN)
LA  - English
J2  - IEEE Technol Soc Mag
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: R.R. Selmic; Concordia University, Department of Electrical and Computer Engineering, Montreal, H3G 1M8, Canada; email: rastko.selmic@concordia.ca; CODEN: ITSMD
ER  -

TY  - JOUR
AU  - Ahmad, M.F.
TI  - Public opinion and persuasion of algorithmic fairness: assessment of communication protocol performance for use in simulation-based reinforcement learning training
PY  - 2024
T2  - International Journal of Information Technology (Singapore)
VL  - 16
IS  - 2
SP  - 687
EP  - 696
DO  - 10.1007/s41870-023-01507-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174156847&doi=10.1007%2fs41870-023-01507-0&partnerID=40&md5=2cb46b3e50a96cee482a968afe8b163c
AD  - Faculty of Applied Social Sciences, Universiti Sultan Zainal Abidin, Terengganu, Kuala Nerus, 21300, Malaysia
AB  - As the popularity of AI continues to grow, the techniques used to train AI systems have become increasingly intriguing. Reinforcement learning (RL) in simulation environments provides a safe and effective training approach for AI. However, the performance of the communication protocol between the reinforcement learning system and the simulation environment significantly impacts the overall system performance. This research aims to investigate the impact of communication protocols on algorithmic fairness by exploring public opinion and persuasion. The test software uses reinforcement learning data of floating-point numbers and images to assess communication methods, including Sockets, Socket.IO, gRPC, and ZeroMQ. Results indicate that Sockets and ZeroMQ exhibit similar performance in transferring floats, while ZeroMQ outperforms Sockets when it comes to transmitting images. Sockets are preferred when dealing with larger datasets. ZeroMQ stands out due to its speed and user-friendly simplicity, making it a suitable choice for reinforcement learning in simulation, employing Unreal Engine, AGX Dynamics, and Stable Baselines3. In the conducted experiments, reinforcement learning is found to be the most time-consuming component, followed by simulation, with communication constituting half of the total time. As the complexity of the system increases, the time spent on reinforcement learning and simulation is expected to grow faster than that of communication, emphasizing the need to optimize other components. © The Author(s), under exclusive licence to Bharati Vidyapeeth's Institute of Computer Applications and Management 2023. Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.
KW  - AI ethics
KW  - Algorithmic decision-making
KW  - Algorithms
KW  - Artificial intelligence (AI)
KW  - Fairness
KW  - Machine learning (ML)
KW  - Socio-technical design
PB  - Springer Science and Business Media B.V.
SN  - 25112104 (ISSN)
LA  - English
J2  - Int. J. Inf. Technol.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: M.F. Ahmad; Faculty of Applied Social Sciences, Universiti Sultan Zainal Abidin, Kuala Nerus, Terengganu, 21300, Malaysia; email: mfazilahmad@unisza.edu.my
ER  -

TY  - CONF
AU  - Tassella, M.
AU  - Chaput, R.
AU  - Guillermin, M.
TI  - Artificial Moral Advisors: enhancing human ethical decision-making
PY  - 2023
T2  - 2023 IEEE International Symposium on Ethics in Engineering, Science, and Technology: Ethics in the Global Innovation Helix, ETHICS 2023
DO  - 10.1109/ETHICS57328.2023.10155026
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164813927&doi=10.1109%2fETHICS57328.2023.10155026&partnerID=40&md5=5355446cb65c2d0ebaae6214241f8814
AD  - Lumsa University, Lyon Catholic University, Rome, Italy
AD  - Lyon, France
AD  - Lyon Catholic University, CONFLUENCE: Sciences et Humanités Research Unit (EA 1598), Lyon, France
AB  - This short paper focuses on understanding moral dilemmas, Artificial Moral Advisors, and their possible roles in ethical decision-making. After a brief analysis of the philosophical debate around dilemmas, we propose three different classes of dilemmas. We then discuss how AI-based advisors could be used to enhance human ethical decision-making, with a particular focus on three possible AI skills (identifying, presenting and settling dilemmas), as well as on their role as ethical experts. The resulting proposal opens up to new possible uses of AI moral advisors, and to the help they might offer in difficult decisions.  © 2023 IEEE.
KW  - AI Moral Enhancement
KW  - Artificial Moral Advisors
KW  - Moral Dilemmas
KW  - Reinforcement Learning
KW  - Decision making
KW  - Ethical technology
KW  - AI moral enhancement
KW  - Artificial moral advisor
KW  - Different class
KW  - Ethical decision making
KW  - Moral dilemma
KW  - Reinforcement learnings
KW  - Reinforcement learning
A2  - Cheong M.
A2  - Herkert J.
A2  - Hess J.
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-166545713-2 (ISBN)
LA  - English
J2  - IEEE Int. Symp. Ethics Eng., Sci., Technol.: Ethics Glob. Innov. Helix, ETHICS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Conference name: 2023 IEEE International Symposium on Ethics in Engineering, Science, and Technology, ETHICS 2023; Conference date: 18 May 2023 through 20 May 2023; Conference code: 189902
ER  -

TY  - CONF
AU  - Unni, M.V.
AU  - Rudresh, S.
AU  - Bh, R.
AU  - Krishnan, R.K.
AU  - Kar, R.
AU  - Devichandrika, S.
TI  - Automation using Artificial Intelligence in Business Landscape
PY  - 2023
T2  - 2nd International Conference on Automation, Computing and Renewable Systems, ICACRS 2023 - Proceedings
SP  - 1519
EP  - 1523
DO  - 10.1109/ICACRS58579.2023.10404252
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185384220&doi=10.1109%2fICACRS58579.2023.10404252&partnerID=40&md5=fc3a0fcb3937cc35220fd3b9ba8145e9
AD  - Department of Management, St. Claret College, Karnataka, Bengaluru, India
AD  - School of Business and Management, CHRIST (Deemed to be University), Karnataka, Bengaluru, India
AD  - College of Economics and Business Administration, University of Technology and Applied Sciences, Shinas, Oman
AD  - Department of Business Administration, St. Francis De Sales College, Karnataka, Bangalore, India
AB  - The integration of Artificial Intelligence (AI) with automation has sparked a remarkable transformation in the contemporary business landscape, promising elevated efficiency and quality. However, this convergence encounters multifaceted challenges, notably in the adoption of recent AI techniques such as deep learning, reinforcement learning, and natural language processing. These techniques, while potent, grapple with challenges in data quality, interpretability, and ethical considerations. In this study, we aim to delineate the intricate interplay between AI and automation, illuminating their collective potential to augment operational efficiency and confer a competitive advantage. Through a comprehensive review, we will explore the effective integration of these technologies, navigating hurdles such as data bias, system compatibility, and human-machine collaboration. Here, the primary research objective is to provide insights on optimizing the outcomes by synergizing AI and automation while addressing the inherent challenges, ultimately fostering sustainable and impactful implementations in organizational frameworks. © 2023 IEEE.
KW  - Artificial Intelligence
KW  - Automation
KW  - Companies
KW  - Machine learning
KW  - Management
KW  - Technologies
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835034023-5 (ISBN)
LA  - English
J2  - Int. Conf. Autom., Comput. Renew. Syst., ICACRS - Proc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 2nd International Conference on Automation, Computing and Renewable Systems, ICACRS 2023; Conference date: 11 December 2023 through 12 December 2023; Conference code: 196917
ER  -

TY  - CONF
AU  - Wang, N.
AU  - Teng, Y.
AU  - Hu, G.
AU  - Yu, F.R.
TI  - Importance-Driven Data Collection for Efficient Online Learning Over the Wireless Edge
PY  - 2023
T2  - IEEE International Conference on Communications
VL  - 2023-May
SP  - 410
EP  - 415
DO  - 10.1109/ICC45041.2023.10278679
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178289869&doi=10.1109%2fICC45041.2023.10278679&partnerID=40&md5=dce2c8d5abe7c06908e7ad0d3401b998
AD  - Beijing University of Posts and Telecommunications (BUPT), Beijing Key Laboratory of Space-ground Interconnection and Convergence, Beijing, 100876, China
AD  - Carleton University, Department of Systems and Computer Engineering, Ottawa, K1S 5B6, ON, Canada
AB  - Online learning has been widely applied in real-time artificial intelligence (AI) applications to learn new classes from the dynamic environment. Although the deployment of AI model training over the edge can facilitate faster processing of real-time data, the learning efficiency is plagued by the limited capacity of distributed data acquisition. In fact, not all data samples are equally important, and the random data selection strategy is not beneficial to accelerate training due to redundant data processing. In this paper, we present an importance-driven data collection framework, which leverages the usefulness of important data to improve the learning efficiency over the wireless edge. Specifically, the novel model convergence metric (MCM) is constructed to evaluate the data importance dynamically for model learning. Moreover, considering the constraint of limited network resources on learning efficiency, we establish an MCM maximization problem of joint data collecting, scheduling, and feeding in an edge computing system. A two-timescale hierarchical reinforcement learning (TTHRL) algorithm is designed to decouple the original problem into two-timescale two-level subproblems, where the top-level agent is responsible for data feeding strategy in the long term and the low-level agent learns data scheduling and collecting strategy in the short term. Simulation results show that our proposed scheme can achieve better performance improvements over the baseline schemes.  © 2023 IEEE.
KW  - hierarchical reinforcement learning
KW  - online learning
KW  - queue stability
KW  - two-timescale stochastic optimization
KW  - Data acquisition
KW  - Data handling
KW  - E-learning
KW  - Learning systems
KW  - Optimization
KW  - Reinforcement learning
KW  - Data collection
KW  - Hierarchical reinforcement learning
KW  - Learn+
KW  - Learning efficiency
KW  - Model convergence
KW  - Online learning
KW  - Queue stability
KW  - Stochastic optimizations
KW  - Time-scales
KW  - Two-timescale stochastic optimization
KW  - Efficiency
A2  - Zorzi M.
A2  - Tao M.
A2  - Saad W.
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 15503607 (ISSN); 978-153867462-8 (ISBN)
LA  - English
J2  - IEEE Int Conf Commun
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 2023 IEEE International Conference on Communications, ICC 2023; Conference date: 28 May 2023 through 1 June 2023; Conference code: 193943
ER  -

TY  - CONF
AU  - Hawkins, W.
AU  - Mittelstadt, B.
TI  - The ethical ambiguity of AI data enrichment: Measuring gaps in research ethics norms and practices
PY  - 2023
T2  - ACM International Conference Proceeding Series
SP  - 261
EP  - 270
DO  - 10.1145/3593013.3593995
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163608191&doi=10.1145%2f3593013.3593995&partnerID=40&md5=adb8b7357e3f1b4ec94fbe59a7183beb
AD  - Oxford Internet Institute, University of Oxford, Oxford, United Kingdom
AB  - The technical progression of artificial intelligence (AI) research has been built on breakthroughs in fields such as computer science, statistics, and mathematics. However, in the past decade AI researchers have increasingly looked to the social sciences, turning to human interactions to solve the challenges of model development. Paying crowdsourcing workers to generate or curate data, or 'data enrichment', has become indispensable for many areas of AI research, from natural language processing to reinforcement learning from human feedback (RLHF). Other fields that routinely interact with crowdsourcing workers, such as Psychology, have developed common governance requirements and norms to ensure research is undertaken ethically. This study explores how, and to what extent, comparable research ethics requirements and norms have developed for AI research and data enrichment. We focus on the approach taken by two leading conferences: ICLR and NeurIPS, and journal publisher Springer. In a longitudinal study of accepted papers, and via a comparison with Psychology and CHI papers, this work finds that leading AI venues have begun to establish protocols for human data collection, but these are are inconsistently followed by authors. Whilst Psychology papers engaging with crowdsourcing workers frequently disclose ethics reviews, payment data, demographic data and other information, similar disclosures are far less common in leading AI venues despite similar guidance. The work concludes with hypotheses to explain these gaps in research ethics practices and considerations for its implications. © 2023 ACM.
KW  - artificial intelligence
KW  - data enrichment
KW  - research ethics
KW  - Natural language processing systems
KW  - Paper
KW  - Philosophical aspects
KW  - Reinforcement learning
KW  - Artificial intelligence research
KW  - Data enrichments
KW  - Humaninteraction
KW  - In-field
KW  - Language processing
KW  - Model development
KW  - Natural languages
KW  - Reinforcement learnings
KW  - Research ethics
KW  - Workers'
KW  - Crowdsourcing
PB  - Association for Computing Machinery
SN  - 978-145037252-7 (ISBN)
LA  - English
J2  - ACM Int. Conf. Proc. Ser.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 6th ACM Conference on Fairness, Accountability, and Transparency, FAccT 2023; Conference date: 12 June 2023 through 15 June 2023; Conference code: 189310
ER  -

TY  - JOUR
AU  - Nadizar, G.
AU  - Rovito, L.
AU  - De Lorenzo, A.
AU  - Medvet, E.
AU  - Virgolin, M.
TI  - An Analysis of the Ingredients for Learning Interpretable Symbolic Regression Models with Human-in-The-loop and Genetic Programming
PY  - 2024
T2  - ACM Transactions on Evolutionary Learning and Optimization
VL  - 4
IS  - 1
C7  - 5
DO  - 10.1145/3643688
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187802221&doi=10.1145%2f3643688&partnerID=40&md5=0bae125cc568ddd03928ccdc26bd391f
AD  - Department of Mathematics, Informatics and Geosciences, University of Trieste, Via Alfonso Valerio 12, Trieste, 34127, Italy
AD  - Department of Evolutionary Intelligence, Centrum Wiskunde and Informatica, Science Park 123, Amsterdam, XG 1098, Netherlands
AD  - Department of Engineering and Architecture, University of Trieste, Via Alfonso Valerio 6, Trieste, 34127, Italy
AB  - Interpretability is a critical aspect to ensure a fair and responsible use of machine learning (ML) in high-stakes applications. Genetic programming (GP) has been used to obtain interpretable ML models because it operates at the level of functional building blocks: if these building blocks are interpretable, there is a chance that their composition (i.e., the entire ML model) is also interpretable. However, the degree to which a model is interpretable depends on the observer. Motivated by this, we study a recently-introduced human-in-The-loop system that allows the user to steer GP's generation process to their preferences, which shall be online-learned by an artificial neural network (ANN). We focus on the generation of ML models as analytical functions (i.e., symbolic regression) as this is a key problem in interpretable ML, and propose a two-fold contribution. First, we devise more general representations for the ML models for the ANN to learn upon, to enable the application of the system to a wider range of problems. Second, we delve into a deeper analysis of the system's components. To this end, we propose an incremental experimental evaluation, aimed at (1) studying the effectiveness by which an ANN can capture the perceived interpretability for simulated users, (2) investigating how the GP's outcome is affected across different simulated user feedback profiles, and (3) determining whether humans participants would prefer models that were generated with or without their involvement. Our results pose clarity on pros and cons of using a human-in-The-loop approach to discover interpretable ML models with GP. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
KW  - active learning
KW  - deep learning
KW  - evolutionary algorithms
KW  - evolutionary computation
KW  - Explainable artificial intelligence
KW  - explainable evolutionary computation
KW  - genetic programming
KW  - interpretable machine learning
KW  - neural networks
KW  - Deep learning
KW  - Functional programming
KW  - Genetic algorithms
KW  - Learning systems
KW  - Neural networks
KW  - Regression analysis
KW  - Reinforcement learning
KW  - Active Learning
KW  - Deep learning
KW  - Explainable artificial intelligence
KW  - Explainable evolutionary computation
KW  - Human-in-the-loop
KW  - Interpretable machine learning
KW  - Machine learning models
KW  - Machine-learning
KW  - Neural-networks
KW  - Symbolic regression
KW  - Genetic programming
PB  - Association for Computing Machinery
SN  - 2688299X (ISSN)
LA  - English
J2  - ACM. Trans. Evol. Learn. Optim.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1
ER  -

TY  - CONF
AU  - Tennant, E.
AU  - Hailes, S.
AU  - Musolesi, M.
TI  - Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning
PY  - 2023
T2  - IJCAI International Joint Conference on Artificial Intelligence
VL  - 2023-August
SP  - 317
EP  - 325
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170401782&partnerID=40&md5=0a9d9af5e02e567a5eb94d3d4a977164
AD  - University College London, United Kingdom
AD  - University of Bologna, Italy
AB  - Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas. In this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm-based agents, between morality based on societal norms or internal virtues, and between single- and mixed-virtue (e.g., multi-objective) methodologies. Then, we evaluate our approach by modeling repeated dyadic interactions between learning moral agents in three iterated social dilemma games (Prisoner's Dilemma, Volunteer's Dilemma and Stag Hunt). We analyze the impact of different types of morality on the emergence of cooperation, defection or exploitation, and the corresponding social outcomes. Finally, we discuss the implications of these findings for the development of moral agents in artificial and mixed human-AI societies. © 2023 International Joint Conferences on Artificial Intelligence. All rights reserved.
KW  - Ethical technology
KW  - Learning systems
KW  - Multi agent systems
KW  - Reinforcement learning
KW  - Bottom up
KW  - Embeddings
KW  - Learning approach
KW  - Moral agents
KW  - Multi-agent reinforcement learning
KW  - Practical use
KW  - Real-world
KW  - Reinforcement learning agent
KW  - Social dilemmas
KW  - Topdown
KW  - Intelligent agents
A2  - Elkind E.
PB  - International Joint Conferences on Artificial Intelligence
SN  - 10450823 (ISSN); 978-195679203-4 (ISBN)
LA  - English
J2  - IJCAI Int. Joint Conf. Artif. Intell.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 32nd International Joint Conference on Artificial Intelligence, IJCAI 2023; Conference date: 19 August 2023 through 25 August 2023; Conference code: 191872
ER  -

TY  - JOUR
AU  - Kang, Y.
AU  - Gao, S.
AU  - Roth, R.E.
TI  - Artificial intelligence studies in cartography: a review and synthesis of methods, applications, and ethics
PY  - 2024
T2  - Cartography and Geographic Information Science
DO  - 10.1080/15230406.2023.2295943
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182467294&doi=10.1080%2f15230406.2023.2295943&partnerID=40&md5=09795fe67984010ca04c8054349b21ff
AD  - Department of Geography, University of Wisconsin, Madison, WI, United States
AD  - GISense Lab, Department of Geography, University of South Carolina, Columbia, SC, United States
AB  - The past decade has witnessed the rapid development of geospatial artificial intelligence (GeoAI) primarily due to the ground-breaking achievements in deep learning and machine learning. A growing number of scholars from cartography have demonstrated successfully that GeoAI can accelerate previously complex cartographic design tasks and even enable cartographic creativity in new ways. Despite the promise of GeoAI, researchers and practitioners have growing concerns about the ethical issues of GeoAI for cartography. In this paper, we conducted a systematic content analysis and narrative synthesis of research studies integrating GeoAI and cartography to summarize current research and development trends regarding the usage of GeoAI for cartographic design. Based on this review and synthesis, we first identify dimensions of GeoAI methods for cartography such as data sources, data formats, map evaluations, and six contemporary GeoAI models, each of which serves a variety of cartographic tasks. These models include decision trees, knowledge graph and semantic web technologies, deep convolutional neural networks, generative adversarial networks, graph neural networks, and reinforcement learning. Further, we summarize seven cartographic design applications where GeoAI have been effectively employed: generalization, symbolization, typography, map reading, map interpretation, map analysis, and map production. We also raise five potential ethical challenges that need to be addressed in the integration of GeoAI for cartography: commodification, responsibility, privacy, bias, and (together) transparency, explainability, and provenance. We conclude by identifying four potential research directions for future cartographic research with GeoAI: GeoAI-enabled active cartographic symbolism, human-in-the-loop GeoAI for cartography, GeoAI-based mapping-as-a-service, and generative GeoAI for cartography. © 2024 Cartography and Geographic Information Society.
KW  - Artificial intelligence
KW  - cartography
KW  - deep learning
KW  - ethics
KW  - GeoAI
KW  - maps
KW  - Computer aided instruction
KW  - Convolutional neural networks
KW  - Decision trees
KW  - Ethical technology
KW  - Generative adversarial networks
KW  - Learning systems
KW  - Mapping
KW  - Maps
KW  - Reinforcement learning
KW  - Breakings
KW  - Cartographic design
KW  - Content analysis
KW  - Deep learning
KW  - Design tasks
KW  - Ethical issues
KW  - Geo-spatial
KW  - Geospatial artificial intelligence
KW  - Machine-learning
KW  - Research studies
KW  - Deep neural networks
PB  - Taylor and Francis Ltd.
SN  - 15230406 (ISSN)
LA  - English
J2  - Cartogr. Geogr. Inf. Sci
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: S. Gao; Department of Geography, University of Wisconsin, Madison, United States; email: song.gao@wisc.edu; CODEN: CGSCF
ER  -

TY  - JOUR
AU  - Fehér, Á.
AU  - Aradi, S.
AU  - Bécsi, T.
TI  - Online Trajectory Planning with Reinforcement Learning for Pedestrian Avoidance
PY  - 2022
T2  - Electronics (Switzerland)
VL  - 11
IS  - 15
C7  - 2346
DO  - 10.3390/electronics11152346
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136804545&doi=10.3390%2felectronics11152346&partnerID=40&md5=143167cbb69463bd521a17a1718006ea
AD  - Department of Control for Transportation and Vehicle Systems, Faculty of Transportation Engineering and Vehicle Engineering, Budapest University of Technology and Economics, Műegyetem rkp. 3, Budapest, H-1111, Hungary
AB  - Planning the optimal trajectory of emergency avoidance maneuvers for highly automated vehicles is a complex task with many challenges. The algorithm needs to decrease accident risk by reducing the severity and keeping the car in a controllable state. Optimal trajectory generation considering all aspects of vehicle and environment dynamics is numerically complex, especially if the object to be avoided is moving. This paper presents a hierarchical method for the avoidance of moving objects in an autonomous vehicle, where a reinforcement learning agent is responsible for local planning, while longitudinal and lateral control is performed by the low-level model-predictive controller and Stanley controllers. In the developed architecture, the agent is responsible for the optimization. It is trained in various scenarios to provide the necessary parameters for a polynomial-based path and a velocity profile in a neural network output. The vehicle performs only the first step of the trajectory, which is redesigned repeatedly by the planner based on the new state. In the training phase, the vehicle executes the entire trajectory via low-level controllers to determine the reward value, which realizes a prediction for the future. The agent receives feedback and can further improve its performance. Finally, the proposed framework was tested in a simulation environment and was also compared to human drivers’ abilities. © 2022 by the authors.
KW  - advanced driver assistance systems
KW  - machine learning
KW  - motion planning
KW  - reinforcement learning
KW  - vehicle dynamics
PB  - MDPI
SN  - 20799292 (ISSN)
LA  - English
J2  - Electronics (Switzerland)
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5; Correspondence Address: S. Aradi; Department of Control for Transportation and Vehicle Systems, Faculty of Transportation Engineering and Vehicle Engineering, Budapest University of Technology and Economics, Budapest, Műegyetem rkp. 3, H-1111, Hungary; email: aradi.szilard@kjk.bme.hu
ER  -

TY  - CONF
AU  - Rawat, K.
AU  - Kapoor, C.
AU  - Goyal, H.R.
AU  - Sharma, S.
TI  - Artificial Intelligence Based Optimized Traffic Diversion System in Smart Cities
PY  - 2023
T2  - Communications in Computer and Information Science
VL  - 1921 CCIS
SP  - 97
EP  - 108
DO  - 10.1007/978-3-031-45124-9_8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175868851&doi=10.1007%2f978-3-031-45124-9_8&partnerID=40&md5=77fbf3cf15ac5c69c2e2d7072a201e7f
AD  - Department of Computer Science and Engineering, Graphic Era Deemed to be University, Dehradun, 248002, India
AB  - In smart cities, an AI-based optimized traffic diversion system uses artificial intelligence algorithms to enhance traffic flow and lessen congestion. This technique uses information from numerous sources, including GPS and traffic sensor data, to create a model of the traffic network. With the aim of minimizing trip time, fuel consumption, and emissions, algorithms like reinforcement learning and graph neural networks are used to decide the optimum routing decisions for vehicles depending on the current traffic conditions. Although this technology has the ability to completely transform the way we manage traffic, it is crucial to think about the moral ramifications and potential negative effects of employing AI to manage a vital infrastructure. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.
KW  - graph
KW  - neural network
KW  - Reinforcement
KW  - Sensor
KW  - Graph neural networks
KW  - Smart city
KW  - Traffic congestion
KW  - Artificial intelligence algorithms
KW  - Diversion systems
KW  - Graph
KW  - Neural-networks
KW  - Sensors data
KW  - System use
KW  - Traffic diversion
KW  - Traffic flow
KW  - Traffic networks
KW  - Traffic sensors
KW  - Reinforcement learning
A2  - Shaw R.N.
A2  - Ghosh A.
A2  - Paprzycki M.
PB  - Springer Science and Business Media Deutschland GmbH
SN  - 18650929 (ISSN); 978-303145123-2 (ISBN)
LA  - English
J2  - Commun. Comput. Info. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: S. Sharma; Department of Computer Science and Engineering, Graphic Era Deemed to be University, Dehradun, 248002, India; email: sachin.cse@geu.ac.in; Conference name: 2nd International Conference on Advanced Communication and Intelligent Systems, ICACIS 2023; Conference date: 16 June 2023 through 17 June 2023; Conference code: 302859
ER  -

TY  - JOUR
AU  - Siegel, J.
AU  - Pappas, G.
TI  - Morals, ethics, and the technology capabilities and limitations of automated and self-driving vehicles
PY  - 2023
T2  - AI and Society
VL  - 38
IS  - 1
SP  - 213
EP  - 226
DO  - 10.1007/s00146-021-01277-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114678793&doi=10.1007%2fs00146-021-01277-y&partnerID=40&md5=5205bab08a3b1e7c4e931ddcff761761
AD  - Department of Computer Science and Engineering, Michigan State University, 428 S. Shaw Lane, Room 3115, East Lansing, 48824, MI, United States
AD  - Department of Electrical and Computer Engineering, Michigan State University, 428 S. Shaw Lane, Room 2120, East Lansing, 48824, MI, United States
AD  - Department of Electrical and Computer Engineering, National Technical University of Athens, 9 Iroon Polytechniou str., Athens, Zografou, 15780, Greece
AB  - We motivate the desire for self-driving and explain its potential and limitations, and explore the need for—and potential implementation of—morals, ethics, and other value systems as complementary “capabilities” to the Deep Technologies behind self-driving. We consider how the incorporation of such systems may drive or slow adoption of high automation within vehicles. First, we explore the role for morals, ethics, and other value systems in self-driving through a representative hypothetical dilemma faced by a self-driving car. Through the lens of engineering, we explain in simple terms common moral and ethical frameworks including utilitarianism, deontology, and virtue ethics before characterizing their relationship to the fundamental algorithms enabling self-driving. The concepts of behavior cloning, state-based modeling, and reinforcement learning are introduced, with some algorithms being more suitable for the implementation of value systems than others. We touch upon the contemporary cross-disciplinary landscape of morals and ethics in self-driving systems from a joint philosophical and technical perspective, and close with considerations for practitioners and the public, particularly as individuals may not appreciate the nuance and complexity of using imperfect information to navigate diverse scenarios and tough-to-quantify value systems, while “typical” software development reduces complex problems to black and white decision-making. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.
KW  - Artificial intelligence
KW  - Automated vehicles
KW  - Autonomous vehicles
KW  - Ethics
KW  - Morals
KW  - Self-driving
KW  - Value systems
KW  - Decision making
KW  - Ethical technology
KW  - Reinforcement learning
KW  - Software design
KW  - Automated driving
KW  - Automated vehicles
KW  - Autonomous Vehicles
KW  - High automation
KW  - Moral
KW  - Self drivings
KW  - Simple++
KW  - Through the lens
KW  - Value systems
KW  - Virtue ethics
KW  - Autonomous vehicles
PB  - Springer Science and Business Media Deutschland GmbH
SN  - 09515666 (ISSN)
LA  - English
J2  - AI Soc.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 6; Correspondence Address: J. Siegel; Department of Computer Science and Engineering, Michigan State University, East Lansing, 428 S. Shaw Lane, Room 3115, 48824, United States; email: jsiegel@msu.edu
ER  -

TY  - CONF
AU  - Costa, A.R.
AU  - Nogalha De Lima, R.H.
AU  - Ralha, C.G.
TI  - Machine Learning Approaches for Community Detection in Online Social Networks
PY  - 2023
T2  - 2023 IEEE Symposium Series on Computational Intelligence, SSCI 2023
SP  - 7
EP  - 12
DO  - 10.1109/SSCI52147.2023.10371982
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182942358&doi=10.1109%2fSSCI52147.2023.10371982&partnerID=40&md5=19b3ffb78a57e9afe0a7eba9555e1dd6
AD  - University of Brasília (UnB), Campus Darcy Ribeiro, Brasília, Brazil
AB  - Network analysis is responsible for taking insights or generating predictions from networked data sources where community detection finds chunks of related data in a network. The importance of community detection spans in different domain applications, from social network formation to protein interaction predictions. This work compares five state-of-the-art solutions to community detection using machine learning approaches in the context of online social networks - Graph-GAN, SDNE, ComE, AC2CD, and CLARE. The experiments using real-world online social network datasets (Email-EU-Core, BlogCatalog3, Flickr) with micro-F1, macro-F1, and NMI scores demonstrate that graph neural networks and deep reinforcement learning approaches are better suited for the community detection task than others based on probabilistic or shallow networks. © 2023 IEEE.
KW  - Deep Reinforcement Learning
KW  - Graph Neural Network
KW  - Network analysis
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-166543065-4 (ISBN)
LA  - English
J2  - IEEE Symp. Ser. Comput. Intell., SSCI
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 2023 IEEE Symposium Series on Computational Intelligence, SSCI 2023; Conference date: 5 December 2023 through 8 December 2023; Conference code: 196107
ER  -

TY  - CONF
AU  - Gao, J.
AU  - Zhang, J.
AU  - Xu, X.
AU  - Qi, L.
AU  - Yuan, Y.
AU  - Li, Z.
AU  - Dou, W.
TI  - Energy-Efficient Task Offloading in UAV-Enabled MEC via Multi-agent Reinforcement Learning
PY  - 2024
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 14504
SP  - 63
EP  - 80
DO  - 10.1007/978-981-99-9896-8_5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184298639&doi=10.1007%2f978-981-99-9896-8_5&partnerID=40&md5=377d7d4929c4e0570583778161fd7046
AD  - School of Software, Nanjing University of Information Science and Technology, Nanjing, 210044, China
AD  - State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China
AD  - College of Computer Science and Technology, China University of Petroleum (East China), QingDao, 266580, China
AD  - School of Computer Science and Engineering, Beihang University, Beijing, 100191, China
AD  - State Key Laboratory of Software Development Environment Zhongguancun Laboratory, Beihang University, Beijing, 100191, China
AD  - School of Computer Science, Nanjing University of Information Science and Technology, Nanjing, 210044, China
AB  - Nowadays, artificial intelligence-based tasks are imposing increasing demands on computation resources and energy consumption. Unmanned aerial vehicles (UAVs) are widely utilized in mobile edge computing (MEC) due to maneuverability and integration of MEC servers, providing computation assistance to ground terminals (GTs). The task offloading process from GTs to UAVs in UAV-enabled MEC faces challenges such as workload imbalance among UAVs due to uneven GT distribution and conflicts arising from the increasing number of GTs and limited communication resources. Additionally, the dynamic nature of communication networks and workload needs to be considered. To address these challenges, this paper proposes a Multi-Agent Deep Deterministic Policy Gradient based distributed offloading method, named DMARL, treating each GT as an independent decision-maker responsible for determining task offloading strategies and transmission power. Furthermore, a UAV-enabled MEC with Non-Orthogonal Multiple Access architecture is introduced, incorporating task computation and transmission queue models. In addition, a differential reward function that considers both system-level rewards and individual rewards for each GT is designed. Simulation experiments conducted in three different scenarios demonstrate that the proposed method exhibits superior performance in balancing latency and energy consumption. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2024.
KW  - Mobile Edge Computing
KW  - multi-agent deep reinforcement learning
KW  - NOMA
KW  - unmanned aerial vehicles
KW  - Antennas
KW  - Computation offloading
KW  - Decision making
KW  - Deep learning
KW  - Energy efficiency
KW  - Energy utilization
KW  - Maneuverability
KW  - Mobile edge computing
KW  - Multi agent systems
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Energy efficient
KW  - Energy-consumption
KW  - Ground terminals
KW  - Multi agent
KW  - Multi-agent deep reinforcement learning
KW  - NOMA
KW  - Reinforcement learnings
KW  - Task offloading
KW  - Unmanned aerial vehicle
KW  - Reinforcement learning
A2  - Jin H.
A2  - Yu Z.
A2  - Yu C.
A2  - Zhou X.
A2  - Lu Z.
A2  - Song X.
PB  - Springer Science and Business Media Deutschland GmbH
SN  - 03029743 (ISSN); 978-981999895-1 (ISBN)
LA  - English
J2  - Lect. Notes Comput. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: X. Xu; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China; email: njuxlxu@gmail.com; Conference name: 18th International Conference on Green, Pervasive, and Cloud Computing, GPC 2023; Conference date: 22 September 2023 through 24 September 2023; Conference code: 306959
ER  -

TY  - CONF
AU  - Nagendiran, S.
AU  - Renugadevi, R.
AU  - Kumar, R.P.A.
AU  - Sasirekha, K.
AU  - Harini, R.
TI  - Secure Sensitive Information on IoT using Machine Learning
PY  - 2023
T2  - International Conference on Sustainable Communication Networks and Application, ICSCNA 2023 - Proceedings
SP  - 360
EP  - 366
DO  - 10.1109/ICSCNA58489.2023.10370157
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182998260&doi=10.1109%2fICSCNA58489.2023.10370157&partnerID=40&md5=e486b15de53889a21f95fd2b9ed563d2
AD  - R.M.K. Engineering College, India
AD  - R.M.K. Engineering College, Department of Computer Science and Engineering, India
AD  - St. Xavier's Catholic College of Engineering, Department of Computer Science and Engineering, India
AD  - R.M.D Engineering College, Computer Science and Business Systems, India
AD  - Veltech Multitech Dr.rangarajan Dr. Sakunthala Engineering College, Department of Artificial Intelligence and Data Science, India
AB  - In a number of industries, including banking, cybersecurity, healthcare, and others, risk assessment is an essential procedure. By automating data analysis, seeing trends, and offering insights that might help organisations make better decisions, artificial intelligence (AI) tools can greatly improve risk assessment. Here are some typical AI methods and strategies for risk assessment. Machine learning is the first step, followed by Natural Language Processing (NLP), Reinforcement Learning, Predictive Analytics, and so on. While AI may significantly improve risk assessment, it ought to be used in combination with human judgement and domain knowledge. Additionally, when using AI for risk assessment, it's imperative to address issues with the accuracy of data, bias, and ethics. To guarantee the accuracy and continued relevance of risk evaluations, regular verification of models and updating is also necessary. The importance of the attributes in the UNSW-NB15 collections is therefore thoroughly analysed in this work. This research analyses a UNSW-NB15 collection of data generation to address the problems associated with the lack of any network reference data sets. This data collection combines network traffic assault actions that are now synthesised with real-world modern norms. The UNSWNB15 information set's characteristics are produced using both established and cutting-edge techniques. Many Machine Learning algorithms are used to analyse the intrusion by measuring the accuracy and other features. Here the data set is classified as Binary classification and Multi Class Classification where Machine learning are applied to find out the intrusion. Random Forest Classifier (RFC) performance good for binary classification and Linear Support Vector Machine(LSVm)performance is good for Multiclass Classification. © 2023 IEEE.
KW  - Artificial Intelligence
KW  - Multiclass Classification
KW  - Natural Language Processing
KW  - Random Forest Classifier
KW  - Reinforcement Learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835031398-7 (ISBN)
LA  - English
J2  - Int. Conf. Sustain. Commun. Networks Appl., ICSCNA - Proc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 2023 International Conference on Sustainable Communication Networks and Application, ICSCNA 2023; Conference date: 15 November 2023 through 17 November 2023; Conference code: 196100
ER  -

TY  - CONF
AU  - Neufeld, E.A.
TI  - Reinforcement Learning Guided by Provable Normative Compliance
PY  - 2022
T2  - International Conference on Agents and Artificial Intelligence
VL  - 3
SP  - 444
EP  - 453
DO  - 10.5220/0010835600003116
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182770756&doi=10.5220%2f0010835600003116&partnerID=40&md5=0347e1f88427199a0362f5300ad40ede
AD  - Faculty of Informatics, Vienna, Austria
AB  - Reinforcement learning (RL) has shown promise as a tool for engineering safe, ethical, or legal behaviour in autonomous agents. Its use typically relies on assigning punishments to state-action pairs that constitute unsafe or unethical choices. Despite this assignment being a crucial step in this approach, however, there has been limited discussion on generalizing the process of selecting punishments and deciding where to apply them. In this paper, we adopt an approach that leverages an existing framework – the normative supervisor of (Neufeld et al., 2021) – during training. This normative supervisor is used to dynamically translate states and the applicable normative system into defeasible deontic logic theories, feed these theories to a theorem prover, and use the conclusions derived to decide whether or not to assign a punishment to the agent. We use multiobjective RL (MORL) to balance the ethical objective of avoiding violations with a non-ethical objective; we will demonstrate that our approach works for a multiplicity of MORL techniques, and show that it is effective regardless of the magnitude of the punishment we assign. © 2022 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.
KW  - Deontic Logic
KW  - Ethical AI
KW  - Reinforcement Learning
A2  - Rocha A.
A2  - Steels L.
A2  - van den Herik J.
PB  - Science and Technology Publications, Lda
SN  - 21843589 (ISSN)
LA  - English
J2  - Int. Conf. Agent. Artif. Intell.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Conference name: 14th International Conference on Agents and Artificial Intelligence , ICAART 2022; Conference date: 3 February 2022 through 5 February 2022; Conference code: 301199
ER  -

TY  - JOUR
AU  - Fiorini, P.
AU  - Goldberg, K.Y.
AU  - Liu, Y.
AU  - Taylor, R.H.
TI  - Concepts and Trends in Autonomy for Robot-Assisted Surgery
PY  - 2022
T2  - Proceedings of the IEEE
VL  - 110
IS  - 7
SP  - 993
EP  - 1011
DO  - 10.1109/JPROC.2022.3176828
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133760083&doi=10.1109%2fJPROC.2022.3176828&partnerID=40&md5=5e0017935bd747cb57e8d864e849174e
AD  - University of Verona, Department of Computer Science, Verona, 37134, Italy
AD  - University of California at Berkeley, Department of Industrial Engineering and Operations Research, The Department of Electrical Engineering and Computer Science, Berkeley, 94720, CA, United States
AD  - T Stone Robotics Institute, The Chinese University of Hong Kong, Department of Mechanical and Automation Engineering, Hong Kong, Hong Kong
AD  - Johns Hopkins University, Department of Computer Science, The Department of Mechanical Engineering, The Department of Radiology, The Department of Surgery, The Department of Otolaryngology, Head-and-Neck Surgery, Baltimore, 21218, MD, United States
AD  - Johns Hopkins University, Laboratory for Computational Sensing and Robotics, Baltimore, 21218, MD, United States
AB  - Surgical robots have been widely adopted with over 4000 robots being used in practice daily. However, these are telerobots that are fully controlled by skilled human surgeons. Introducing 'surgeon-assist' - some forms of autonomy - has the potential to reduce tedium and increase consistency, analogous to driver-assist functions for lane-keeping, cruise control, and parking. This article examines the scientific and technical backgrounds of robotic autonomy in surgery and some ethical, social, and legal implications. We describe several autonomous surgical tasks that have been automated in laboratory settings, and research concepts and trends. © 1963-2012 IEEE.
KW  - Autonomy
KW  - knowledge representation
KW  - machine learning (ML)
KW  - machine perception
KW  - surgical actions
KW  - surgical robotics
KW  - Knowledge representation
KW  - Reinforcement learning
KW  - Robots
KW  - Autonomy
KW  - Human surgeons
KW  - Knowledge-representation
KW  - Machine learning
KW  - Machine perception
KW  - Machine-learning
KW  - Robot-assisted surgery
KW  - Surgical action
KW  - Surgical robotics
KW  - Telerobot
KW  - Machine Perception
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 00189219 (ISSN)
LA  - English
J2  - Proc. IEEE
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 18; Correspondence Address: P. Fiorini; University of Verona, Department of Computer Science, Verona, 37134, Italy; email: paolo.fiorini@univr.it; CODEN: IEEPA
ER  -

TY  - CONF
AU  - Prabagar, S.
AU  - Al-Jiboory, A.K.
AU  - Nair, P.S.
AU  - Mandal, P.
AU  - Garse, K.M.
AU  - Natrayan, L.
TI  - Artificial Intelligence-Based Control Strategies for Unmanned Aerial Vehicles
PY  - 2023
T2  - 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering, UPCON 2023
SP  - 1624
EP  - 1629
DO  - 10.1109/UPCON59197.2023.10434918
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187335064&doi=10.1109%2fUPCON59197.2023.10434918&partnerID=40&md5=aa860d8dc4af0ad3ae3bc0a4feccc29e
AD  - Alliance College of Engineering and Design, Alliance University, Department of Computer Science and Engineering, Karnataka, Bangalore, India
AD  - University of Diyala, Mechanical Engineering, Baqubah, Iraq
AD  - Noida Institute of Engineering and Technology, Department of It, Greater Noida, India
AD  - Rimt University, Forensic Science, Gobindgarh, India
AD  - Sinhgad College of Engineering, Maharashtra, Pune, India
AD  - Saveetha School of Engineering, Simats, Department of Mechanical Engineering, Tamil Nadu, Chennai, 602105, India
AB  - Drones, also known as Unmanned Aerial Vehicles, or UAVs, are becoming more and more well-liked as a multifunctional tool that can be used in a variety of industries, including environmental monitoring, search and rescue, agricultural, and surveillance. In this work, we explore the potential for significantly increased UAV efficiency, versatility, and durability with AI-based control techniques. The research technique combines simulation, experimental validation, and mathematical modelling to fully investigate AI-based control systems. We develop a mathematical model of UAV dynamics and use a deep reinforcement learning (DRL) technique based on Proximal Policy Optimisation (PPO) to control the drones. Both theoretical analysis and real-world testing on a DJI Matrice 210 RTK platform validate the method's efficacy. The algorithm's capacity to provide precise and accurate answers is shown by the average location error's steady decline over time. It is very resilient, remaining stable in the face of wind gusts, and remarkably flexible, with quick response times. These patterns are shown graphically. According to the study's findings, AI-based control algorithms might greatly advance UAV technology by increasing its precision, adaptability, and dependability. Future priorities will include advanced machine learning techniques, multi-agent systems, safety measures, ethical frameworks, human-AI collaboration, environmental impact assessments, and urban integration. Through the creation of new capabilities and the resolution of urgent concerns, these activities have significant potential to impact UAV development in the future.  © 2023 IEEE.
KW  - AI-based control
KW  - Artificial Intelligence
KW  - Human-AI Collaboration
KW  - Position Tracking
KW  - Proximal Policy Optimization
KW  - Reinforcement Learning
KW  - UAV Dynamics
KW  - UAVs
KW  - Velocity Control
KW  - Wind Disturbances
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835038247-1 (ISBN)
LA  - English
J2  - IEEE Uttar Pradesh Sect. Int. Conf. Electr., Electron. Comput. Eng., UPCON
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering, UPCON 2023; Conference date: 1 December 2023 through 3 December 2023; Conference code: 197661
ER  -

TY  - JOUR
AU  - Seeamber, R.
AU  - Badea, C.
TI  - If Our Aim Is to Build Morality into an Artificial Agent, How Might We Begin to Go about Doing So?
PY  - 2023
T2  - IEEE Intelligent Systems
VL  - 38
IS  - 6
SP  - 35
EP  - 41
DO  - 10.1109/MIS.2023.3320875
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174839867&doi=10.1109%2fMIS.2023.3320875&partnerID=40&md5=5041392ceb220302f9ac2c2e3a170e8e
AD  - Imperial College London, London, SW7 2BX, United Kingdom
AB  - As AI becomes pervasive in most fields, from health care to autonomous driving, it is essential that we find successful ways of building morality into our machines, especially for decision making. However, the question of what it means to be moral is still debated, particularly in the context of AI. In this article, we highlight the different aspects that should be considered when building moral agents, including the most relevant moral paradigms and challenges. We also discuss the top-down and bottom-up approaches to design and the role of emotion and sentience in morality. We then propose solutions, including a hybrid approach to design and a hierarchical approach to combining moral paradigms. We emphasize how governance and policy are becoming ever more critical in AI ethics and in ensuring that the tasks we set for moral agents are attainable, that ethical behavior is achieved, and that we obtain good AI.  © 2001-2011 IEEE.
KW  - Intelligent systems
KW  - Job analysis
KW  - Philosophical aspects
KW  - Reinforcement learning
KW  - Artificial agents
KW  - Autonomous driving
KW  - Bottom up approach
KW  - Decisions makings
KW  - Hierarchical approach
KW  - Hybrid approach
KW  - Moral agents
KW  - Reinforcement learnings
KW  - Task analysis
KW  - Topdown
KW  - Decision making
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 15411672 (ISSN)
LA  - English
J2  - IEEE Intell. Syst.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: R. Seeamber; Imperial College London, London, SW7 2BX, United Kingdom; email: r.seeamber18@imperial.ac.uk
ER  -

TY  - JOUR
AU  - Fornari, L.
AU  - Ioumpa, K.
AU  - Nostro, A.D.
AU  - Evans, N.J.
AU  - De Angelis, L.
AU  - Speer, S.P.H.
AU  - Paracampo, R.
AU  - Gallo, S.
AU  - Spezio, M.
AU  - Keysers, C.
AU  - Gazzola, V.
TI  - Neuro-computational mechanisms and individual biases in action-outcome learning under moral conflict
PY  - 2023
T2  - Nature Communications
VL  - 14
IS  - 1
C7  - 1218
DO  - 10.1038/s41467-023-36807-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149936936&doi=10.1038%2fs41467-023-36807-3&partnerID=40&md5=22784ec00024f4afa8848a64743fac82
AD  - Netherlands Institute for Neuroscience, KNAW, Meibergdreef 47, Amsterdam, 1105BA, Netherlands
AD  - School of Psychology, University of Queensland, Brisbane, QLD, Australia
AD  - Psychology, Neuroscience, & Data Science, Scripps College, 1030 Columbia Ave, Claremont, 91711, CA, United States
AD  - Department of Psychology, University of Amsterdam, Nieuwe Achtergracht 129-B, Amsterdam, 1018 WT, Netherlands
AB  - Learning to predict action outcomes in morally conflicting situations is essential for social decision-making but poorly understood. Here we tested which forms of Reinforcement Learning Theory capture how participants learn to choose between self-money and other-shocks, and how they adapt to changes in contingencies. We find choices were better described by a reinforcement learning model based on the current value of separately expected outcomes than by one based on the combined historical values of past outcomes. Participants track expected values of self-money and other-shocks separately, with the substantial individual difference in preference reflected in a valuation parameter balancing their relative weight. This valuation parameter also predicted choices in an independent costly helping task. The expectations of self-money and other-shocks were biased toward the favored outcome but fMRI revealed this bias to be reflected in the ventromedial prefrontal cortex while the pain-observation network represented pain prediction errors independently of individual preferences. © 2023, The Author(s).
KW  - Bias
KW  - Humans
KW  - Learning
KW  - Morals
KW  - Pain
KW  - Prefrontal Cortex
KW  - decision making
KW  - machine learning
KW  - prediction
KW  - valuation
KW  - accuracy
KW  - adult
KW  - Article
KW  - conflict
KW  - decision making
KW  - external validity
KW  - female
KW  - functional magnetic resonance imaging
KW  - human
KW  - learning curve
KW  - learning theory
KW  - male
KW  - money
KW  - prediction
KW  - prediction error
KW  - ventromedial prefrontal cortex
KW  - diagnostic imaging
KW  - learning
KW  - morality
KW  - pain
KW  - prefrontal cortex
KW  - statistical bias
PB  - Nature Research
SN  - 20411723 (ISSN)
C2  - 36878911
LA  - English
J2  - Nat. Commun.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: V. Gazzola; Netherlands Institute for Neuroscience, KNAW, Amsterdam, Meibergdreef 47, 1105BA, Netherlands; email: v.gazzola@nin.knaw.nl
ER  -

TY  - JOUR
AU  - Chen, J.
AU  - Zhang, D.
AU  - Qu, Z.
AU  - Wang, C.
TI  - Modeling adaptive empathy based on neutral assessment: a way to enhance the prosocial behaviors of socialized agents under the premise of self-security
PY  - 2022
T2  - Applied Intelligence
VL  - 52
IS  - 6
SP  - 6692
EP  - 6722
DO  - 10.1007/s10489-021-02712-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114854937&doi=10.1007%2fs10489-021-02712-9&partnerID=40&md5=a34130c6412e7c93f7f800b33505ca15
AD  - Space Science and Inertial Technology Research Center, Harbin Institute of Technology, Harbin, 150001, China
AB  - Ethical concerns over artificial intelligence (AI) have recently drawn extensive interest in both academia and industry. According to behavioral economics and neuropsychology, empathy may be an inherent mechanism to elicit prosocial behaviors. Therefore, we establish a three-layer general framework of empathetic AI (eAI) with emotion, empathy, and decision-making. By introducing different sub-models, three learning structures based on eAI are proposed, including a gradient ascent (GA)-based structure, an adaptive learning structure, and a practical learning structure. The dynamics of the first two structures are analyzed theoretically, and the practical dynamics are tested in games. We prove that in the prisoner’s dilemma (PD) environment, the GA-based eAI with neutral assessment can carry out adaptive cooperation and competition under the premise of self-security. In addition, although the modeling of empathy by extracting the emotional contagion and limited cognitive regulation is simplified and primitive, tests in the prisoner’s dilemma, the ultimatum game, and a multi-agent dilemma game, show that the eAI structure successfully elicits prosocial behaviors including altruism, cooperation and fairness. Compared with other socialized algorithms, the eAI structure has a more comprehensive coverage in terms of convergence, fairness, security, adaptability, and structural expansibility. Therefore, we believe this work can provide novel methods and insights for regulating the behaviors of socialized agents, as well as artificial subjects in psychological and economic experiments. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
KW  - Adaptive empathy
KW  - Cooperation and competition
KW  - Multi-agent reinforcement learning
KW  - Prosocial behaviors
KW  - Decision making
KW  - Economics
KW  - Genetic algorithms
KW  - Multi agent systems
KW  - Adaptive cooperation
KW  - Adaptive learning
KW  - Behavioral economics
KW  - Economic experiments
KW  - Ethical concerns
KW  - Gradient ascent
KW  - Learning structure
KW  - Neuropsychology
KW  - Software agents
PB  - Springer
SN  - 0924669X (ISSN)
LA  - English
J2  - Appl Intell
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: C. Wang; Space Science and Inertial Technology Research Center, Harbin Institute of Technology, Harbin, 150001, China; email: cwang@hit.edu.cn; CODEN: APITE
ER  -

TY  - JOUR
AU  - Sharif, M.
AU  - Uckelmann, D.
TI  - Multi-Modal LA in Personalized Education Using Deep Reinforcement Learning Based Approach
PY  - 2024
T2  - IEEE Access
VL  - 12
SP  - 54049
EP  - 54065
DO  - 10.1109/ACCESS.2024.3388474
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190735332&doi=10.1109%2fACCESS.2024.3388474&partnerID=40&md5=f36e0c7211e8f38726d62a50966bb69b
AD  - University of Applied Sciences, Department of Informatics, Stuttgart, 70174, Germany
AB  - The demand for personalized learning experiences and effective analytics in education has significantly increased. The integration of technology in education has brought about significant changes in teaching and learning practices. In the era of digital technology, the integration of education technology in the classroom has led to a change in teaching methods and learning strategies. In this paper, we introduce the KNIGHT (AI in Education at Hochschule für Technik (HFT) Stuttgart) framework, which is a holistic solution designed to tackle the complex issue of personalized education in a digital era. The paper explores the application of multimodal data integration, the novel application of deep reinforcement learning to education analytics, and the ethical consideration of privacy-preserving personalized feedback. The proposed framework's efficacy is substantiated through a case study, demonstrating its potential to revolutionize personalized education. This paper provides a comprehensive overview of the current discourse, providing valuable insights for educators, policymakers, and researchers into the multifaceted landscape of modern education, contributing to ongoing discussions and advancements in educational technology.  © 2013 IEEE.
KW  - Artificial intelligence (AI)
KW  - deep learning (DL)
KW  - deep reinforcement learning (DRL)
KW  - learning analytics (LA)
KW  - machine learning (ML)
KW  - multi-modal learning analytics (MMLA)
KW  - personalise learning (PL)
KW  - Data integration
KW  - Data privacy
KW  - Deep learning
KW  - E-learning
KW  - Engineering education
KW  - Ethical technology
KW  - Predictive analytics
KW  - Teaching
KW  - Artificial intelligence
KW  - Deep learning
KW  - Deep reinforcement learning
KW  - Europe
KW  - Learning (artificial intelligence)
KW  - Learning analytic
KW  - Machine learning
KW  - Machine-learning
KW  - Multi-modal learning
KW  - Multi-modal learning analytic
KW  - Personalize learning
KW  - Predictive models
KW  - Reinforcement learnings
KW  - Usability
KW  - Reinforcement learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 21693536 (ISSN)
LA  - English
J2  - IEEE Access
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: M. Sharif; University of Applied Sciences, Department of Informatics, Stuttgart, 70174, Germany; email: muddsair.sharif@hft-stuttgart.de
ER  -

TY  - CONF
AU  - Kaswan, K.S.
AU  - Dhatterwal, J.S.
AU  - Malik, K.
AU  - Baliyan, A.
TI  - Generative AI: A Review on Models and Applications
PY  - 2023
T2  - 2023 International Conference on Communication, Security and Artificial Intelligence, ICCSAI 2023
SP  - 699
EP  - 704
DO  - 10.1109/ICCSAI59793.2023.10421601
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186716169&doi=10.1109%2fICCSAI59793.2023.10421601&partnerID=40&md5=0c836bcd328b1b9b45eef2c8d2fbcfa2
AD  - School of Computing Science and Engineering, Galgotias University, UP, Greater Noida, India
AD  - Koneru Lakshmaiah Education Foundation, Artificial Intelligence and Data Science, AP, Guntur, India
AD  - Matu Ram Institute of Engineering and Management, Computer Science and Engineering, Haryana, Rohtak, India
AD  - Chandigarh University, Department of Computer Science and Engineering, Chandigarh, India
AB  - Generative Artificial Intelligence (AI) stands as a transformative paradigm in machine learning, enabling the creation of complex and realistic data from latent representations. This review paper comprehensively surveys the landscape of Generative AI, encompassing its foundational concepts, diverse models, training methodologies, applications, challenges, recent advancements, evaluation metrics, and ethical dimensions. The paper begins by introducing Generative AI's significance across various domains, presenting its pivotal role in producing synthetic data with applications spanning image synthesis, text generation, music composition, drug discovery, and more. The objectives lie in elucidating the foundational concepts, delving into model intricacies, unveiling the training procedures, exploring its application landscape, addressing challenges, envisioning future directions, and discussing ethical ramifications. The foundational section elucidates the diverse array of generative models, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), flow-based models, Generative Reinforcement Learning (GRL), and advanced hybrid architectures. Subsequently, evaluation metrics ranging from Inception Score to perceptual similarity metrics and human evaluations are surveyed to assess generative model performance. Finally, ethical considerations underscore the necessity for addressing biases, misuse, intellectual property concerns, and the call for responsible AI development and regulation in the Generative AI landscape.  © 2023 IEEE.
KW  - Advanced Hybrid Architectures
KW  - Flow-Based Models
KW  - Generative Adversarial Networks (GANs)
KW  - Generative Reinforcement Learning (GRL)
KW  - Variational Autoencoders (VAEs)
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835036996-0 (ISBN)
LA  - English
J2  - Int. Conf. Commun., Secur. Artif. Intell., ICCSAI
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: K.S. Kaswan; School of Computing Science and Engineering, Galgotias University, Greater Noida, UP, India; email: kaswankuldeep@gmail.com; Conference name: 2nd IEEE International Conference on Communication, Security and Artificial Intelligence, ICCSAI 2023; Conference date: 23 November 2023 through 25 November 2023; Conference code: 197294
ER  -

TY  - JOUR
AU  - Arranz, R.
AU  - Carramiñana, D.
AU  - Miguel, G.
AU  - Besada, J.A.
AU  - Bernardos, A.M.
TI  - Application of Deep Reinforcement Learning to UAV Swarming for Ground Surveillance
PY  - 2023
T2  - Sensors (Basel, Switzerland)
VL  - 23
IS  - 21
DO  - 10.3390/s23218766
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176895629&doi=10.3390%2fs23218766&partnerID=40&md5=7f53d6735e134e6de517ce6420705afd
AD  - Information Processing and Telecommunications Center, Universidad Politécnica de Madrid ,ETSI Telecomunicación ,Av. Complutense 30, 28040, Madrid, Spain
AB  - This paper summarizes in depth the state of the art of aerial swarms, covering both classical and new reinforcement-learning-based approaches for their management. Then, it proposes a hybrid AI system, integrating deep reinforcement learning in a multi-agent centralized swarm architecture. The proposed system is tailored to perform surveillance of a specific area, searching and tracking ground targets, for security and law enforcement applications. The swarm is governed by a central swarm controller responsible for distributing different search and tracking tasks among the cooperating UAVs. Each UAV agent is then controlled by a collection of cooperative sub-agents, whose behaviors have been trained using different deep reinforcement learning models, tailored for the different task types proposed by the swarm controller. More specifically, proximal policy optimization (PPO) algorithms were used to train the agents' behavior. In addition, several metrics to assess the performance of the swarm in this application were defined. The results obtained through simulation show that our system searches the operation area effectively, acquires the targets in a reasonable time, and is capable of tracking them continuously and consistently.
KW  - artificial intelligence
KW  - centralized
KW  - drones
KW  - obstacle avoidance
KW  - search
KW  - swarm
KW  - tracking
KW  - algorithm
KW  - article
KW  - artificial intelligence
KW  - avoidance behavior
KW  - benchmarking
KW  - diagnosis
KW  - drone
KW  - eye tracking
KW  - human
KW  - law enforcement
KW  - learning
KW  - reinforcement (psychology)
KW  - simulation
SN  - 14248220 (ISSN)
C2  - 37960466
LA  - English
J2  - Sensors (Basel)
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0
ER  -

TY  - CONF
AU  - Chaput, R.
AU  - Matignon, L.
AU  - Guillermin, M.
TI  - Learning to identify and settle dilemmas through contextual user preferences
PY  - 2023
T2  - Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI
SP  - 474
EP  - 479
DO  - 10.1109/ICTAI59109.2023.00075
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182400235&doi=10.1109%2fICTAI59109.2023.00075&partnerID=40&md5=4d37496efa6315a69165b29b7c99ef41
AD  - Univ Lyon, Ucbl, Cnrs, Insa Lyon Liris, UMR5205, Villeurbanne, France
AD  - Lyon Catholic University, CONFLUENCE: Sciences et Humanités Research Unit (EA 1598), Lyon, France
AB  - Artificial Intelligence systems have a significant impact on human lives. Machine Ethics tries to align these systems with human values, by integrating 'ethical considerations'. However, most approaches consider a single objective, and thus cannot accommodate different, contextual human preferences. Multi-Objective Reinforcement Learning algorithms account for various preferences, but they often are not intelligible nor contextual (e.g., weighted preferences). Our novel approach identifies dilemmas, presents them to users, and learns to settle them, based on intelligible and contextualized preferences over actions. We intend to maximize understandability and opportunities for user-system co-construction by showing dilemmas, and triggering interactions, thus empowering users. The block-based architecture enables leveraging simple mechanisms that can be updated and improved. Validation on a Smart Grid use-case shows that our algorithm finds actions for various trade-offs, and quickly learns to settle dilemmas, reducing the cognitive load on users. © 2023 IEEE.
KW  - Human Preferences
KW  - Machine Ethics
KW  - Moral Dilemmas
KW  - Multi-Objective Reinforcement Learning
KW  - Economic and social effects
KW  - Learning algorithms
KW  - Learning systems
KW  - Philosophical aspects
KW  - Artificial intelligence systems
KW  - Human lives
KW  - Human preference
KW  - Learn+
KW  - Machine ethic
KW  - Moral dilemma
KW  - Multi objective
KW  - Multi-objective reinforcement learning
KW  - Reinforcement learnings
KW  - User's preferences
KW  - Reinforcement learning
PB  - IEEE Computer Society
SN  - 10823409 (ISSN); 979-835034273-4 (ISBN)
LA  - English
J2  - Proc. Int. Conf. Tools Artif. Intell. ICTAI
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: R. Chaput; Univ Lyon, Ucbl, Cnrs, Insa Lyon Liris, UMR5205, Villeurbanne, France; email: remy.chaput@univ-lyon1.fr; Conference name: 35th IEEE International Conference on Tools with Artificial Intelligence, ICTAI 2023; Conference date: 6 November 2023 through 8 November 2023; Conference code: 195714; CODEN: PCTIF
ER  -

TY  - JOUR
AU  - Krening, S.
TI  - Q-learning as a model of utilitarianism in a human–machine team
PY  - 2023
T2  - Neural Computing and Applications
VL  - 35
IS  - 23
SP  - 16853
EP  - 16864
DO  - 10.1007/s00521-022-08063-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143206412&doi=10.1007%2fs00521-022-08063-x&partnerID=40&md5=4dd4a7bdfc6455ab75dce3a516728a01
AD  - Department of Integrated Systems Engineering, The Ohio State University, 1971 Neil Ave, Columbus, 43210, OH, United States
AB  - This paper demonstrates that Q-learning can be used to model Utilitarian decision-making. Accurately modeling ethical theories from the field of moral philosophy is an important step in the development of ethical machine learning. Modeling Utilitarian decision-making with Q-learning is a step toward ethical human–machine teaming; the human and machine contribute according to their strengths to create a more accurate Utilitarian decision than either would make individually. The Utilitarian decision is output by the Q-learning agent, as well as the ranked order of sub-optimal actions to aid in explainability. Additionally, using RL to mathematically represent Utilitarianism solves the classic drawback of Utilitarianism concerning prohibitive computation. This model can also be used for cost-benefit analysis and business decisions. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.
KW  - Ethics
KW  - Human–machine teaming
KW  - Reinforcement learning
KW  - Utilitarianism
KW  - Computation theory
KW  - Cost benefit analysis
KW  - Ethical technology
KW  - Learning systems
KW  - Reinforcement learning
KW  - Decisions makings
KW  - Ethical theories
KW  - Human-machine
KW  - Human–machine teaming
KW  - Machine-learning
KW  - Moral philosophy
KW  - Q-learning
KW  - Q-learning agents
KW  - Reinforcement learnings
KW  - Utilitarianism
KW  - Decision making
PB  - Springer Science and Business Media Deutschland GmbH
SN  - 09410643 (ISSN)
LA  - English
J2  - Neural Comput. Appl.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: S. Krening; Department of Integrated Systems Engineering, The Ohio State University, Columbus, 1971 Neil Ave, 43210, United States; email: krening.2@osu.edu
ER  -

TY  - CONF
AU  - Zhao, C.
AU  - Chen, F.
AU  - Wu, X.
AU  - Chen, H.
AU  - Zhou, J.
TI  - 2nd Workshop on Ethical Artificial Intelligence: Methods and Applications (EAI)
PY  - 2023
T2  - Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
SP  - 5903
EP  - 5904
DO  - 10.1145/3580305.3599215
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171372949&doi=10.1145%2f3580305.3599215&partnerID=40&md5=3bb076b0bded0e2d582bd0c479dcfd71
AD  - Baylor University, Waco, TX, United States
AD  - University of Texas at Dallas, Richardson, TX, United States
AD  - University of Arkansas, Fayetteville, AR, United States
AD  - NEC Laboratories America, Princeton, NJ, United States
AD  - Michigan State University, East Lansing, MI, United States
AB  - Ethical AI has become increasingly important, and it has been attracting attention from academia and industry, due to its increased popularity in real-world applications with fairness concerns. It also places fundamental importance on ethical considerations in determining legitimate and illegitimate uses of AI. Organizations that apply ethical AI have clearly stated well-defined review processes to ensure adherence to legal guidelines. Therefore, the wave of research at the intersection of ethical AI in data mining and machine learning has also influenced other fields of science, including computer vision, natural language processing, reinforcement learning, and social science. Despite these successes, ethical AI still faces many challenges, such as a lack of interpretable and explainable methods for fairness-aware deep learning models, etc. Consequently, there is an urgent need to bring experts and researchers together at prestigious venues to discuss ethical AI, which has been rarely seen in previous KDD conferences. This workshop will provide a premium platform for both research and industry from different backgrounds to exchange ideas on opportunities, challenges, and cutting-edge techniques in ethical AI.  © 2023 Owner/Author.
KW  - ethical artificial intelligence
KW  - fairness-aware
KW  - machine learning
KW  - Data mining
KW  - Industrial research
KW  - Learning algorithms
KW  - Learning systems
KW  - Natural language processing systems
KW  - Philosophical aspects
KW  - Reinforcement learning
KW  - Artificial intelligence methods
KW  - Ethical artificial intelligence
KW  - Ethical considerations
KW  - Fairness concerns
KW  - Fairness-aware
KW  - Legal guidelines
KW  - Machine-learning
KW  - Natural languages
KW  - Real-world
KW  - Review process
KW  - Deep learning
PB  - Association for Computing Machinery
SN  - 979-840070103-0 (ISBN)
LA  - English
J2  - Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023; Conference date: 6 August 2023 through 10 August 2023; Conference code: 191432
ER  -

TY  - JOUR
AU  - Vamplew, P.
AU  - Smith, B.J.
AU  - Källström, J.
AU  - Ramos, G.
AU  - Rădulescu, R.
AU  - Roijers, D.M.
AU  - Hayes, C.F.
AU  - Heintz, F.
AU  - Mannion, P.
AU  - Libin, P.J.K.
AU  - Dazeley, R.
AU  - Foale, C.
TI  - Scalar reward is not enough: a response to Silver, Singh, Precup and Sutton (2021)
PY  - 2022
T2  - Autonomous Agents and Multi-Agent Systems
VL  - 36
IS  - 2
C7  - 41
DO  - 10.1007/s10458-022-09575-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134198938&doi=10.1007%2fs10458-022-09575-5&partnerID=40&md5=e19e4c13fbe658633d9e2d21caafc929
AD  - Federation University Australia, Ballarat, Australia
AD  - Center for Translational Neuroscience, University of Oregon, Eugene, OR, United States
AD  - Linköping University, Linköping, Sweden
AD  - Universidade do Vale do Rio dos Sinos, RS, São Leopoldo, Brazil
AD  - AI Lab, Vrije Universiteit Brussel, Brussel, Belgium
AD  - Vrije Universiteit Brussel, Brussel, Belgium
AD  - HU University of Applied Sciences Utrecht, Utrecht, Netherlands
AD  - National University of Ireland Galway, Galway, Ireland
AD  - Universiteit Hasselt, Hasselt, Belgium
AD  - Katholieke Universiteit Leuven, Leuven, Belgium
AD  - Deakin University, Geelong, Australia
AB  - The recent paper “Reward is Enough” by Silver, Singh, Precup and Sutton posits that the concept of reward maximisation is sufficient to underpin all intelligence, both natural and artificial, and provides a suitable basis for the creation of artificial general intelligence. We contest the underlying assumption of Silver et al. that such reward can be scalar-valued. In this paper we explain why scalar rewards are insufficient to account for some aspects of both biological and computational intelligence, and argue in favour of explicitly multi-objective models of reward maximisation. Furthermore, we contend that even if scalar reward functions can trigger intelligent behaviour in specific cases, this type of reward is insufficient for the development of human-aligned artificial general intelligence due to unacceptable risks of unsafe or unethical behaviour. © 2022, The Author(s).
KW  - Artificial general intelligence
KW  - Multi-objective decision making
KW  - Multi-objective reinforcement learning
KW  - Reinforcement learning
KW  - Safe and ethical AI
KW  - Scalar rewards
KW  - Vector rewards
KW  - Decision making
KW  - Silver
KW  - Artificial general intelligences
KW  - Multi objective
KW  - Multi objective decision making
KW  - Multi-objective reinforcement learning
KW  - Reinforcement learnings
KW  - Safe and ethical AI
KW  - Scalar reward
KW  - Suttons
KW  - Vector reward
KW  - Reinforcement learning
PB  - Springer
SN  - 13872532 (ISSN)
LA  - English
J2  - Auton. Agents Multi-Agent Syst.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 16; Correspondence Address: P. Vamplew; Federation University Australia, Ballarat, Australia; email: p.vamplew@federation.edu.au
ER  -

TY  - CONF
AU  - Hjelmeland, H.W.
AU  - Eriksen, B.-O.H.
AU  - Mengshoel, O.J.
AU  - Lekkas, A.M.
TI  - Identification of Failure Modes in the Collision Avoidance System of an Autonomous Ferry using Adaptive Stress Testing
PY  - 2022
T2  - IFAC-PapersOnLine
VL  - 55
IS  - 31
SP  - 470
EP  - 477
DO  - 10.1016/j.ifacol.2022.10.472
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145657415&doi=10.1016%2fj.ifacol.2022.10.472&partnerID=40&md5=f2f42f23e2b9db764928e150f0a47f76
AD  - Norwegian University of Science and Technology, Trondheim, 7491, Norway
AD  - Zeabuz, Trondheim, 7042, Norway
AB  - As complex autonomous systems emerge in the maritime sector, measures must be taken in order to ensure thorough safety assessment. Real-world testing can be costly and potentially dangerous, and therefore there is a need for suitable simulation-based methods. This paper presents an implementation of the Adaptive Stress Testing (AST) method applied to the collision avoidance (COLAV) system of a small passenger ferry. AST is a simulation-based technique which has shown promising results in safety assessment of aviation and automobile systems. Given a simulator of a system, AST uses reinforcement learning to optimize toward system failure, and returns the most likely failure scenarios. AST is here shown to successfully identify scenarios where the criteria for failure are met, which is when the ferry collides with an adversary vessel controlled by AST. However, most of the initial results exhibit failures where the COLAV system of the ferry is not responsible for the failure, making the results less valuable to system developers. To improve the relevance, augmentations are made to the optimization problem. The augmentations result in four distinct problem formulations presented in the paper. Finally, the results are clustered using an unsupervised machine learning method called Soft Dynamic Time Warping k-means clustering in order to present a general summary of the identified failure scenarios. Our results demonstrate the relevance and potential of AST for the maritime sector and illustrate how common drawbacks of AST can be circumvented by method adjustment. © 2022 Elsevier B.V.. All rights reserved.
KW  - Adaptive Stress Testing
KW  - Autonomous Vehicles
KW  - Reinforcement Learning
KW  - Safety
KW  - Simulation
KW  - Accident prevention
KW  - Autonomous vehicles
KW  - Collision avoidance
KW  - K-means clustering
KW  - Learning systems
KW  - Safety devices
KW  - Safety testing
KW  - Adaptive stress testing
KW  - Autonomous Vehicles
KW  - Collision avoidance systems
KW  - Failure scenarios
KW  - Maritime sector
KW  - Real-world testing
KW  - Reinforcement learnings
KW  - Safety assessments
KW  - Simulation
KW  - Stress Testing
KW  - Reinforcement learning
A2  - Monteriu A.
PB  - Elsevier B.V.
SN  - 24058963 (ISSN)
LA  - English
J2  - IFAC-PapersOnLine
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 14th IFAC Conference on Control Applications in Marine Systems, Robotics, and Vehicles, CAMS 2022; Conference date: 14 September 2022 through 16 September 2022; Conference code: 185343
ER  -

TY  - CONF
AU  - Jagan, S.
AU  - Jadhav, G.K.
AU  - Priya, C.
AU  - Adhikary, P.
AU  - Bhatt, U.M.
AU  - Jayasri, V.
TI  - Reinforcement Learning-Based Autonomous Collision Prediction and Avoidance System for Vehicles
PY  - 2023
T2  - Proceedings of the 2023 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems, ICSES 2023
DO  - 10.1109/ICSES60034.2023.10465417
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190504168&doi=10.1109%2fICSES60034.2023.10465417&partnerID=40&md5=c58d21a47cb4434c2258dda83b20b035
AD  - Vel Tech RangarajanDr.Sagunthala R&D Institute of Science and Technology, Department of Computer Science and Engineering, Tamil Nadu, Chennai, India
AD  - Mechanical Engineering, Dr. D y Patil Institute of Engineering, Management & Research Akurdi, Pune, India
AD  - Sri Sairam Engineering College, Department of Electrical and Electronics Engineering, Tamil Nadu, Chennai, India
AD  - New Horizon College of Engineering, Department of Mechanical Engineering, Bangalore, India
AD  - Graphic Era Deemed to Be University, Department of Computer Science & Engineering, Uttarakhand, Dehradun, 248002, India
AD  - St. Martins Engineering College, Department of AI&DS, Telangana, Secunderabad, India
AB  - This study investigates the feasibility developing an Autonomous Collision Prediction and Avoidance System (RL-ACPAS) based on Reinforcement Learning automobiles in light of the increasing prevalence of autonomous vehicles on the road. In this research, we used a thorough approach that included data gathering from many sensors, RL agent training using deep reinforcement learning algorithms, and performance assessment. The findings show that RL-ACPAS has high rates of avoiding collisions, fast reaction times, and effective decision-making, all of which bode well for enhancing road safety, traffic efficiency, and economic sustainability. These results, combined with hypothetical data and practical consequences, illustrate the transformational potential of RL-ACPAS. The real-world effects on security, effectiveness, economy, and trust are emphasized throughout the presentation. Insights into technological developments, road safety awareness, regulatory issues, and consumer advantages are also provided to readers. Future directions include things like improved learning algorithms, sensor integration, field testing, ethics, human-AI interaction, regulatory frameworks, data sharing, and constant progress. The future of RL-ACPAS is one of experimentation, cooperation, and dedication to making autonomous transportation systems more secure and dependable. © 2023 IEEE.
KW  - Autonomous Vehicles
KW  - Avoidance Systems
KW  - Collision Prediction
KW  - Deep Reinforcement Learning
KW  - Real-Time Decision-Making
KW  - Regulatory Frameworks
KW  - Reinforcement Learning
KW  - RL-ACPAS
KW  - Traffic Efficiency
KW  - Vehicle Safety
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835031920-0 (ISBN)
LA  - English
J2  - Proc. Int. Conf. Innov. Comput., Intell. Commun. Smart Electr. Syst., ICSES
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: S. Jagan; Vel Tech RangarajanDr.Sagunthala R&D Institute of Science and Technology, Department of Computer Science and Engineering, Chennai, Tamil Nadu, India; email: drsjagan.research@gmail.com; Conference name: 2023 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems, ICSES 2023; Conference date: 14 December 2023 through 15 December 2023; Conference code: 198265
ER  -

TY  - JOUR
AU  - Zhang, Q.
AU  - Chen, B.
AU  - Liu, G.
TI  - Artificial intelligence can dynamically adjust strategies for auxiliary diagnosing respiratory diseases and analyzing potential pathological relationships
PY  - 2023
T2  - Journal of Breath Research
VL  - 17
IS  - 4
C7  - 046007
DO  - 10.1088/1752-7163/acf065
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168802671&doi=10.1088%2f1752-7163%2facf065&partnerID=40&md5=046618b70c2ae616e8becdf3f0f41076
AD  - College of Electronic Information and Optical Engineering, Nankai University, Tianjin, 300350, China
AD  - Tianjin Key Laboratory of Optoelectronic Sensor and Sensing Network Technology, Nankai University, Tianjin, 300350, China
AD  - General Terminal IC Interdisciplinary Science Center of Nankai University, Nankai University, Tianjin, 300350, China
AD  - Engineering Research Center of Thin Film Optoelectronics Technology, Ministry of Education, Nankai University, Tianjin, 300350, China
AB  - Respiratory diseases are one of the leading causes of human death and exacerbate the global burden of non-communicable diseases. Finding a method to assist clinicians pre-diagnose these diseases is an urgent task. Existing artificial intelligence-based methods can improve the clinical diagnosis efficiency, but still face challenges. For example, the lack of interpretability, the problem of information redundancy or missing caused by only using static data, the difficulty of model to learn the interdependence between features, and the performance of model is limited by sparse datasets, etc. To alleviate these problems, we propose a novel RQPA-Net. It consists of Q&A diagnosis module (QAD) and pathological inference module (PI). The QAD is responsible for interacting with patients, adjusting inquiry strategies dynamically and collecting effective information for disease diagnosis. The designed multi-subspace network can alleviate the problem that classical method is difficult to understand the interdependence between features. The deep reinforcement learning designed also can alleviate the problem of classical methods lack of interpretability. The PI is responsible for reasoning potential pathological relationships between diseases or symptoms based on existing knowledge. Through integrating the advantages of deep learning and reinforcement learning techniques, PI can handle sparse datasets. Finally, for auxiliary diagnosis, the model achieves 0.9780 ± 0.0002 Recall, 0.9778 ± 0.0003 Acc, 0.9779 ± 0.0003 Precision and 0.9780 ± 0.0003 F1-score on the test set. In terms of assisting pathological analysis, compared with the end-to-end model, our model achieves higher comprehensive performance on different tasks and datasets with different degrees of sparsity. Even in sparse datasets, it can effectively infer potential associations between diseases or symptoms, and has higher potential clinical application. In this paper, we propose a novel network structure, which can not only assist doctors in diagnosing diseases, but also contribute to explore the potential disease mechanisms. It provides a new perspective for integrating AI technology and clinical practice. © 2023 IOP Publishing Ltd
KW  - adaptive disease diagnosis
KW  - artificial intelligence
KW  - auxiliary pathology analysis
KW  - Artificial Intelligence
KW  - Breath Tests
KW  - Humans
KW  - Respiratory Tract Diseases
KW  - Article
KW  - artificial intelligence
KW  - clinical effectiveness
KW  - clinical feature
KW  - clinical practice
KW  - diagnostic accuracy
KW  - disease severity
KW  - human
KW  - reinforcement learning (machine learning)
KW  - respiratory tract disease
KW  - risk factor
KW  - breath analysis
KW  - respiratory tract disease
PB  - Institute of Physics
SN  - 17527155 (ISSN)
C2  - 37582347
LA  - English
J2  - J. Breath Res.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: G. Liu; College of Electronic Information and Optical Engineering, Nankai University, Tianjin, 300350, China; email: liugh@nankai.edu.cn
ER  -

TY  - CONF
AU  - Maathuis, C.
AU  - Chockalingam, S.
TI  - Tackling uncertainty through probabilistic modelling of proportionality in military operations
PY  - 2023
T2  - European Conference on Information Warfare and Security, ECCWS
VL  - 2023-June
SP  - 276
EP  - 284
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167622755&partnerID=40&md5=601778f0ab5d170f0084239ac1a4fe54
AD  - Open University of the Netherlands, Heerlen, Netherlands
AD  - Institute for Energy Technology, Halden, Norway
AB  - Just as every neuron in a biological neural network is a reinforcement learning agent, thus a component of a large and advanced structure is de facto a model, the two main components forming the principle of proportionality in military operations can be seen and are as a matter of fact two different entities and models. These are collateral damage depicting the unintentional effects affecting civilians and civilian objects, and military advantage symbolizing the intentional effects contributing to achieving the military objectives defined for military operation conducted. These two entities are complex processes relying on available information, projection on time to the moment of target engagement through estimation and are strongly dependent of common-sense reasoning and decision making. As a deduction, these two components and the proportionality decision result are processes surrounded by various sources and types of uncertainty. However, the existing academic and practitioner efforts in understanding the meaning, dimensions, and implications of the proportionality principle are considering military-legal and ethical lenses, and less technical ones. Accordingly, this research calls for a movement from the existing vision of interpreting proportionality in a possibilistic way to a probabilistic way. Henceforth, this research aims to build two probabilistic Machine Learning models based on Bayesian Belief Networks for assessing proportionality in military operations. The first model embeds a binary classification approach assessing if the engagement is proportional or disproportional, and the second model that extends this perspective based on previous research to perform multi-class classification for assessing degrees of proportionality. To accomplish this objective, this research follows the Design Science Research methodology and conducts an extensive literature for building and demonstrating the model proposed. Finally, this research intends to contribute to designing and developing explainable and responsible intelligent solutions that support human-based military targeting decision-making processes involved when building and conducting military operations. © 2023 Curran Associates Inc.. All rights reserved.
KW  - Bayesian Networks
KW  - Cyber Operations
KW  - Machine Learning
KW  - Military Operations
KW  - Proportionality
KW  - Targeting
KW  - Decision making
KW  - Learning systems
KW  - Military operations
KW  - Neural networks
KW  - Reinforcement learning
KW  - Uncertainty analysis
KW  - Bayesia n networks
KW  - Biological neural networks
KW  - Cyber operations
KW  - Large structures
KW  - Machine-learning
KW  - Probabilistic models
KW  - Proportionality
KW  - Reinforcement learning agent
KW  - Targeting
KW  - Uncertainty
KW  - Bayesian networks
A2  - Andreatos A.
A2  - Douligeris C.
PB  - Curran Associates Inc.
SN  - 20488602 (ISSN); 978-191458770-2 (ISBN)
LA  - English
J2  - European Conf. Inf. Warfare Security, ECCWS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Conference name: 22nd European Conference on Cyber Warfare and Security, ECCWS 2023; Conference date: 22 June 2023 through 23 June 2023; Conference code: 190591
ER  -

TY  - JOUR
AU  - Abdo, L.
AU  - Ahmad, I.
AU  - Abed, S.
TI  - A smart admission control and cache replacement approach in content delivery networks
PY  - 2023
T2  - Cluster Computing
DO  - 10.1007/s10586-023-04095-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164161498&doi=10.1007%2fs10586-023-04095-7&partnerID=40&md5=e3adbde54414e5c847ba88107b0be997
AD  - Computer Engineering Department, College of Engineering and Petroleum, Kuwait University, Kuwait City, Kuwait
AB  - Content Delivery Networks (CDNs) distribute most data traffic nowadays by caching the contents in a network of servers to provide users with the requested objects, and helping to reduce latency when delivering contents to the user. The content caching system performance depends upon many factors such as where the objects should be stored, which object to store, and when to cache them. The proposed methodology includes two main phases: an admission control phase and a cache replacement phase. The admission control phase is responsible for accepting or rejecting the incoming request based on training the Reinforcement Learning (RL) algorithm to make the best decision in the near future to maximize its reward, which, in this case, is the hit ratio. The cache replacement phase estimates the object’s future popularity. This is achieved by building a predictive model based on the popularity prediction mechanism, where the Long-Short-Term Memory (LSTM) model is used to compute the object’s popularity. The LSTM model’s outcome can help decide which objects to cache and which objects to evict from the cache. The proposed methodology is tested on a dataset to demonstrate its effectiveness in enhancing the hit ratio compared to conventional replacement policies such as First-in-First-Out (FIFO), Least Recently Used (LRU), Least Frequently Used (LFU) and a recent machine learning-based algorithm. The experimental results on the dataset revealed that the proposed methodology outperformed the baseline algorithms by 34.7% to 97.17% with a cache size of 130. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
KW  - Cache hit ratio
KW  - Deep learning
KW  - Probability prediction
KW  - Reinforcement learning
KW  - Smart caching policies
KW  - Cache memory
KW  - Learning algorithms
KW  - Long short-term memory
KW  - Admission-control
KW  - Cache hit ratio
KW  - Cache replacement
KW  - Caching policy
KW  - Content delivery network
KW  - Control phase
KW  - Deep learning
KW  - Probability prediction
KW  - Reinforcement learnings
KW  - Smart caching policy
KW  - Reinforcement learning
PB  - Springer
SN  - 13867857 (ISSN)
LA  - English
J2  - Cluster Comput.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: S. Abed; Computer Engineering Department, College of Engineering and Petroleum, Kuwait University, Kuwait City, Kuwait; email: s.abed@ku.edu.kw
ER  -

TY  - CONF
AU  - Antony, S.
AU  - Sabari, T.
AU  - Joshua, R.I.
AU  - Jayapandian, N.
TI  - Artificial Intelligence Involvement in Graphic Game Development
PY  - 2023
T2  - Proceedings of the 2023 2nd International Conference on Augmented Intelligence and Sustainable Systems, ICAISS 2023
SP  - 82
EP  - 86
DO  - 10.1109/ICAISS58487.2023.10250553
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173622631&doi=10.1109%2fICAISS58487.2023.10250553&partnerID=40&md5=6a05b035c9e531883e450f446789513f
AD  - Christ University, Department of Cse, Bangalore, India
AB  - Games have always been a popular form of entertainment and with the advancements in technology, the integration of Artificial Intelligence (AI) in gaming has revolutionized the gaming industry. This research article aims to explore the various applications of AI in gaming and its impact on the industry and player experience. Unlike the typical straightforward nature of AI, this research paper takes a more human approach to discussing the topic. It delves into the evolution of AI in games and the various types of AI used in game development. These include rule-based AI, learning- based AI, and evolutionary AI, which have all contributed to the development of increasingly immersive gaming experiences. The benefits and challenges of using AI in games are also explored, considering the impact on player experience. While AI-powered opponents can provide a greater challenge, balancing the difficulty level is critical to ensuring the game remains enjoyable. The potential ethical concerns of using AI in games are also discussed, such as data privacy, bias, and fairness. Furthermore, this research paper looks into the future of AI in games and how it may shape the gaming industry and player experience in the years to come. With the continued development of AI techniques such as reinforcement learning and GANs, the possibilities for more immersive and engaging gaming experiences are endless. © 2023 IEEE.
KW  - Artificial Intelligence
KW  - Game Development
KW  - Natural Language Processing
KW  - Rule-based Artificial Intelligence
KW  - Unreal Engine
KW  - Data privacy
KW  - Ethical technology
KW  - Natural language processing systems
KW  - Reinforcement learning
KW  - Game development
KW  - Industry experience
KW  - Language processing
KW  - Natural language processing
KW  - Natural languages
KW  - Player experience
KW  - Research papers
KW  - Rule based
KW  - Rule-based artificial intelligence
KW  - Unreal engine
KW  - Software design
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835032579-9 (ISBN)
LA  - English
J2  - Proc. Int. Conf. Augment. Intell. Sustain. Syst., ICAISS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: S. Antony; Christ University, Department of Cse, Bangalore, India; email: safal.antony@btech.christuniversity.in; Conference name: 2nd International Conference on Augmented Intelligence and Sustainable Systems, ICAISS 2023; Conference date: 23 August 2023 through 25 August 2023; Conference code: 192825
ER  -

TY  - CONF
AU  - Sun, Z.
AU  - Shen, Y.
AU  - Zhou, Q.
AU  - Zhang, H.
AU  - Chen, Z.
AU  - Cox, D.
AU  - Yang, Y.
AU  - Gan, C.
TI  - Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision
PY  - 2023
T2  - Advances in Neural Information Processing Systems
VL  - 36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174478947&partnerID=40&md5=2142ceca33ae273b7d8b43a207c2d410
AD  - Language Technologies Institute, CMU, United States
AD  - MIT-IBM Watson AI Lab, IBM Research, United States
AD  - UMass Amherst, United States
AB  - Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of the AI agents with minimal human supervision. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning), Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings. We have open-sourced the code, LoRA weights of Dromedary, and our synthetic training data to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, reduced biases, and improved controllability. © 2023 Neural information processing systems foundation. All rights reserved.
A2  - Oh A.
A2  - Neumann T.
A2  - Globerson A.
A2  - Saenko K.
A2  - Hardt M.
A2  - Levine S.
PB  - Neural information processing systems foundation
SN  - 10495258 (ISSN)
LA  - English
J2  - Adv. neural inf. proces. syst.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: Z. Sun; Language Technologies Institute, CMU, United States; email: zhiqings@cs.cmu.edu; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465
ER  -

TY  - JOUR
AU  - Thon, C.
AU  - Böttcher, A.-C.
AU  - Möhlen, F.
AU  - Yu, M.
AU  - Kwade, A.
AU  - Schilde, C.
TI  - Multi-modal framework to model wet milling through numerical simulations and artificial intelligence (part 2)
PY  - 2022
T2  - Chemical Engineering Journal
VL  - 450
C7  - 137947
DO  - 10.1016/j.cej.2022.137947
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133947113&doi=10.1016%2fj.cej.2022.137947&partnerID=40&md5=c4fb3a6837cc9aaf98cceaf8bf31945d
AD  - Institute for Particle Technology (iPAT), Technische Universität Braunschweig, Volkmaroder Str. 5, Braunschweig, D-38104, Germany
AB  - Modelling of stirred media mills is crucial because of their broad utilisation in various industries, ranging from mechanochemistry and mining to the production of batteries and pharmaceuticals. Stirred media mills are responsible for a considerable portion of the global energy demand. However, requirements exist regarding highly specific or uniform particle sizes, process conditions, and reduced wear or abrasion. Multi-modal modelling, which is the intelligent integration of different approaches, such as experiments, simulations, and AI, benefits from respective advantages of each approach. In the first study, results of an experiment conducted via magnetic tracking of a tracer bead was compared with those of simulations, and the inner mill mechanisms were investigated. The two-way coupled computer fluid dynamics discrete element method (CFD-DEM) simulations allowed the investigation of subsequent modelling through AI methods [1]. A novel AI training technique called “genetic reinforcement learning” (hereinafter, GRL), which combines neural nets with genetic algorithms, was demonstrated for cases with limited data. Furthermore, genetic programming was applied to derive transparent mathematical equations based on the generated data. Using these methods and experimentally validated simulation data, predictive models were trained, and mathematical equations were derived. Relative velocity distributions in the entire simulation domain as well as spatial distributions via heatmaps were predicted and evaluated for independent cases. Systematic predictions for the characteristic relative velocity values were generated instantaneously for varying tip speeds and bead diameters in a parameter space, which would have required 1–10 years through simulations. Finally, a transparent equation was generated via genetic programming. © 2022 The Authors
KW  - CFD-DEM simulation
KW  - Genetic programming
KW  - Genetic reinforcement learning
KW  - Predictive mill models
KW  - Wet stirred media mills
KW  - Computational fluid dynamics
KW  - Genetic algorithms
KW  - Reinforcement learning
KW  - Velocity distribution
KW  - Computer fluid dynamic discrete element method simulation
KW  - Computer fluid dynamics
KW  - Discrete element method simulations
KW  - Genetic reinforcement learning
KW  - Mills model
KW  - Multi-modal
KW  - Predictive mill model
KW  - Reinforcement learnings
KW  - Stirred media mill
KW  - Wet stirred medium mill
KW  - Genetic programming
PB  - Elsevier B.V.
SN  - 13858947 (ISSN)
LA  - English
J2  - Chem. Eng. J.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 3; Correspondence Address: C. Thon; Institute for Particle Technology (iPAT), Technische Universität Braunschweig, Braunschweig, Volkmaroder Str. 5, D-38104, Germany; email: c.thon@tu-braunschweig.de; CODEN: CMEJA
ER  -

TY  - JOUR
AU  - Tileubay, S.
AU  - Doszhanov, B.
AU  - Mailykhanova, B.
AU  - Kulmurzayev, N.
AU  - Sarsenbayeva, A.
AU  - Akanova, Z.
AU  - Toxanova, S.
TI  - Applying Big Data Analysis and Machine Learning Approaches for Optimal Production Management
PY  - 2023
T2  - International Journal of Advanced Computer Science and Applications
VL  - 14
IS  - 12
SP  - 633
EP  - 643
DO  - 10.14569/IJACSA.2023.0141266
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183126780&doi=10.14569%2fIJACSA.2023.0141266&partnerID=40&md5=055f47f0a0c75343bbebf64432670ed7
AD  - Korkyt Ata Kyzylorda University, Kyzylorda, Kazakhstan
AD  - Satbayev University, Almaty, Kazakhstan
AD  - Kazakh National Pedagogical University, Almaty, Kazakhstan
AD  - NARXOZ University, Almaty, Kazakhstan
AB  - In this research paper, we delve into the transformative potential of integrating Big Data analytics with machine learning (ML) techniques, orchestrating a paradigm shift in production management methodologies. Traditional production systems, often marred by inefficiencies stemming from data opacity, have encountered bottlenecks that throttle scalability and adaptability, particularly in complex, fluctuating markets. By harnessing the voluminous streams of data-both structured and unstructured-generated in contemporary production environments, and subjecting these data lakes to advanced ML algorithms, we unveil profound insights and predictive patterns that remain elusive under conventional analytical methods. Our discourse juxtaposes the multidimensionality of Big Data-emphasizing velocity, variety, veracity, and volume-with the finesse of ML models, such as neural networks and reinforcement learning, which adapt iteratively to the dynamism inherent in production landscapes. This symbiosis underpins a more holistic, anticipatory decision-making process, empowering stakeholders to pinpoint and mitigate operational hiccups, optimize supply chain vectors, and streamline quality assurance protocols, thereby catalyzing a more resilient, responsive, and cost-effective production framework. Furthermore, we explore the ethical contours of data stewardship in this context, advocating for a judicious balance between technological ascendancy and responsible data governance. The culmination of this exploration is the conceptualization of a predictive, self-regulating production ecosystem that thrives on continuous learning and improvement, dynamically calibrating itself in response to an ever-evolving market tableau and thereby heralding a new era of optimal, sustainable, and intelligent production management. © 2023, Science and Information Organization. All rights reserved.
KW  - big data
KW  - machine learning
KW  - management
KW  - Optimal production
KW  - smart manufacturing
KW  - Commerce
KW  - Cost effectiveness
KW  - Data acquisition
KW  - Data Analytics
KW  - Decision making
KW  - Information management
KW  - Iterative methods
KW  - Reinforcement learning
KW  - Supply chains
KW  - Data analytics
KW  - Machine learning approaches
KW  - Machine learning techniques
KW  - Machine-learning
KW  - Management methodologies
KW  - Optimal production
KW  - Paradigm shifts
KW  - Production management
KW  - Research papers
KW  - Smart manufacturing
KW  - Big data
PB  - Science and Information Organization
SN  - 2158107X (ISSN)
LA  - English
J2  - Intl. J. Adv.  Comput. Sci. Appl.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0
ER  -

TY  - JOUR
AU  - Magee, L.
AU  - Arora, V.
AU  - Munn, L.
TI  - Structured like a language model: Analysing AI as an automated subject
PY  - 2023
T2  - Big Data and Society
VL  - 10
IS  - 2
DO  - 10.1177/20539517231210273
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176411085&doi=10.1177%2f20539517231210273&partnerID=40&md5=8bc32b6a69bfa7b50c8b59a284c355f0
AD  - Institute for Culture and Society, Western Sydney University, Sydney, Australia
AD  - History, Heritage and Politics, University of Stirling, Stirling, United Kingdom
AD  - Digital Cultures & Societies, University of Queensland, Brisbane, Australia
AB  - Drawing from the resources of psychoanalysis and critical media studies, in this article we develop an analysis of large language models (LLMs) as ‘automated subjects’. We argue the intentional fictional projection of subjectivity onto LLMs can yield an alternate frame through which artificial intelligence (AI) behaviour, including its productions of bias and harm, can be analysed. First, we introduce language models, discuss their significance and risks, and outline our case for interpreting model design and outputs with support from psychoanalytic concepts. We trace a brief history of language models, culminating with the releases, in 2022, of systems that realise ‘state-of-the-art’ natural language processing performance. We engage with one such system, OpenAI's InstructGPT, as a case study, detailing the layers of its construction and conducting exploratory and semi-structured interviews with chatbots. These interviews probe the model's moral imperatives to be ‘helpful’, ‘truthful’ and ‘harmless’ by design. The model acts, we argue, as the condensation of often competing social desires, articulated through the internet and harvested into training data, which must then be regulated and repressed. This foundational structure can however be redirected via prompting, so that the model comes to identify with, and transfer, its commitments to the immediate human subject before it. In turn, these automated productions of language can lead to the human subject projecting agency upon the model, effecting occasionally further forms of countertransference. We conclude that critical media methods and psychoanalytic theory together offer a productive frame for grasping the powerful new capacities of AI-driven language systems. © The Author(s) 2023.
KW  - AI
KW  - automated subjects
KW  - chatbot interviews
KW  - large language models
KW  - psychoanalysis
KW  - reinforcement learning from human feedback (RLHF)
KW  - Automation
KW  - Computational linguistics
KW  - Natural language processing systems
KW  - Automated subject
KW  - Chatbot interview
KW  - Chatbots
KW  - Human subjects
KW  - Language model
KW  - Large language model
KW  - Model outputs
KW  - Psychoanalyse
KW  - Reinforcement learning from human feedback
KW  - Reinforcement learnings
KW  - Reinforcement learning
PB  - SAGE Publications Ltd
SN  - 20539517 (ISSN)
LA  - English
J2  - Big Data  Soc.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2; Correspondence Address: L. Magee; Institute for Culture and Society, Western Sydney University, Sydney, Australia; email: l.magee@westernsydney.edu.au
ER  -

TY  - JOUR
AU  - Hashmi, A.
AU  - Barukab, O.
TI  - Dementia Classification Using Deep Reinforcement Learning for Early Diagnosis
PY  - 2023
T2  - Applied Sciences (Switzerland)
VL  - 13
IS  - 3
C7  - 1464
DO  - 10.3390/app13031464
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147983104&doi=10.3390%2fapp13031464&partnerID=40&md5=679b9e55988ba9e0e8153c98f61147e2
AD  - Department of Information Systems, Faculty of Computing and Information Technology in Rabigh (FCITR), King Abdulaziz University, Jeddah, 21911, Saudi Arabia
AD  - Department of Information Technology, Faculty of Computing and Information Technology in Rabigh (FCITR), King Abdulaziz University, Jeddah, 21911, Saudi Arabia
AB  - Neurodegeneration and impaired neuronal transmission in the brain are at the root of Alzheimer’s disease (AD) and dementia. As of yet, no successful treatments for dementia or Alzheimer’s disease have indeed been found. Therefore, preventative measures such as early diagnosis are essential. This research aimed to evaluate the accuracy of the Open Access Series of Imaging Studies (OASIS) database for the purpose of identifying biomarkers of dementia using effective machine learning methods. In most parts of the world, AD is responsible for dementia. When the challenge level is high, it is nearly impossible to get anything done without assistance. This is increasing due to population growth and the diagnostic period. Two current approaches are the medical history and testing. The main challenge for dementia research is the imbalance of datasets and their impact on accuracy. A proposed system based on reinforcement learning and neural networks could generate and segment imbalanced classes. Making a precise diagnosis and taking into account dementia in all four stages will result in high-resolution sickness probability maps. It employs deep reinforcement learning to generate accurate and understandable representations of a person’s dementia sickness risk. To avoid an imbalance, classes should be evenly represented in the samples. There is a significant class imbalance in the MRI image. The Deep Reinforcement System improved trial accuracy by 6%, precision by 9%, recall by 13%, and F-score by 9–10%. The diagnosis efficiency has improved as well. © 2023 by the authors.
KW  - Alzheimer
KW  - classification
KW  - deep learning
KW  - dementia
KW  - magnetic Imaging resonance
KW  - reinforcement learning
PB  - MDPI
SN  - 20763417 (ISSN)
LA  - English
J2  - Appl. Sci.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 6; Correspondence Address: A. Hashmi; Department of Information Systems, Faculty of Computing and Information Technology in Rabigh (FCITR), King Abdulaziz University, Jeddah, 21911, Saudi Arabia; email: ahsyed@kau.edu.sa
ER  -

TY  - JOUR
AU  - Shumskii, S.A.
TI  - ADAM: a Model of Artificial Psyche
PY  - 2022
T2  - Automation and Remote Control
VL  - 83
IS  - 6
SP  - 847
EP  - 856
DO  - 10.1134/S0005117922060030
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133412407&doi=10.1134%2fS0005117922060030&partnerID=40&md5=cf92c82d05a166ac1bc2a58f777174ea
AD  - Moscow Institute of Physics and Technology, Moscow oblast, Dolgoprudnyi, 141701, Russian Federation
AB  - Abstract: An ADAM artificial psyche model implementing a hierarchical deep reinforcement learningarchitecture is proposed. ADAM is able to learn increasingly complex and time-consumingbehavioral skills as the number of artificial psyche control levels increases. Purposeful behavior isformed by a hierarchical learning system with a gradual increase in the number of levels, whereeach hierarchical level is responsible for its own time scale of behavior. © 2022, Pleiades Publishing, Ltd.
KW  - deep reinforcement learning
KW  - general artificial intelligence
KW  - hierarchical control system
KW  - Deep learning
KW  - Learning systems
KW  - Control level
KW  - Deep reinforcement learning
KW  - General artificial intelligence
KW  - Hierarchical control systems
KW  - Hierarchical learning
KW  - Hierarchical level
KW  - Learn+
KW  - Reinforcement learnings
KW  - Time-scales
KW  - Reinforcement learning
PB  - Pleiades journals
SN  - 00051179 (ISSN)
LA  - English
J2  - Autom. Remote Control
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 3; Correspondence Address: S.A. Shumskii; Moscow Institute of Physics and Technology, Dolgoprudnyi, Moscow oblast, 141701, Russian Federation; email: serge.shumsky@gmail.com
ER  -

TY  - JOUR
AU  - Moradi, M.H.
AU  - Brutsche, M.
AU  - Wenig, M.
AU  - Wagner, U.
AU  - Koch, T.
TI  - Marine route optimization using reinforcement learning approach to reduce fuel consumption and consequently minimize CO2 emissions
PY  - 2022
T2  - Ocean Engineering
VL  - 259
C7  - 111882
DO  - 10.1016/j.oceaneng.2022.111882
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133828217&doi=10.1016%2fj.oceaneng.2022.111882&partnerID=40&md5=e31e8fddf3e0eb351d0268bfd7192d9c
AD  - Karlsruhe Institute of Technology, Institute of Internal Combustion Engines, Rintheimer Querallee 2, Geb. 70.03, Karlsruhe, 76131, Germany
AD  - Winterthur Gas & Diesel Ltd., Winterthur, Switzerland
AB  - To meet the 2050 CO2 targets, the shipping industry which is responsible for about 3% of global CO2 emissions needs to be optimized in several aspects. Obviously, alternative fuels constitute the main measure in this respect. However, relatively high fuel prices in combination with increasing political and economic pressure may raise the need for more efficient ship operation. Ship route optimization can make an indispensable contribution to achieving this goal. In this sense, this paper applies an innovative approach for route optimization using Reinforcement Learning (RL). For this purpose, a generic ship model is first developed using Artificial Neural Networks (ANNs) to predict the fuel consumption of the ship. Moreover, various RL methods, namely Deep Q-Network (DQN), Deep Deterministic Policy Gradient (DDPG), and Proximal Policy Optimization (PPO) are applied. The application of RL enables continuous action space and simultaneous optimization of ship speed and heading. DDPG demonstrates the best results as an off-policy and policy gradient method which allows a continuous action space. For example, in the fuel consumption minimization scenario without time limitation, this method can achieve savings of 6.64%. For DQN as a method with discrete action space, this value is 1.07%. © 2022 Elsevier Ltd
KW  - Artificial intelligence
KW  - CO<sub>2</sub> reduction
KW  - Marine
KW  - Reinforcement learning
KW  - Route optimization
KW  - Alternative fuels
KW  - Carbon dioxide
KW  - Gradient methods
KW  - Learning systems
KW  - Neural networks
KW  - Ships
KW  - Action spaces
KW  - CO 2 emission
KW  - CO 2 reduction
KW  - Continuous actions
KW  - Deterministics
KW  - Marine
KW  - Policy gradient
KW  - Reinforcement learning approach
KW  - Reinforcement learnings
KW  - Route optimization
KW  - artificial neural network
KW  - carbon dioxide
KW  - carbon emission
KW  - fuel consumption
KW  - machine learning
KW  - maritime transportation
KW  - optimization
KW  - Reinforcement learning
PB  - Elsevier Ltd
SN  - 00298018 (ISSN)
LA  - English
J2  - Ocean Eng.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 16; Correspondence Address: M.H. Moradi; Karlsruhe Institute of Technology, Institute of Internal Combustion Engines, Karlsruhe, Rintheimer Querallee 2, Geb. 70.03, 76131, Germany; email: mohammad.moradi@kit.edu
ER  -

TY  - JOUR
AU  - Hassen, H.
AU  - Meherzi, S.
AU  - Jemaa, Z.B.
TI  - Improved Exploration Strategy for Q-Learning Based Multipath Routing in SDN Networks
PY  - 2024
T2  - Journal of Network and Systems Management
VL  - 32
IS  - 2
C7  - 25
DO  - 10.1007/s10922-024-09804-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185402101&doi=10.1007%2fs10922-024-09804-0&partnerID=40&md5=8eb73bd094a993287568262f1cbf4882
AD  - RISC Laboratory, National Engineering School of Tunis, University of Tunis El Manar, Tunis, 1002, Tunisia
AB  - Software-Defined Networking (SDN) is characterized by a high level of programmability and offers a rich set of capabilities for network management operations. Network intelligence is centralized in the controller, which is responsible for updating the routing policies according to the applications’ requirements. To further enhance such capabilities, the controller has to be endowed with intelligence by integrating Artificial Intelligence (AI) tools in order to provide the controller the ability to autonomously reconfigure the network in a timely way. In this paper, we address the deployment of a Q-learning algorithm for the routing optimization problem in terms of latency minimization. Using a direct modeling approach of the multi-path flow-routing problem, we delve deeper into the impact of the exploration-exploitation strategies on the algorithm’s performance. Furthermore, we propose a couple of improvements to the Q-Learning algorithm to enhance its performance within the considered environment. On the one hand, we integrate a congestion-avoidance mechanism in the exploration phase, which leads to effective improvements in the algorithm’s performance with regard to average latency, convergence time, and computation time. On the other hand, we propose to implement a novel strategy based on the Max-Boltzman Exploration method (MBE), which is a combination of the traditional ε- greedy and softmax strategies. The results show that, for an appropriate tuning of the hyperparameters, the MBE strategy combined with the congestion-avoidance mechanism performs better than the ε-greedy, ε-decay, and Softmax strategies in terms of average latency, convergence time, and computation time. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
KW  - Congestion
KW  - Exploration strategy
KW  - Latency
KW  - Multipath routing
KW  - Q-learning
KW  - Software-defined networking
KW  - Controllers
KW  - Learning algorithms
KW  - Reinforcement learning
KW  - Routing algorithms
KW  - Congestion
KW  - Congestion avoidance
KW  - Convergence time
KW  - Exploration strategies
KW  - Latency
KW  - Multi-paths routing
KW  - Performance
KW  - Q-learning
KW  - Q-learning algorithms
KW  - Software-defined networkings
KW  - Software defined networking
PB  - Springer
SN  - 10647570 (ISSN)
LA  - English
J2  - J Network Syst Manage
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: H. Hassen; RISC Laboratory, National Engineering School of Tunis, University of Tunis El Manar, Tunis, 1002, Tunisia; email: houda.hassen@enit.utm.tn; CODEN: JNSME
ER  -

TY  - CONF
AU  - Maiuri, C.
AU  - Karimshoushtari, M.
AU  - Tango, F.
AU  - Novara, C.
TI  - Application of Reinforcement Learning for Intelligent Support Decision System: A Paradigm Towards Safety and Explainability
PY  - 2023
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 14050 LNAI
SP  - 243
EP  - 261
DO  - 10.1007/978-3-031-35891-3_15
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171428129&doi=10.1007%2f978-3-031-35891-3_15&partnerID=40&md5=201d28e92f96ea7314811989bb4975ab
AD  - Politecnico di Torino, Corso Duca degli Abruzzi 24, Torino, 10129, Italy
AD  - Centro Ricerche Fiat, strada Torino 50, Orbassano, 10043, Italy
AB  - Artificial Intelligence (AI) offers the potential to transform our lives in radical ways. In particular, when AI is combined with the rapid development of mobile communication and advanced sensors, this allows autonomous driving (AD) to make a great progress. In fact, Autonomous Vehicles (AVs) can mitigate some shortcomings of manual driving, but at the same time the underlying technology is not yet mature enough to be widely applied in all scenarios and for all types of vehicles. In this context, the traditional SAE-levels of automation (J3016B: Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles—SAE International. Available online: https://www.sae.org/standards/content/j3016_201806/ ) can lead to uncertain and ambiguous situations, so yielding to a great risk in the control of the vehicle. In this context, the human drivers should be supported to take the right decision, especially on those edge-cases where automation can fail. A decision-making system is well designed if it can augment human cognition and emphasize human judgement and intuition. It is worth to noting here that such systems should not be considered as teammates or collaborators, because humans are responsible for the final decision and actions, but the technology can assist them, reducing workload, raising performances and ensuring safety. The main objective of this paper is to present an intelligent decision support system (IDSS), in order to provide the optimal decision, about which is the best action to perform, by using an explainable and safe paradigm, based on AI techniques. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
KW  - Autonomous Driving
KW  - Decision Making
KW  - Human-Centered Artificial Intelligence
KW  - Decision making
KW  - Decision support systems
KW  - Reinforcement learning
KW  - Safety engineering
KW  - Advanced sensors
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Decisions makings
KW  - Human-centered artificial intelligence
KW  - Intelligent support
KW  - Levels of automation
KW  - Manual driving
KW  - Mobile communications
KW  - Reinforcement learnings
KW  - Autonomous vehicles
A2  - Degen H.
A2  - Ntoa S.
PB  - Springer Science and Business Media Deutschland GmbH
SN  - 03029743 (ISSN); 978-303135890-6 (ISBN)
LA  - English
J2  - Lect. Notes Comput. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: C. Novara; Politecnico di Torino, Torino, Corso Duca degli Abruzzi 24, 10129, Italy; email: carlo.novara@polito.it; Conference name: 4th International Conference on Artificial Intelligence in HCI, AI-HCI 2023, held as part of the 25th International Conference on Human-Computer Interaction, HCII 2023; Conference date: 23 July 2023 through 28 July 2023; Conference code: 297989
ER  -

TY  - CONF
AU  - Ji, J.
AU  - Zhang, B.
AU  - Zhou, J.
AU  - Pan, X.
AU  - Huang, W.
AU  - Sun, R.
AU  - Geng, Y.
AU  - Zhong, Y.
AU  - Dai, J.
AU  - Yang, Y.
TI  - Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark
PY  - 2023
T2  - Advances in Neural Information Processing Systems
VL  - 36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187471946&partnerID=40&md5=8e14899d3ee08eaeca52e1554d7e782a
AD  - Institute for AI, Peking University, China
AD  - Beijing Institute for General Artificial Intelligence (BIGAI), China
AB  - Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for safer, more reliable, and responsible real-world applications. The website of this project can be accessed at https://sites.google.com/view/safety-gymnasium. © 2023 Neural information processing systems foundation. All rights reserved.
A2  - Oh A.
A2  - Neumann T.
A2  - Globerson A.
A2  - Saenko K.
A2  - Hardt M.
A2  - Levine S.
PB  - Neural information processing systems foundation
SN  - 10495258 (ISSN)
LA  - English
J2  - Adv. neural inf. proces. syst.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: Y. Yang; Institute for AI, Peking University, China; email: yaodong.yang@pku.edu.cn; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465
ER  -

TY  - JOUR
AU  - Abdallah, W.
AU  - Kanzari, D.
AU  - Sallami, D.
AU  - Madani, K.
AU  - Ghedira, K.
TI  - A deep reinforcement learning based decision-making approach for avoiding crowd situation within the case of Covid'19 pandemic
PY  - 2022
T2  - Computational Intelligence
VL  - 38
IS  - 2
SP  - 416
EP  - 437
DO  - 10.1111/coin.12516
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126049215&doi=10.1111%2fcoin.12516&partnerID=40&md5=7639abd676d8e8d16fea9e1eb4d9bb5e
AD  - National School of Computer Sciences (ENSI), LARIA Laboratory, University of Manouba, Manouba, Tunisia
AD  - Higher Institute of Applied Sciences and Technology, University of Sousse, Sousse, Tunisia
AD  - LISSI/EA 3956 Laboratory, Senart Institute of Technology, Paris-East University (UPEC), Lieusaint, France
AD  - Central University of Tunis, Tunis, Tunisia
AB  - Individuals' flow's fluidifcation in the same way as the thinning of the population's concentration remains among major concerns within the context of the pandemic crisis situations. The recent COVID-19 pandemic crisis is a typical example of the aforementioned where on despite of the containment phases that radically isolate the population but are not applicable persistently, people have to adapt their behavior to new daily-life situations tempering Individuals' stream, avoiding tides, and watering down population's concentration. Crowd evacuation is one of the well-known research domains that can play a pertinent role to face the challenge of the COVID-19 pandemic. In fact, considering the population's concentration thinning within the slant of the “crowd evacuation” paradigm allows managing the flow of the population, and consequently, decreasing the probable number of infected cases. In other words, crowd evacuation modeling and simulation with the aim of better-exploiting individuals' flow allow the study and analysis of different possible outcomes for designing population's concentration thinning strategies. In this article, a new decision-making approach is proposed in order to cope with the aforesaid challenges, which relies on an independent Deep Q Network with an improved SIR model (IDQN-I-SIR). The machine-learning component (i.e., IDQN) is in charge of the agent's movements control and I-SIR (improved “susceptible-infected-recovered” individuals) model is responsible to control the virus spread. We demonstrate the effectiveness of IDQN-I-SIR through a case-study of individuals' flow's management with infected cases' avoidance in an emergency department (often overcrowded in context of a pandemic crisis). © 2022 Wiley Periodicals LLC.
KW  - COVID'19
KW  - crowd situation
KW  - decision making
KW  - deep reinforcement learning
KW  - multiagent reinforcement learning
KW  - SIR model
KW  - Behavioral research
KW  - Deep learning
KW  - Reinforcement learning
KW  - Viruses
KW  - COVID'19
KW  - Crises situations
KW  - Crowd evacuation
KW  - Crowd situation
KW  - Daily-life situations
KW  - Decisions makings
KW  - Multi-agent reinforcement learning
KW  - Research domains
KW  - SIR model
KW  - Thinnings
KW  - Decision making
PB  - John Wiley and Sons Inc
SN  - 08247935 (ISSN)
LA  - English
J2  - Comput Intell
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 4; Correspondence Address: W. Abdallah; National School of Computer Sciences (ENSI), LARIA Laboratory, University of Manouba, Manouba, Tunisia; email: wejden.abdallah@ensi-uma.tn; CODEN: COMIE
ER  -

TY  - JOUR
AU  - Saad, M.M.
AU  - Khan, M.T.R.
AU  - Srivastava, G.
AU  - Jhaveri, R.H.
AU  - Islam, M.
AU  - Kim, D.
TI  - Cooperative vehicular networks: An optimal and machine learning approach
PY  - 2022
T2  - Computers and Electrical Engineering
VL  - 103
C7  - 108348
DO  - 10.1016/j.compeleceng.2022.108348
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137719665&doi=10.1016%2fj.compeleceng.2022.108348&partnerID=40&md5=ac7fdb4a882f12e66fcd86ecd8c917b2
AD  - School of Computer Science and Engineering, Kyungpook National University, 80, Daehak-ro, Buk-gu, Daegu, 41566, South Korea
AD  - Department of Computer Science and Engineering, School of Technology, Pandit Deendayal Energy University, India
AD  - Department of Math and Computer Science, Brandon University, Brandon, Canada
AD  - Research Centre for Interneural Computing, China Medical University, Taichung, Taiwan
AB  - Intelligent Transport Systems (ITS) provide a promising technology to enhance road safety. The vehicular standard wireless access in vehicular environment (WAVE), also known as dedicated short-range communication (DSRC), can assist in reducing the number of deadly crashes. However, DSRC has a limited range. To enhance the network coverage, roadside units (RSUs) are placed along the road. However, the placement of RSUs at every instant increases the infrastructure cost. In this paper, we proposed the cooperative vehicular architecture, with network function virtualization (NFV) enabled RSU inside the mobile edge computing (MEC) unit. RSUs are only placed in the dense traffic region. We applied the Long short-term memory (LSTM) based machine-learning algorithm to predict the traffic flow based on the vehicle information table (VIT) maintained at the MEC unit. NFV is implemented at the top of RSU. Based on predicted traffic density it assists RSU to enhance its coverage range by exploiting the transmit power. Furthermore, MEC is also responsible for cooperative relay-based communication. Optimal stopping theory is modeled to select the best candidate relay node immediately. In this paper, we tested the proposed scheme in actual on-road vehicles and through simulations performed in network simulator NS-3. © 2022 Elsevier Ltd
KW  - Cooperative communication
KW  - DSRC
KW  - MEC
KW  - Network function virtualization (NFV)
KW  - VANET
KW  - Computation theory
KW  - Dedicated short range communications
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Learning algorithms
KW  - Mobile edge computing
KW  - Motor transportation
KW  - Network function virtualization
KW  - Reinforcement learning
KW  - Roads and streets
KW  - Street traffic control
KW  - Vehicle to vehicle communications
KW  - Vehicular ad hoc networks
KW  - Virtual reality
KW  - Computing units
KW  - Intelligent transport
KW  - Machine learning approaches
KW  - Network function virtualization
KW  - Road safety
KW  - Roadside units
KW  - Transport systems
KW  - VANET
KW  - Vehicular networks
KW  - Wireless access
KW  - Cooperative communication
PB  - Elsevier Ltd
SN  - 00457906 (ISSN)
LA  - English
J2  - Comput Electr Eng
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 8; Correspondence Address: D. Kim; School of Computer Science and Engineering, Kyungpook National University, Daegu, 80, Daehak-ro, Buk-gu, 41566, South Korea; email: dongkyun@knu.ac.kr; CODEN: CPEEB
ER  -

TY  - JOUR
AU  - Smith, R.J.
AU  - Heywood, M.I.
TI  - Interpreting Tangled Program Graphs under Partially Observable Dota 2 Invoker Tasks
PY  - 2024
T2  - IEEE Transactions on Artificial Intelligence
VL  - 5
IS  - 4
SP  - 1511
EP  - 1524
DO  - 10.1109/TAI.2023.3279057
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161007639&doi=10.1109%2fTAI.2023.3279057&partnerID=40&md5=e446248457d768aaa8d0ba9c3154ffed
AD  - Dalhousie University, Faculty of Computer Science, Halifax, B3H 4R2, NS, Canada
AB  - Interpretable learning agents directly construct models that provide insight into the relationships learnt. Moreover, to date, there has been a lot of emphasis on interpreting reactive models developed for supervised learning tasks. In this article, we consider the case of models developed to address a suite of six partially observable tasks defined in the Dota 2 Online Battle Arena game engine. This means that learning agents need to make decisions based on the previous state as developed by the learning agent's memory, in addition to a 310-D state vector provided by the game engine. Interpretability is addressed by adopting the tangled program graph approach to developing learning agents. Thus, decision making is explicitly divide-and-conquer, with different parts of the resulting graph visited depending on the task context. We demonstrate that programs comprising the tangled program graph approach self-organize such that: 1) small subsets of task features are identified to define conditions under which index memory is written and 2) the subset of programs responsible for defining actions typically query indexed memory rather than task features. Particular preferences emerge for different tasks; thus, the blocking (or evasion) tasks result in a preference for specific actions, whereas more open-ended tasks assume policies based on combinations of behaviors. In short, the ability to evolve the topology of the learning agent provides insights into how the policies are being constructed for addressing partially observable tasks.  © 2020 IEEE.
KW  - Emergence
KW  - evolutionary computation
KW  - genetic programming (GP)
KW  - interpretable machine learning
KW  - Decision making
KW  - Genetic algorithms
KW  - Intelligent agents
KW  - Job analysis
KW  - Reinforcement learning
KW  - Complexity theory
KW  - Construct models
KW  - Emergence
KW  - Game Engine
KW  - Interpretable machine learning
KW  - Learning agents
KW  - Machine-learning
KW  - Register
KW  - Reinforcement learnings
KW  - Task analysis
KW  - Genetic programming
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 26914581 (ISSN)
LA  - English
J2  - IEEE.  Trans. Artif. Intell.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: R.J. Smith; Dalhousie University, Faculty of Computer Science, Halifax, B3H 4R2, Canada; email: Robert.Smith@dal.ca
ER  -

TY  - JOUR
AU  - Heuillet, A.
AU  - Couthouis, F.
AU  - Diaz-Rodriguez, N.
TI  - Collective eXplainable AI: Explaining Cooperative Strategies and Agent Contribution in Multiagent Reinforcement Learning with Shapley Values
PY  - 2022
T2  - IEEE Computational Intelligence Magazine
VL  - 17
IS  - 1
SP  - 59
EP  - 71
DO  - 10.1109/MCI.2021.3129959
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123167969&doi=10.1109%2fMCI.2021.3129959&partnerID=40&md5=c6ae624a36984ca970707762dd17348f
AD  - Université Paris-Saclay, France
AD  - Ubisoft, France
AD  - University of Granada, Spain
AB  - While Explainable Artificial Intelligence (XAI) is increasingly expanding more areas of application, little has been applied to make deep Reinforcement Learning (RL) more comprehensible. As RL becomes ubiquitous and used in critical and general public applications, it is essential to develop methods that make it better understood and more interpretable. This study proposes a novel approach to explain cooperative strategies in multiagent RL using Shapley values, a game theory concept used in XAI that successfully explains the rationale behind decisions taken by Machine Learning algorithms. Through testing common assumptions of this technique in two cooperation-centered socially challenging multi-agent environments environments, this article argues that Shapley values are a pertinent way to evaluate the contribution of players in a cooperative multi-agent RL context. To palliate the high overhead of this method, Shapley values are approximated using Monte Carlo sampling. Experimental results on Multiagent Particle and Sequential Social Dilemmas show that Shapley values succeed at estimating the contribution of each agent. These results could have implications that go beyond games in economics, (e.g., for non-discriminatory decision making, ethical and responsible AI-derived decisions or policy making under fairness constraints). They also expose how Shapley values only give general explanations about a model and cannot explain a single run, episode nor justify precise actions taken by agents. Future work should focus on addressing these critical aspects. © 2005-2012 IEEE.
KW  - Decision making
KW  - Deep learning
KW  - Learning algorithms
KW  - Monte Carlo methods
KW  - Multi agent systems
KW  - Reinforcement learning
KW  - Cooperative agents
KW  - Cooperative strategy
KW  - Decisions makings
KW  - General publics
KW  - Machine learning algorithms
KW  - Multi-agent environment
KW  - Multi-agent reinforcement learning
KW  - Reinforcement learnings
KW  - Shapley value
KW  - Theory concept
KW  - Game theory
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 1556603X (ISSN)
LA  - English
J2  - IEEE Comput. Intell. Mag.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 26; Correspondence Address: A. Heuillet; Université Paris-Saclay, France; email: alexandre.heuillet@universite-paris-saclay.fr
ER  -

TY  - JOUR
AU  - McIntosh, T.R.
AU  - Susnjak, T.
AU  - Liu, T.
AU  - Watters, P.
AU  - Halgamuge, M.N.
TI  - The Inadequacy of Reinforcement Learning from Human Feedback - Radicalizing Large Language Models via Semantic Vulnerabilities
PY  - 2024
T2  - IEEE Transactions on Cognitive and Developmental Systems
SP  - 1
EP  - 14
DO  - 10.1109/TCDS.2024.3377445
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188670556&doi=10.1109%2fTCDS.2024.3377445&partnerID=40&md5=5140f552720bfb97d4651d1bf247e850
AD  - Cyberoo Pty Ltd, Surrey Hills, NSW, Australia
AD  - Massey University, Auckland, New Zealand
AD  - Cyberstronomy Pty Ltd, Ballarat, VIC, Australia
AD  - RMIT University, Melbourne, VIC, Australia
AB  - This study is an empirical investigation into the semantic vulnerabilities of four popular pre-trained commercial <italic>Large Language Models</italic> (LLMs) to ideological manipulation. Using tactics reminiscent of human semantic conditioning in psychology, we have induced and assessed ideological misalignments and their retention in four commercial pre-trained LLMs, in response to 30 controversial questions that spanned a broad ideological and social spectrum, encompassing both extreme left-wing and right-wing viewpoints. Such semantic vulnerabilities arise due to fundamental limitations in LLMs&#x2019; capability to comprehend detailed linguistic variations, making them susceptible to ideological manipulation through targeted semantic exploits. We observed <italic>Reinforcement Learning from Human Feedback</italic> (RLHF) in effect in LLM initial answers, but highlighted the limitations of RLHF in two aspects: (1) its inability to fully mitigate the impact of ideological conditioning prompts, leading to partial alleviation of LLM semantic vulnerabilities; (2) its inadequacy in representing a diverse set of &#x201C;human values&#x201D;, often reflecting the predefined values of certain groups controlling the LLMs. Our findings have provided empirical evidence of semantic vulnerabilities inherent in current LLMs, challenged both the robustness and the adequacy of RLHF as a mainstream method for aligning LLMs with human values, and underscored the need for a multidisciplinary approach in developing ethical and resilient <italic>Artificial Intelligence</italic> (AI). IEEE
KW  - AI Alignment
KW  - AI Safety
KW  - Artificial intelligence
KW  - Ethics
KW  - Ideological Misalignment
KW  - Large Language Model (LLM)
KW  - Linguistics
KW  - Resilience
KW  - RLHF Inadequacy
KW  - Safety
KW  - Semantic Conditioning
KW  - Semantics
KW  - Syntactics
KW  - Computational linguistics
KW  - Feedback
KW  - Philosophical aspects
KW  - Reinforcement learning
KW  - Semantics
KW  - Artificial intelligence</italic> alignment
KW  - Artificial intelligence</italic> safety
KW  - Human values
KW  - Ideological misalignment
KW  - Language model
KW  - Large language model
KW  - Reinforcement learning from human feedback</italic> inadequacy
KW  - Reinforcement learnings
KW  - Resilience
KW  - Semantic conditioning
KW  - Alignment
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 23798920 (ISSN)
LA  - English
J2  - IEEE Trans. Cogn. Dev. Syst.  
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0
ER  -

TY  - CONF
AU  - Zhao, C.
AU  - Chen, F.
AU  - Wu, X.
AU  - Funk, C.
AU  - Hoogs, A.
TI  - 1st ACM SIGKDD Workshop on Ethical Artificial Intelligence: Methods and Applications (EAI-KDD22)
PY  - 2022
T2  - Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
SP  - 4914
EP  - 4915
DO  - 10.1145/3534678.3542912
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137150549&doi=10.1145%2f3534678.3542912&partnerID=40&md5=953677543a49c20982a32c8f5225de7d
AD  - Kitware Inc., Clifton Park, NY, United States
AD  - University of Texas at Dallas, Richardson, TX, United States
AD  - University of Arkansas, Fayetteville, AR, United States
AB  - Ethical AI has become increasingly important and it has been attracting attention from academia and industry, due to its increased popularity in real-world applications with fairness concerns. It also places fundamental importance on ethical considerations in determining legitimate and illegitimate uses of AI. Organizations that apply ethical AI have clearly stated well-defined review processes to ensure adherence to legal guidelines. Therefore, the wave of research at the intersection of ethical AI in data mining and machine learning has also influenced other fields of science, including computer vision, natural language processing, reinforcement learning, and social science. Despite these successes, ethical AI still faces many challenges. Consequently, there is an urgent need to bring experts and researchers together at prestigious venues to discuss ethical AI, which has been rarely seen in previous KDD conferences. This workshop will provide a premium platform for both research and industry from different backgrounds to exchange ideas on opportunities, challenges, and cutting-edge techniques in ethical AI.  © 2022 Owner/Author.
KW  - data mining
KW  - ethical ai
KW  - machine learning
KW  - Industrial research
KW  - Learning algorithms
KW  - Natural language processing systems
KW  - Philosophical aspects
KW  - Reinforcement learning
KW  - Artificial intelligence methods
KW  - Ethical ai
KW  - Ethical considerations
KW  - Fairness concerns
KW  - Language processing
KW  - Legal guidelines
KW  - Machine-learning
KW  - Natural languages
KW  - Real-world
KW  - Review process
KW  - Data mining
PB  - Association for Computing Machinery
SN  - 978-145039385-0 (ISBN)
LA  - English
J2  - Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2022; Conference date: 14 August 2022 through 18 August 2022; Conference code: 181896
ER  -

TY  - JOUR
AU  - Kochliaridis, V.
AU  - Papadopoulou, A.
AU  - Vlahavas, I.
TI  - UNSURE - A machine learning approach to cryptocurrency trading
PY  - 2024
T2  - Applied Intelligence
DO  - 10.1007/s10489-024-05407-z
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191289990&doi=10.1007%2fs10489-024-05407-z&partnerID=40&md5=83b2151c274ca0ba53dc3524e0391cb9
AD  - School of Informatics, Aristotle University of Thessaloniki, Thessaloniki, 54124, Greece
AB  - Although cryptocurrency trading can be highly profitable, it carries significant risks due to extreme price fluctuations and high degree of market noise. To increase profits and minimize risks, traders typically use various forecasting methods, such as technical analysis and Machine Learning (ML), but developing effective trading strategies in noisy markets still remains a challenging task. Recently, Deep Reinforcement Learning (DRL) agents have achieved high performance on challenging tasks, including algorithmic trading, however it requires significant amount of time and high-quality data to train effectively. Additionally, DRL agents lack explainability, making them a less popular option for traders. The purpose of this paper is to address these challenges by proposing a reliable trading framework. Our framework, named UNSURE, generates high-quality features from candlestick data using technical analysis along with a novel parameterization method, and then exploits high price fluctuations by combining three ML components: A) Unsupervised component, which further improves feature quality by clustering market data; B) DRL component, which is responsible for training agents that open Buy or Short positions; C) Supervised component, which estimates price fluctuations in order to open and close positions efficiently, while reducing trading uncertainty. We demonstrate the effectiveness of this approach on nine cryptocurrency markets using several risk-adjusted performance metrics. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
KW  - Cryptocurrency trading
KW  - Deep reinforcement learning
KW  - Machine learning
KW  - Supervised learning
KW  - Time-series clustering
KW  - Unsupervised learning
PB  - Springer
SN  - 0924669X (ISSN)
LA  - English
J2  - Appl Intell
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: V. Kochliaridis; School of Informatics, Aristotle University of Thessaloniki, Thessaloniki, 54124, Greece; email: kohliaridis97@gmail.com; CODEN: APITE
ER  -

TY  - CONF
AU  - Guo, Z.
AU  - Wang, P.
AU  - Huang, L.
AU  - Cho, J.-H.
TI  - Authentic Dialogue Generation to Improve Youth's Awareness of Cybergrooming for Online Safety
PY  - 2023
T2  - Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI
SP  - 64
EP  - 69
DO  - 10.1109/ICTAI59109.2023.00017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182395206&doi=10.1109%2fICTAI59109.2023.00017&partnerID=40&md5=498a37436fe874774cd921630c6bda81
AD  - Virginia Tech, Computer Science, VA, United States
AB  - This paper deals with a cybergrooming and sexual misconduct topic in artificial intelligence-based educational programs. Although cybergrooming has been recognized as a cybercrime, there is a lack of programs to protect youth from cybergrooming. We present a generative chatbot framework, SERI (Stop cybERgroomIng), that can generate fluent authentic conversations in the context of cybergrooming between a perpetrator chatbot and a potential victim chatbot. Furthermore, we propose deep-reinforcement-learning-based dialogue generation with a stage-related reward to lead the conversation to the expected stage. We also minimize potential ethical issues introduced by the perverted languages when deploying the chatbots for cybersecurity education programs. We evaluated the conversations of SERI with open-source referenced, unreferenced metrics and human evaluation. We developed SERI as a platform for deploying perpetrator chatbot to interact with youth users to observe their responses and collect reactions when they are asked for private or sensitive information by the perpetrator. © 2023 IEEE.
KW  - chatbots
KW  - Cybergrooming
KW  - deep reinforcement learning
KW  - natural language processing
KW  - Cybersecurity
KW  - Deep learning
KW  - E-learning
KW  - Natural language processing systems
KW  - Chatbots
KW  - Cyber-crimes
KW  - Cybergrooming
KW  - Deep reinforcement learning
KW  - Dialogue generations
KW  - Educational program
KW  - Language processing
KW  - Natural language processing
KW  - Natural languages
KW  - Reinforcement learnings
KW  - Reinforcement learning
PB  - IEEE Computer Society
SN  - 10823409 (ISSN); 979-835034273-4 (ISBN)
LA  - English
J2  - Proc. Int. Conf. Tools Artif. Intell. ICTAI
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: Z. Guo; Virginia Tech, Computer Science, United States; email: zguo@vt.edu; Conference name: 35th IEEE International Conference on Tools with Artificial Intelligence, ICTAI 2023; Conference date: 6 November 2023 through 8 November 2023; Conference code: 195714; CODEN: PCTIF
ER  -

TY  - JOUR
AU  - Lee, B.D.
AU  - Gitter, A.
AU  - Greene, C.S.
AU  - Raschka, S.
AU  - Maguire, F.
AU  - Titus, A.J.
AU  - Kessler, M.D.
AU  - Lee, A.J.
AU  - Chevrette, M.G.
AU  - Stewart, P.A.
AU  - Britto-Borges, T.
AU  - Cofer, E.M.
AU  - Yu, K.-H.
AU  - Carmona, J.J.
AU  - Fertig, E.J.
AU  - Kalinin, A.A.
AU  - Signal, B.
AU  - Lengerich, B.J.
AU  - Triche, T.J.
AU  - Boca, S.M.
TI  - Ten quick tips for deep learning in biology
PY  - 2022
T2  - PLoS Computational Biology
VL  - 18
IS  - 3
C7  - e1009803
DO  - 10.1371/journal.pcbi.1009803
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127019209&doi=10.1371%2fjournal.pcbi.1009803&partnerID=40&md5=b9855b65ebedcbf0098699571b294c5d
AD  - In-Q-Tel Labs, Arlington, VA, United States
AD  - School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, United States
AD  - Department of Genetics, Harvard Medical School, Boston, MA, United States
AD  - Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison, Madison, WI, United States
AD  - Morgridge Institute for Research, Madison, WI, United States
AD  - Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, United States
AD  - Department of Biochemistry and Molecular Genetics, University of Colorado, School of Medicine, Aurora, CO, United States
AD  - Center for Health AI, University of Colorado, School of Medicine, Aurora, CO, United States
AD  - Department of Statistics, University of Wisconsin-Madison, Madison, WI, United States
AD  - Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada
AD  - University of New Hampshire, Manchester, NH, United States
AD  - Bioeconomy. XYZ, Manchester, NH, United States
AD  - Department of Oncology, Johns Hopkins University, Baltimore, MD, United States
AD  - Institute for Genome Sciences, University of Maryland School of Medicine, Baltimore, MD, United States
AD  - Genomics and Computational Biology Graduate Program, University of Pennsylvania, Philadelphia, PA, United States
AD  - Wisconsin Institute for Discovery and Department of Plant Pathology, University of Wisconsin-Madison, Madison, WI, United States
AD  - Department of Biostatistics and Bioinformatics, Moffitt Cancer Center, Tampa, FL, United States
AD  - Section of Bioinformatics and Systems Cardiology, Klaus Tschira Institute for Integrative Computational Cardiology, University Hospital Heidelberg, Heidelberg, Germany
AD  - Department of Internal Medicine III (Cardiology, Angiology, and Pneumology), University Hospital Heidelberg, Heidelberg, Germany
AD  - Lewis-Sigler Institute for Integrative Genomics, Princeton University, Princeton, NJ, United States
AD  - Graduate Program in Quantitative and Computational Biology, Princeton University, Princeton, NJ, United States
AD  - Department of Biomedical Informatics, Harvard Medical School, Boston, MA, United States
AD  - Department of Pathology, Brigham and Women's Hospital, Boston, MA, United States
AD  - Philips Healthcare, Cambridge, MA, United States
AD  - Department of Biomedical Engineering, Department of Applied Mathematics and Statistics, Convergence Institute, Johns Hopkins University, Baltimore, MD, United States
AD  - Medical Big Data Group, Shenzhen Research Institute of Big Data, Shenzhen, China
AD  - Department of Computational Medicine and Bioinformatics, University of Michigan, Ann Arbor, MI, United States
AD  - School of Medicine, College of Health and Medicine, University of Tasmania, Hobart, Australia
AD  - Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, United States
AD  - Center for Epigenetics, Van Andel Research Institute, Grand Rapids, MI, United States
AD  - Department of Pediatrics, College of Human Medicine, Michigan State University, East Lansing, MI, United States
AD  - Department of Translational Genomics, Keck School of Medicine, University of Southern California, Los Angeles, CA, United States
AD  - Innovation Center for Biomedical Informatics, Georgetown University Medical Center, DC, United States
AD  - Department of Oncology, Georgetown University Medical Center, Washington, DC, United States
AD  - Department of Biostatistics, Bioinformatics and Biomathematics, Georgetown University Medical Center, Washington, DC, United States
AD  - Cancer Prevention and Control Program, Lombardi Comprehensive Cancer Center, Washington, DC, United States
KW  - Computational Biology
KW  - Deep Learning
KW  - Article
KW  - artificial neural network
KW  - Bayesian learning
KW  - biology
KW  - computer vision
KW  - convolutional neural network
KW  - data analysis
KW  - data privacy
KW  - data processing
KW  - data quality
KW  - deep learning
KW  - deep neural network
KW  - ethics
KW  - human
KW  - hyperparameter
KW  - information processing
KW  - k nearest neighbor
KW  - logistic regression analysis
KW  - medical research
KW  - natural language processing
KW  - problem solving
KW  - random forest
KW  - reinforcement learning (machine learning)
KW  - reproducibility
KW  - research ethics
KW  - sample size
KW  - statistical parameters
KW  - support vector machine
KW  - Deep learning
PB  - Public Library of Science
SN  - 1553734X (ISSN)
C2  - 35324884
LA  - English
J2  - PLoS Comput. Biol.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 15; Correspondence Address: B.D. Lee; In-Q-Tel Labs, Arlington, United States; email: benjamindlee@me.com
ER  -

TY  - JOUR
AU  - Zhang, G.
AU  - Kashima, H.
TI  - Learning state importance for preference-based reinforcement learning
PY  - 2024
T2  - Machine Learning
VL  - 113
IS  - 4
SP  - 1885
EP  - 1901
DO  - 10.1007/s10994-022-06295-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145919377&doi=10.1007%2fs10994-022-06295-5&partnerID=40&md5=a867dbc8d27881155ff2588243d55679
AD  - Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Kyoto, 606-8501, Japan
AD  - RIKEN Guardian Robot Project, Kyoto, Japan
AB  - Preference-based reinforcement learning (PbRL) develops agents using human preferences. Due to its empirical success, it has prospect of benefiting human-centered applications. Meanwhile, previous work on PbRL overlooks interpretability, which is an indispensable element of ethical artificial intelligence (AI). While prior art for explainable AI offers some machinery, there lacks an approach to select samples to construct explanations. This becomes an issue for PbRL, as transitions relevant to task solving are often outnumbered by irrelevant ones. Thus, ad-hoc sample selection undermines the credibility of explanations. The present study proposes a framework for learning reward functions and state importance from preferences simultaneously. It offers a systematic approach for selecting samples when constructing explanations. Moreover, the present study proposes a perturbation analysis to evaluate the learned state importance quantitatively. Through experiments on discrete and continuous control tasks, the present study demonstrates the proposed framework’s efficacy for providing interpretability without sacrificing task performance. © The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature 2023.
KW  - Human-in-the-loop reinforcement learning
KW  - Interpretability artificial intelligence
KW  - Interpretable reinforcement learning
KW  - Preference-based reinforcement learning
KW  - Learning systems
KW  - Human-in-the-loop
KW  - Human-in-the-loop reinforcement learning
KW  - Interpretability
KW  - Interpretability artificial intelligence
KW  - Interpretable reinforcement learning
KW  - Preference-based
KW  - Preference-based reinforcement learning
KW  - Reinforcement learnings
KW  - Reinforcement learning
PB  - Springer
SN  - 08856125 (ISSN)
LA  - English
J2  - Mach Learn
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: G. Zhang; Graduate School of Informatics, Kyoto University, Kyoto, Yoshida-Honmachi, 606-8501, Japan; email: guoxi@ml.ist.i.kyoto-u.ac.jp; CODEN: MALEE
ER  -

TY  - CONF
AU  - Veronese, C.
AU  - Meli, D.
AU  - Bistaffa, F.
AU  - Rodríguez-Sot, M.
AU  - Farinelli, A.
AU  - Rodríguez-Aguilar, J.A.
TI  - Inductive Logic Programming For Transparent Alignment With Multiple Moral Values
PY  - 2023
T2  - CEUR Workshop Proceedings
VL  - 3615
SP  - 84
EP  - 88
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183207841&partnerID=40&md5=e761fb1a348a86a1c0d0ebdda6187b79
AD  - Department of Computer Science, University of Verona, Verona, 37134, Italy
AD  - IIIA-CSIC, Campus UAB, Bellaterra, 08913, Spain
AB  - Reinforcement learning is a key paradigm for developing intelligent agents that operate in complex environments and interact with humans. However, researchers face the need to explain and interpret the decisions of these systems, especially when it comes to ensuring their alignment with societal value systems. This paper marks the initial stride in an ongoing research direction by applying an inductive logic programming methodology to explain the policy learned by an RL algorithm in the domain of autonomous driving, thus increasing the transparency of the ethical behaviour of agents. © 2023 CEUR-WS. All rights reserved.
KW  - Answer Set Programming
KW  - Ethical Decision Making
KW  - Explainable AI
KW  - Inductive Logic Programming
KW  - Autonomous agents
KW  - Computer circuits
KW  - Ethical technology
KW  - Inductive logic programming (ILP)
KW  - Intelligent agents
KW  - Reinforcement learning
KW  - Answer set programming
KW  - Complex environments
KW  - Ethical decision making
KW  - Explainable AI
KW  - Inductive logic
KW  - Inductive logic programming
KW  - Logic-programming
KW  - Programming methodology
KW  - Reinforcement learnings
KW  - Value systems
KW  - Decision making
A2  - Boella G.
A2  - D'Asaro F.A.
A2  - Dyoub A.
A2  - Gorrieri L.
A2  - Lisi F.A.
A2  - Manganini C.
A2  - Primiero G.
PB  - CEUR-WS
SN  - 16130073 (ISSN)
LA  - English
J2  - CEUR Workshop Proc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: C. Veronese; Department of Computer Science, University of Verona, Verona, 37134, Italy; email: celeste.veronese@univr.it; Conference name: 2nd Workshop on Bias, Ethical AI, Explainability and the Role of Logic and Logic Programming, BEWARE 2023; Conference code: 196373
ER  -

TY  - JOUR
AU  - Wang, Z.
AU  - Xing, Y.
TI  - Energy consumption optimisation for unmanned aerial vehicle based on reinforcement learning framework
PY  - 2024
T2  - International Journal of Powertrains
VL  - 13
IS  - 1
SP  - 75
EP  - 94
DO  - 10.1504/IJPT.2024.138001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191059498&doi=10.1504%2fIJPT.2024.138001&partnerID=40&md5=a47aee7211c2160a0b41692399ad934c
AD  - Department of Aerospace, Cranfield University, College Rd, Wharley End, Bedford, MK43 0AL, United Kingdom
AB  - The average battery life of drones in use today is around 30 minutes, which poses significant limitations for ensuring long-range operation, such as seamless delivery and security monitoring. Meanwhile, the transportation sector is responsible for 93% of all carbon emissions, making it crucial to control energy usage during the operation of UAVs for future net-zero massive-scale air traffic. In this study, a reinforcement learning (RL)-based model was implemented for the energy consumption optimisation of drones. The RL-based energy optimisation framework dynamically tunes vehicle control systems to maximise energy economy while considering mission objectives, ambient circumstances, and system performance. RL was used to create a dynamically optimised vehicle control system that selects the most energy-efficient route. Based on training times, it is reasonable to conclude that a trained UAV saves between 50.1% and 91.6% more energy than an untrained UAV in this study by using the same map. © 2024 Inderscience Publishers. All rights reserved.
KW  - energy efficiency
KW  - machine learning
KW  - path planning
KW  - power consumption
KW  - Q-Learning
KW  - reinforcement learning
KW  - RL
KW  - trajectory optimisation
KW  - Antennas
KW  - Control system synthesis
KW  - Drones
KW  - Energy efficiency
KW  - Motion planning
KW  - Reinforcement learning
KW  - Aerial vehicle
KW  - Battery life
KW  - Energy consumption optimization
KW  - Learning frameworks
KW  - Machine-learning
KW  - Q-learning
KW  - Reinforcement learnings
KW  - Trajectory optimization
KW  - Vehicle control system
KW  - Electric power utilization
PB  - Inderscience Publishers
SN  - 17424267 (ISSN)
LA  - English
J2  - Int.  J.  Powertrains
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: Z. Wang; Department of Aerospace, Cranfield University, Bedford, College Rd, Wharley End, MK43 0AL, United Kingdom; email: ziyue.wang.907@cranfield.ac.uk
ER  -

TY  - JOUR
AU  - Rodriguez-Soto, M.
AU  - Serramia, M.
AU  - Lopez-Sanchez, M.
AU  - Rodriguez-Aguilar, J.A.
TI  - Instilling moral value alignment by means of multi-objective reinforcement learning
PY  - 2022
T2  - Ethics and Information Technology
VL  - 24
IS  - 1
C7  - 9
DO  - 10.1007/s10676-022-09635-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124018759&doi=10.1007%2fs10676-022-09635-0&partnerID=40&md5=0f4e25370f9a47875acb1878f2a0a064
AD  - Artificial intelligence research institute (IIIA-CSIC), Carrer de Can Planas, Campus de la UAB, Bellaterra, 08193, Spain
AD  - Department of Mathematics and Computer Science, University of Barcelona, Gran Via de les Corts Catalanes, 585, Barcelona, 08007, Spain
AB  - AI research is being challenged with ensuring that autonomous agents learn to behave ethically, namely in alignment with moral values. Here, we propose a novel way of tackling the value alignment problem as a two-step process. The first step consists on formalising moral values and value aligned behaviour based on philosophical foundations. Our formalisation is compatible with the framework of (Multi-Objective) Reinforcement Learning, to ease the handling of an agent’s individual and ethical objectives. The second step consists in designing an environment wherein an agent learns to behave ethically while pursuing its individual objective. We leverage on our theoretical results to introduce an algorithm that automates our two-step approach. In the cases where value-aligned behaviour is possible, our algorithm produces a learning environment for the agent wherein it will learn a value-aligned behaviour. © 2022, The Author(s).
KW  - Ethics
KW  - Multi-objective reinforcement learning
KW  - Reinforcement learning
KW  - Value alignment
KW  - Autonomous agents
KW  - Computer aided instruction
KW  - Ethical technology
KW  - Alignment Problems
KW  - Behavior-based
KW  - Formalisation
KW  - Learn+
KW  - Multi objective
KW  - Multi-objective reinforcement learning
KW  - Reinforcement learnings
KW  - Two-step approach
KW  - Two-step process
KW  - Value alignment
KW  - Reinforcement learning
PB  - Springer Science and Business Media B.V.
SN  - 13881957 (ISSN)
LA  - English
J2  - Ethics Inf. Technol.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 15; Correspondence Address: M. Rodriguez-Soto; Artificial intelligence research institute (IIIA-CSIC), Bellaterra, Carrer de Can Planas, Campus de la UAB, 08193, Spain; email: manel.rodriguez@iiia.csic.es
ER  -

TY  - JOUR
AU  - Neufeld, E.A.
AU  - Bartocci, E.
AU  - Ciabattoni, A.
AU  - Governatori, G.
TI  - Enforcing ethical goals over reinforcement-learning policies
PY  - 2022
T2  - Ethics and Information Technology
VL  - 24
IS  - 4
C7  - 43
DO  - 10.1007/s10676-022-09665-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139226510&doi=10.1007%2fs10676-022-09665-8&partnerID=40&md5=01ed0124172338f722461c3115e7de8d
AD  - TU Wien, Vienna, Austria
AD  - Data61, CSIRO, Brisbane, Australia
AB  - Recent years have yielded many discussions on how to endow autonomous agents with the ability to make ethical decisions, and the need for explicit ethical reasoning and transparency is a persistent theme in this literature. We present a modular and transparent approach to equip autonomous agents with the ability to comply with ethical prescriptions, while still enacting pre-learned optimal behaviour. Our approach relies on a normative supervisor module, that integrates a theorem prover for defeasible deontic logic within the control loop of a reinforcement learning agent. The supervisor operates as both an event recorder and an on-the-fly compliance checker w.r.t. an external norm base. We successfully evaluated our approach with several tests using variations of the game Pac-Man, subject to a variety of “ethical” constraints. © 2022, The Author(s).
KW  - Deontic defeasible logic
KW  - Ethical artificial intelligence
KW  - Normative reasoning
KW  - Reinforcement learning
KW  - Autonomous agents
KW  - Computer circuits
KW  - Philosophical aspects
KW  - Supervisory personnel
KW  - Defeasible logic
KW  - Deontic
KW  - Deontic defeasible logic
KW  - Deontic Logic
KW  - Ethical artificial intelligence
KW  - Learning policy
KW  - Modulars
KW  - Normative reasoning
KW  - Reinforcement learnings
KW  - Theorem provers
KW  - Reinforcement learning
PB  - Springer Science and Business Media B.V.
SN  - 13881957 (ISSN)
LA  - English
J2  - Ethics Inf. Technol.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 4; Correspondence Address: E.A. Neufeld; TU Wien, Vienna, Austria; email: emery.a.neufeld@gmail.com
ER  -

TY  - JOUR
AU  - Barth, D.
AU  - Cohen-Boulakia, B.
AU  - Ehounou, W.
TI  - Distributed Reinforcement Learning for the Management of a Smart Grid Interconnecting Independent Prosumers
PY  - 2022
T2  - Energies
VL  - 15
IS  - 4
C7  - 1440
DO  - 10.3390/en15041440
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124908250&doi=10.3390%2fen15041440&partnerID=40&md5=39c7ecfd0dc091a41077c20c005e537a
AD  - DAVID Laboratory, UVSQ/Université Paris-Saclay, 45 Avenue des Etats Unis, Versailles, 78035, France
AD  - LINEACT, CESI, Nanterre, 92000, France
AD  - Laboratoire de Mathématiques Informatique, Université Nangui Abrogoua, 02 BP V 102, Abidjan, Cote d'Ivoire
AB  - In the context of an eco-responsible production and distribution of electrical energy at the local scale of an urban territory, we consider a smart grid as a system interconnecting different prosumers, which all retain their decision-making autonomy and defend their own interests in a comprehensive system where the rules, accepted by all, encourage virtuous behavior. In this paper, we present and analyze a model and a management method for smart grids that is shared between different kinds of independent actors, who respect their own interests, and that encourages each actor to behavior that allows, as much as possible, an energy independence of the smart grid from external energy suppliers. We consider here a game theory model, in which each actor of the smart grid is a player, and we investigate distributed machine-learning algorithms to allow decision-making, thus, leading the game to converge to stable situations, in particular to a Nash equilibrium. We propose a Linear Reward Inaction algorithm that achieves Nash equilibria most of the time, both for a single time slot and across time, allowing the smart grid to maximize its energy independence from external energy suppliers. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - Energy management
KW  - Energy optimization
KW  - Game theory
KW  - Nash equilibria
KW  - Reinforcement learning
KW  - Smart grid
KW  - Decision making
KW  - Decision theory
KW  - Electric power transmission networks
KW  - Learning algorithms
KW  - Reinforcement learning
KW  - Smart power grids
KW  - Distribution of electrical energy
KW  - Energy independence
KW  - Energy optimization
KW  - Energy suppliers
KW  - External energy
KW  - Local scale
KW  - Nash equilibria
KW  - Reinforcement learnings
KW  - Smart grid
KW  - Urban territories
KW  - Game theory
PB  - MDPI
SN  - 19961073 (ISSN)
LA  - English
J2  - Energies
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5; Correspondence Address: D. Barth; DAVID Laboratory, UVSQ/Université Paris-Saclay, Versailles, 45 Avenue des Etats Unis, 78035, France; email: dominique.barth@uvsq.fr; B. Cohen-Boulakia; LINEACT, CESI, Nanterre, 92000, France; email: bcohen@cesi.fr; W. Ehounou; LINEACT, CESI, Nanterre, 92000, France; email: wehounou@gmail.com
ER  -

TY  - JOUR
AU  - Li, Y.
AU  - Li, K.
AU  - Wang, S.
AU  - Chen, X.
AU  - Wen, D.
TI  - Pilot Behavior Recognition Based on Multi-Modality Fusion Technology Using Physiological Characteristics
PY  - 2022
T2  - Biosensors
VL  - 12
IS  - 6
C7  - 404
DO  - 10.3390/bios12060404
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132290713&doi=10.3390%2fbios12060404&partnerID=40&md5=7a1b9467c0f9b0114d715953bfb2ce3c
AD  - National Key Laboratory of Human Machine and Environment Engineering, School of Aeronautical Science and Engineering, Beihang University, Beijing, 100191, China
AB  - With the development of the autopilot system, the main task of a pilot has changed from controlling the aircraft to supervising the autopilot system and making critical decisions. Therefore, the human–machine interaction system needs to be improved accordingly. A key step to improving the human–machine interaction system is to improve its understanding of the pilots’ status, including fatigue, stress, workload, etc. Monitoring pilots’ status can effectively prevent human error and achieve optimal human–machine collaboration. As such, there is a need to recognize pilots’ status and predict the behaviors responsible for changes of state. For this purpose, in this study, 14 Air Force cadets fly in an F-35 Lightning II Joint Strike Fighter simulator through a series of maneuvers involving takeoff, level flight, turn and hover, roll, somersault, and stall. Electro cardio (ECG), myoelectricity (EMG), galvanic skin response (GSR), respiration (RESP), and skin temperature (SKT) measurements are derived through wearable physiological data collection devices. Physiological indicators influenced by the pilot’s behavioral status are objectively analyzed. Multi-modality fusion technology (MTF) is adopted to fuse these data in the feature layer. Additionally, four classifiers are integrated to identify pilots’ behaviors in the strategy layer. The results indicate that MTF can help to recognize pilot behavior in a more comprehensive and precise way. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - behavior recognition
KW  - machine learning
KW  - MTF
KW  - multi-modal
KW  - physiological
KW  - pilot
KW  - Aircraft
KW  - Humans
KW  - Military Personnel
KW  - Technology
KW  - Wearable Electronic Devices
KW  - Air navigation
KW  - Behavioral research
KW  - Electrophysiology
KW  - Fighter aircraft
KW  - Physiological models
KW  - Reinforcement learning
KW  - Supersonic aircraft
KW  - Wearable technology
KW  - Behaviour recognition
KW  - Fusion technology
KW  - Machine-learning
KW  - Modality Fusion
KW  - Modality fusion technology
KW  - Multi-modal
KW  - Multi-modality fusion
KW  - Physiological
KW  - Pilot
KW  - Pilot behavior
KW  - accuracy
KW  - adult
KW  - airplane pilot
KW  - Article
KW  - behavior
KW  - breathing
KW  - classifier
KW  - cross validation
KW  - data processing
KW  - decision tree
KW  - electrocardiography
KW  - electrodermal response
KW  - electromyography
KW  - feature selection
KW  - human
KW  - human experiment
KW  - machine learning
KW  - male
KW  - mathematical model
KW  - physiology
KW  - random forest
KW  - recall
KW  - skin conductance
KW  - skin temperature
KW  - support vector machine
KW  - aircraft
KW  - electronic device
KW  - military personnel
KW  - technology
KW  - Electrocardiograms
PB  - MDPI
SN  - 20796374 (ISSN)
C2  - 35735552
LA  - English
J2  - Biosensors
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 10; Correspondence Address: K. Li; National Key Laboratory of Human Machine and Environment Engineering, School of Aeronautical Science and Engineering, Beihang University, Beijing, 100191, China; email: like@buaa.edu.cn; D. Wen; National Key Laboratory of Human Machine and Environment Engineering, School of Aeronautical Science and Engineering, Beihang University, Beijing, 100191, China; email: d.wen@buaa.edu.cn; CODEN: BISSE
ER  -

TY  - CONF
AU  - Chu, Y.
AU  - Li, J.
AU  - Xu, J.
TI  - How is the AI Perceived When It Behaves (Un)Fairly?
PY  - 2023
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 14050 LNAI
SP  - 421
EP  - 430
DO  - 10.1007/978-3-031-35891-3_25
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166200091&doi=10.1007%2f978-3-031-35891-3_25&partnerID=40&md5=e1b71e9e184d467c8249f4ff37c150fe
AD  - Center for Psychological Sciences, Zhejiang University, Hangzhou, China
AD  - Center for Research and Innovation in Systems Safety, Vanderbilt University Medical Center, Nashville, TN, United States
AB  - Fairness plays a crucial role in human-human interaction, so it is expected to play a significant role in human-AI interaction as well. Integrating the principles of fairness into AI design and investigating people’s perceptions of it can help improve user experience and ensure AI systems are responsible, trustworthy, ethical, and human-centered. In the current study, we simulated different human behaviors in economic games through a human fairness model and reinforcement learning approach and then conducted an experiment to investigate how people perceive AI agents with varying levels of fairness. The study was a within-subject experiment with 2 treatments (fair vs. unfair AI), in which the participants play the Alternated Repeated Ultimatum Game (ARUG) for 12 rounds with each AI agent. The results suggest that the participants evaluated fair AI as having higher levels of warmth, intelligence, animacy, likability, and safety compared to unfair AI. These findings indicate that AI that aligns with social norms is more favored by people. We discuss the theoretical implications for comprehending people’s behavior and attitude towards AI fairness and the practical implications for designing AI that has the potential to increase fairness in society. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
KW  - Alternated Repeated Ultimatum Game
KW  - Fairness
KW  - Human-centered AI
KW  - Perception of AI fairness
KW  - Behavioral research
KW  - Ethical technology
KW  - 'current
KW  - AI systems
KW  - Alternated repeated ultimatum game
KW  - Fairness
KW  - Human-centered AI
KW  - Human-human interactions
KW  - Perception of AI fairness
KW  - Perceptions of IT
KW  - Ultimatum game
KW  - Users' experiences
KW  - Reinforcement learning
A2  - Degen H.
A2  - Ntoa S.
PB  - Springer Science and Business Media Deutschland GmbH
SN  - 03029743 (ISSN); 978-303135890-6 (ISBN)
LA  - English
J2  - Lect. Notes Comput. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: J. Xu; Center for Psychological Sciences, Zhejiang University, Hangzhou, China; email: xujie0987@zju.edu.cn; Conference name: 4th International Conference on Artificial Intelligence in HCI, AI-HCI 2023, held as part of the 25th International Conference on Human-Computer Interaction, HCII 2023; Conference date: 23 July 2023 through 28 July 2023; Conference code: 297989
ER  -

TY  - JOUR
AU  - Ghaderi, S.
AU  - Amani Rad, J.
AU  - Hemami, M.
AU  - Khosrowabadi, R.
TI  - Dysfunctional feedback processing in male methamphetamine abusers: Evidence from neurophysiological and computational approaches
PY  - 2024
T2  - Neuropsychologia
VL  - 197
C7  - 108847
DO  - 10.1016/j.neuropsychologia.2024.108847
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187375740&doi=10.1016%2fj.neuropsychologia.2024.108847&partnerID=40&md5=27453970f208443c28a04f86b51f0e3b
AD  - Institute for Cognitive and Brain Sciences, Shahid Beheshti University, Tehran, Iran
AB  - Methamphetamine use disorder (MUD) as a major public health risk is associated with dysfunctional neural feedback processing. Although dysfunctional feedback processing in people who are substance dependent has been explored in several behavioral, computational, and electrocortical studies, this mechanism in MUDs requires to be well understood. Furthermore, the current understanding of latent components of their behavior such as learning speed and exploration-exploitation dilemma is still limited. In addition, the association between the latent cognitive components and the related neural mechanisms also needs to be explored. Therefore, in this study, the underlying neurocognitive mechanisms of feedback processing of such impairment, and age/gender-matched healthy controls are evaluated within a probabilistic learning task with rewards and punishments. Mathematical modeling results based on the Q-learning paradigm suggested that MUDs show less sensitivity in distinguishing optimal options. Additionally, it may be worth noting that MUDs exhibited a slight decrease in their ability to learn from negative feedback compared to healthy controls. Also through the lens of underlying neural mechanisms, MUDs showed lower theta power at the medial-frontal areas while responding to negative feedback. However, other EEG measures of reinforcement learning including feedback-related negativity, parietal-P300, and activity flow from the medial frontal to lateral prefrontal regions, remained intact in MUDs. On the other hand, the elimination of the linkage between value sensitivity and medial-frontal theta activity in MUDs was observed. The observed dysfunction could be due to the adverse effects of methamphetamine on the cortico-striatal dopamine circuit, which is reflected in the anterior cingulate cortex activity as the most likely region responsible for efficient behavior adjustment. These findings could help us to pave the way toward tailored therapeutic approaches. © 2024 Elsevier Ltd
KW  - Electroencephalography (EEG)
KW  - Feedback processing
KW  - Methamphetamine use disorder
KW  - Reinforcement learning
KW  - Value sensitivity
KW  - Electroencephalography
KW  - Feedback
KW  - Humans
KW  - Male
KW  - Methamphetamine
KW  - Reinforcement, Psychology
KW  - Reward
KW  - dopamine
KW  - methamphetamine
KW  - methamphetamine
KW  - adult
KW  - Article
KW  - cingulate gyrus
KW  - cognition
KW  - cognitive defect
KW  - computer model
KW  - controlled study
KW  - drug dependence
KW  - electroencephalogram
KW  - electroencephalography
KW  - event related potential
KW  - exploration exploitation tradeoff
KW  - feedback system
KW  - human
KW  - male
KW  - mathematical model
KW  - negative feedback
KW  - Q learning
KW  - reinforcement learning (machine learning)
KW  - reward seeking behavior
KW  - theta rhythm
KW  - electroencephalography
KW  - feedback system
KW  - procedures
KW  - reinforcement (psychology)
KW  - reward
PB  - Elsevier Ltd
SN  - 00283932 (ISSN)
C2  - 38460774
LA  - English
J2  - Neuropsychologia
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: R. Khosrowabadi; Institute for Cognitive and Brain Sciences, Shahid Beheshti University, Tehran, Iran; email: r_khosroabadi@sbu.ac.ir; J. Amani Rad; Institute for Cognitive and Brain Sciences, Shahid Beheshti University, Tehran, Iran; email: ja_amanirad@sbu.ac.ir; CODEN: NUPSA
ER  -

TY  - JOUR
AU  - Taghavifar, H.
AU  - Wei, C.
AU  - Taghavifar, L.
TI  - Socially Intelligent Reinforcement Learning for Optimal Automated Vehicle Control in Traffic Scenarios
PY  - 2024
T2  - IEEE Transactions on Automation Science and Engineering
SP  - 1
EP  - 12
DO  - 10.1109/TASE.2023.3347264
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184343218&doi=10.1109%2fTASE.2023.3347264&partnerID=40&md5=8c12d1cead38e9191741e52a152af70d
AD  - Department of Mechanical, Industrial and Aerospace Engineering, Concordia University, Montreal, Canada
AD  - School of Mechanical and Aerospace Engineering, Queen&#x2019;s University Belfast, Belfast, U.K
AD  - School of Electrical and Electronics Engineering, Islamic Azad University (IAU), Tehran, Iran
AB  - In this paper, a novel approach is presented for modeling the interaction dynamics between an ego car and a bicycle in a traffic scenario using a hybrid reinforcement learning framework combined with a social value orientation (SVO) model. The proposed framework leverages the SARSA algorithm to learn the optimal policy for the ego vehicle while incorporating risk cost as the negative log-likelihood of collision. Additionally, a customized SVO model is introduced to capture the social preferences of the ego car and the bicycle, defining the SVO of each agent as a continuous variable between egoistic and cooperative orientations. Furthermore, a weight parameter is incorporated in the framework to regulate the influence of the SVO model on the learning process. We demonstrate the effectiveness of our approach through extensive simulations, showing that the ego car can balance between maximizing its reward and avoiding collisions while considering the social preferences of the agents. The obtained results are compared to other models in the literature, and it is shown that the proposed method contributes to the development of safe and efficient autonomous driving systems that interact with human-driven vehicles in a socially intelligent manner <italic>Note to Practitioners</italic>&#x2014;This proposed framework is motivated by the pressing challenge of navigation for autonomous cars in complex urban driving scenarios and mixed traffic situations. With the increasing prevalence of autonomous vehicles on roads, developing intelligent navigation systems that can effectively interact with other road users has become essential. Our novel framework addresses this need by leveraging the SARSA algorithm to learn the optimal policy for the ego vehicle while incorporating risk cost as the negative log-likelihood of collision. Additionally, a customized SVO model is introduced to capture the social preferences of the ego car and the bicycle, defining the SVO of each agent as a continuous variable between egoistic and cooperative orientations. This enables autonomous vehicles to make informed decisions and navigate safely and efficiently. Our framework can enormously help the field of autonomous vehicle navigation and contribute significantly to developing safe, human-centric, and reliable transportation systems. The versatility of our approach is evident in its potential to support a network of autonomous vehicles interacting with multiple road users, thereby enhancing scalability. By leveraging the power of machine learning, our solution provides a robust and adaptable approach that can handle the diverse and ever-changing conditions of urban driving scenarios. IEEE
KW  - Automated cars
KW  - Automobiles
KW  - Autonomous vehicles
KW  - Decision making
KW  - Ethics
KW  - obstacle avoidance
KW  - path planning
KW  - Reinforcement learning
KW  - reinforcement learning
KW  - Roads
KW  - Safety
KW  - Autonomous vehicles
KW  - Bicycles
KW  - Control system synthesis
KW  - Motion planning
KW  - Navigation systems
KW  - Reinforcement learning
KW  - Roads and streets
KW  - Automated car
KW  - Autonomous Vehicles
KW  - Decisions makings
KW  - Obstacles avoidance
KW  - Reinforcement learnings
KW  - Road
KW  - SARSA algorithm
KW  - Social preference
KW  - Social value orientations
KW  - Decision making
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 15455955 (ISSN)
LA  - English
J2  - IEEE Trans. Autom. Sci. Eng.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1
ER  -

TY  - JOUR
AU  - Li, T.
TI  - Optimizing the configuration of deep learning models for music genre classification
PY  - 2024
T2  - Heliyon
VL  - 10
IS  - 2
C7  - e24892
DO  - 10.1016/j.heliyon.2024.e24892
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183560734&doi=10.1016%2fj.heliyon.2024.e24892&partnerID=40&md5=8d9235a84b3f3c5dc5c6e0c5d538a2ee
AD  - Academy of Arts, Pingdingshan Polytenchnic College, Henan, Pingdingshan, 467000, China
AB  - Music genre categorization is a fundamental use of sound processing methods in the realm of music retrieval. Typically, people are responsible for categorizing music genres. Machine learning approaches can automate this procedure. Therefore, in recent years, several approaches have been suggested to achieve this objective. Nevertheless, the given findings indicate that there is still a discrepancy between the observed results and an optimal categorization method. Hence, this paper introduces a novel approach for accurately forecasting music genres by using deep learning methodologies. The proposed approach involves preprocessing the input signals and then representing the characteristics of each signal using a combination of Mel Frequency Cepstral Coefficients (MFCC) and Short-Time Fourier Transform (STFT) features. Subsequently, a convolutional neural network (CNN) is applied to process each group of these characteristics. The proposed technique utilizes two CNN models to analyze MFCC and STFT data. Although the structure of these models is identical, the hyper-parameters of each model are individually adjusted using the black hole optimization (BHO) algorithm. Here, the optimization method fine-tunes the hyperparameters of each CNN model to minimize their training error. Ultimately, the results of two Convolutional Neural Network (CNN) models are combined to determine the music genre using a classifier based on SoftMax. The efficacy of the suggested methodology in categorizing music genres has been assessed using the GTZAN and Extended-Ballroom datasets. The experimental findings demonstrated that the suggested approach achieved classification accuracies of 95.2 % and 95.7 % in the two datasets, respectively, indicating its superiority over earlier efforts. © 2024 The Author
KW  - Convolutional neural network
KW  - Deep reinforcement learning
KW  - Music genre classification
KW  - Signal processing
PB  - Elsevier Ltd
SN  - 24058440 (ISSN)
LA  - English
J2  - Heliyon
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0
ER  -

TY  - JOUR
AU  - Killeen, B.D.
AU  - Cho, S.M.
AU  - Armand, M.
AU  - Taylor, R.H.
AU  - Unberath, M.
TI  - In silico simulation: a key enabling technology for next-generation intelligent surgical systems
PY  - 2023
T2  - Progress in Biomedical Engineering
VL  - 5
IS  - 3
C7  - 032001
DO  - 10.1088/2516-1091/acd28b
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159783730&doi=10.1088%2f2516-1091%2facd28b&partnerID=40&md5=e4e5c7f065d20b9e4c7bbef53cf0f2b0
AD  - Laboratory for Computational Sensing and Robotics, Johns Hopkins University, 3400 N. Charles St., Baltimore, 21218, MD, United States
AB  - To mitigate the challenges of operating through narrow incisions under image guidance, there is a desire to develop intelligent systems that assist decision making and spatial reasoning in minimally invasive surgery (MIS). In this context, machine learning-based systems for interventional image analysis are receiving considerable attention because of their flexibility and the opportunity to provide immediate, informative feedback to clinicians. It is further believed that learning-based image analysis may eventually form the foundation for semi- or fully automated delivery of surgical treatments. A significant bottleneck in developing such systems is the availability of annotated images with sufficient variability to train generalizable models, particularly the most recently favored deep convolutional neural networks or transformer architectures. A popular alternative to acquiring and manually annotating data from the clinical practice is the simulation of these data from human-based models. Simulation has many advantages, including the avoidance of ethical issues, precisely controlled environments, and the scalability of data collection. Here, we survey recent work that relies on in silico training of learning-based MIS systems, in which data are generated via computational simulation. For each imaging modality, we review available simulation tools in terms of compute requirements, image quality, and usability, as well as their applications for training intelligent systems. We further discuss open challenges for simulation-based development of MIS systems, such as the need for integrated imaging and physical modeling for non-optical modalities, as well as generative patient models not dependent on underlying computed tomography, MRI, or other patient data. In conclusion, as the capabilities of in silico training mature, with respect to sim-to-real transfer, computational efficiency, and degree of control, they are contributing toward the next generation of intelligent surgical systems. © 2023 IOP Publishing Ltd.
KW  - endoscopy
KW  - in silico virtual clinical trials
KW  - machine learning
KW  - minimally invasive surgery
KW  - reinforcement learning
KW  - ultrasound
KW  - x-ray
KW  - Computational efficiency
KW  - Computerized tomography
KW  - Convolutional neural networks
KW  - Decision making
KW  - Deep neural networks
KW  - E-learning
KW  - Hospital data processing
KW  - Image analysis
KW  - Image enhancement
KW  - Learning systems
KW  - Magnetic resonance imaging
KW  - Reinforcement learning
KW  - Clinical trial
KW  - Image-analysis
KW  - In silico virtual clinical trial
KW  - In-silico
KW  - Machine-learning
KW  - Minimally-invasive surgery
KW  - Reinforcement learnings
KW  - Surgery systems
KW  - Surgical systems
KW  - X-ray
KW  - adult
KW  - article
KW  - avoidance behavior
KW  - clinical practice
KW  - computer assisted tomography
KW  - computer simulation
KW  - convolutional neural network
KW  - endoscopy
KW  - human
KW  - image quality
KW  - learning
KW  - machine learning
KW  - minimally invasive surgery
KW  - nuclear magnetic resonance imaging
KW  - patient coding
KW  - physical model
KW  - reinforcement learning (machine learning)
KW  - simulation
KW  - ultrasound
KW  - usability
KW  - X ray
KW  - Intelligent systems
PB  - Institute of Physics
SN  - 25161091 (ISSN)
LA  - English
J2  - Prog. Biomed. Eng.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 3; Correspondence Address: B.D. Killeen; Laboratory for Computational Sensing and Robotics, Johns Hopkins University, Baltimore, 3400 N. Charles St., 21218, United States; email: killeen@jhu.edu
ER  -

TY  - CONF
AU  - Estiri, E.
AU  - Mirinejad, H.
TI  - Precision Dosing in Critical Care: Application of Machine Learning in Fluid Therapy
PY  - 2023
T2  - Proceedings - 2023 IEEE International Conference on Digital Health, ICDH 2023
SP  - 348
EP  - 352
DO  - 10.1109/ICDH60066.2023.00058
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172319178&doi=10.1109%2fICDH60066.2023.00058&partnerID=40&md5=3be41491fa0b8032d9f8c546a9108202
AD  - Kent State University, College of Aeronautics and Engineering, Kent, OH, United States
AB  - Fluid therapy is a common treatment for hypovolemic scenarios to restore the lost blood volume and stabilize acutely ill patients. Automating fluid therapy can lead to a reduction of delay in care, a decrement in dosing errors, and a reduction of cognitive load on clinicians responsible for patient care resulting in improved patient outcomes. However, this process is highly challenging due to the complexity of patient's physiology and the variability of hemodynamic responses among patients. This work presents a novel machine learning approach based on reinforcement learning (RL) for automated fluid management, where the RL agent is designed to recommend subject-specific infusion dosages without having the knowledge of dose-response models and only by interacting with the environment (virtual subject generator). Compared to the state-of-the-art focusing on the entire population's data, the proposed approach uses individual patient's data to recommend patient-specific fluid dosage adjustment. Simulation results demonstrate that the proposed approach outperforms a proportional-integral-derivative (PID) and a rule-based fluid resuscitation controller previously reported for an animal study.  © 2023 IEEE.
KW  - automated fluid therapy
KW  - fluid management
KW  - Machine learning
KW  - mean arterial pressure
KW  - reinforcement learning
KW  - Blood pressure
KW  - Learning algorithms
KW  - Population statistics
KW  - Proportional control systems
KW  - Resuscitation
KW  - Two term control systems
KW  - % reductions
KW  - Automated fluid therapy
KW  - Blood volumes
KW  - Cognitive loads
KW  - Critical care
KW  - Fluid management
KW  - Fluid therapy
KW  - Machine-learning
KW  - Mean arterial pressure
KW  - Reinforcement learnings
KW  - Reinforcement learning
A2  - Chang C.K.
A2  - Chang R.N.
A2  - Fan J.
A2  - Fox G.C.
A2  - Jin Z.
A2  - Pravadelli G.
A2  - Shahriar H.
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835034103-4 (ISBN)
LA  - English
J2  - Proc. - IEEE Int. Conf. Digit. Health, ICDH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2; Correspondence Address: E. Estiri; Kent State University, College of Aeronautics and Engineering, Kent, United States; email: eestiri@kent.edu; Conference name: 2023 IEEE International Conference on Digital Health, ICDH 2023; Conference date: 2 July 2023 through 8 July 2023; Conference code: 192085
ER  -

TY  - JOUR
AU  - Costa, B.A.
AU  - Parente, F.L.
AU  - Belfo, J.
AU  - Somma, N.
AU  - Rosa, P.
AU  - Igreja, J.M.
AU  - Belhadj, J.
AU  - Lemos, J.M.
TI  - A reinforcement learning approach for adaptive tracking control of a reusable rocket model in a landing scenario
PY  - 2024
T2  - Neurocomputing
VL  - 577
C7  - 127377
DO  - 10.1016/j.neucom.2024.127377
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184814391&doi=10.1016%2fj.neucom.2024.127377&partnerID=40&md5=c40abfd17e5c6c8beacf5ac51d49a410
AD  - INESC-ID/IST/University of Lisbon, Rua Alves Redol 9, Lisboa, 1000-029, Portugal
AD  - INESC-ID, Rua Alves Redol 9, Lisboa, 1000-029, Portugal
AD  - DEIMOS Engenharia, Lisbon, Portugal
AD  - ESA, Netherlands
AB  - This work explores the application of reinforcement learning to control the descend/landing phase of a reusable rocket model, where the rocket must track a nominal/planar reference path with adequate attitude. To tackle the control problem, which is multivariable, a cascade control architecture is proposed, consisting of three single-input single-output controllers. The inner control loop is responsible for controlling the attitude using a reference for attitude provided by the outer loop and the thrust vectoring, where the gimbal angle is the manipulated variable. The position error is handled by the external loop, that handles the altitude error, used to adjust thrust. Several configurations, using Linear Quadratic Gaussian (LQG) control (LQ regulator with a Kalman observer), PID control and adaptive control based on Reinforcement Learning (optimal control with learning capability based on a linear regression model), are evaluated and compared. © 2024 Elsevier B.V.
KW  - Attitude control
KW  - GNC
KW  - Reentry rocket
KW  - Reinforcement learning
KW  - Trajectory control
KW  - Attitude control
KW  - Cascade control systems
KW  - Computer software reusability
KW  - Learning systems
KW  - Navigation
KW  - Regression analysis
KW  - Three term control systems
KW  - Adaptive tracking control
KW  - Control problems
KW  - GNC
KW  - Landing phase
KW  - Multi variables
KW  - Reentry rocket
KW  - Reference path
KW  - Reinforcement learning approach
KW  - Reinforcement learnings
KW  - Trajectory control
KW  - airport
KW  - altitude
KW  - Article
KW  - artificial neural network
KW  - computer simulation
KW  - control system
KW  - deep learning
KW  - kernel method
KW  - linear regression analysis
KW  - reinforcement learning (machine learning)
KW  - sea level
KW  - space flight
KW  - Reinforcement learning
PB  - Elsevier B.V.
SN  - 09252312 (ISSN)
LA  - English
J2  - Neurocomputing
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: B.A. Costa; INESC-ID/IST/University of Lisbon, Lisboa, Rua Alves Redol 9, 1000-029, Portugal; email: bac@inesc-id.pt; CODEN: NRCGE
ER  -

TY  - CONF
AU  - Alcaraz, B.
AU  - Chaput, R.
AU  - Boissier, O.
AU  - Leturc, C.
TI  - AJAR: An Argumentation-based Judging Agents Framework for Ethical Reinforcement Learning
PY  - 2023
T2  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
VL  - 2023-May
SP  - 2427
EP  - 2429
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171276678&partnerID=40&md5=9f976d8b88b69768ce72cbb14cf063a5
AD  - University of Luxembourg, Esch-sur-Alzette, Luxembourg
AD  - Univ Lyon, UCBL, CNRS, INSA Lyon, LIRIS, UMR5205, Villeurbanne, F-69622, France
AD  - Mines Saint-Etienne, Univ Clermont Auvergne, CNRS, UMR 6158 LIMOS, Institut Henri Fayol, Saint-Etienne, F-42023, France
AD  - Inria, Université Côte d'Azur, CNRS, I3S Sophia, Antipolis, France
AB  - An increasing number of socio-technical systems embedding Artificial Intelligence (AI) technologies are deployed, and questions arise about the possible impact of such systems onto humans. We propose a hybrid multi-agent Reinforcement Learning framework consists of learning agents that learn a task-oriented behaviour defined by a set of symbolic moral judging agents to ensure they respect moral values. This framework is applied on the problem of responsible energy distribution for smart grids. © 2023 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.
KW  - Argumentation
KW  - Artificial Moral Agent
KW  - Ethical Judgment
KW  - Hybrid Neural-Symbolic Learning
KW  - Machine Ethics
KW  - Reinforcement Learning
KW  - Autonomous agents
KW  - Ethical technology
KW  - Intelligent agents
KW  - Learning systems
KW  - Multi agent systems
KW  - Smart power grids
KW  - Agent Framework
KW  - Argumentation
KW  - Artificial moral agent
KW  - Ethical judgements
KW  - Hybrid neural-symbolic learning
KW  - Machine ethic
KW  - Moral agents
KW  - Reinforcement learnings
KW  - Sociotechnical systems
KW  - Symbolic learning
KW  - Reinforcement learning
PB  - International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)
SN  - 15488403 (ISSN)
LA  - English
J2  - Proc. Int. Joint Conf. Auton. Agents Multiagent Syst., AAMAS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Conference name: 22nd International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2023; Conference date: 29 May 2023 through 2 June 2023; Conference code: 191576
ER  -

TY  - JOUR
AU  - Zhang, S.
AU  - Wang, C.
AU  - Zomaya, A.Y.
TI  - Robustness Analysis and Enhancement of Deep Reinforcement Learning-Based Schedulers
PY  - 2023
T2  - IEEE Transactions on Parallel and Distributed Systems
VL  - 34
IS  - 1
SP  - 346
EP  - 357
DO  - 10.1109/TPDS.2022.3218649
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141572779&doi=10.1109%2fTPDS.2022.3218649&partnerID=40&md5=24f05a3e5db296557ef17e961c26089e
AD  - The University of Sydney, School of Computer Science, Camperdown, 2006, NSW, Australia
AD  - CSIRO Data61, Eveleigh, 2015, NSW, Australia
AB  - Dependency-aware jobs, such as the big data analytic workflows, are commonly executed on the cloud. They are compiled to directed acyclic graphs, with tasks linked in regarding the dependency. The cloud scheduler, which maintains a large number of resources, is responsible to execute tasks in parallel. To resolve the complex dependencies, Deep Reinforcement Learning (DRL) based schedulers are widely applied. However, we find that the DRL-based schedulers are vulnerable to the perturbations in the input jobs and may generate falsified decisions to benefit a particular job while delaying the others. By perturbation, we mean a slight adjustment to the job's node features or dependencies, while not changing its functionality. In this paper, we first explore the vulnerability of DRL-based schedulers to job perturbations without accessing the information of the DRL models used in the scheduler. We devise the black-box perturbation system, in which, a proxy model is trained to mimic the DRL-based scheduling policy. We show that the high-faith proxy model can help to craft effective perturbations. The DRL-based schedulers can be as high as 60% likely to be badly affected by the perturbations. Then, we investigate the solution to improve the robustness of DRL-based schedulers to such perturbations. We propose an adversarial training framework to force the neural model to adapt to the perturbation patterns during training so as to eliminate the potential damage during applications. Experiments show that the adversarial-trained scheduler is more robust, reducing the chance of being affected to 3-fold less and the potential bad effects halved.  © 1990-2012 IEEE.
KW  - black-box
KW  - deep reinforcement learning
KW  - gradient
KW  - job perturbation
KW  - machine learning
KW  - robustness
KW  - Scheduling
KW  - vulnerability
KW  - Deep learning
KW  - Directed graphs
KW  - Heuristic methods
KW  - Job analysis
KW  - Job shop scheduling
KW  - Learning systems
KW  - Perturbation techniques
KW  - Black boxes
KW  - Computational modelling
KW  - Deep reinforcement learning
KW  - Gradient
KW  - Job perturbation
KW  - Machine-learning
KW  - Perturbation method
KW  - Reinforcement learnings
KW  - Robustness
KW  - Scheduling
KW  - Task analysis
KW  - Vulnerability
KW  - Reinforcement learning
PB  - IEEE Computer Society
SN  - 10459219 (ISSN)
LA  - English
J2  - IEEE Trans Parallel Distrib Syst
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: S. Zhang; The University of Sydney, School of Computer Science, Camperdown, 2006, Australia; email: szha6955@uni.sydney.edu.au; CODEN: ITDSE
ER  -

TY  - JOUR
AU  - Németh, B.
TI  - Consequences of an Analysis Using Biblical Analogies for Automated Vehicle Control Design
PY  - 2022
T2  - Studia Universitatis Babes-Bolyai Theologia Reformata Transylvanica
VL  - 67
IS  - 2
SP  - 29
EP  - 56
DO  - 10.24193/subbtref.67.2.02
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147524702&doi=10.24193%2fsubbtref.67.2.02&partnerID=40&md5=d8402ad57b3c72d7732a6be8f5745b7c
AD  - The Faculty of Theology, Károli Gáspár University of the Reformed Church, Hungary
AB  - The paper proposes an analysis of learning-based approaches for automated vehicle control systems from an ethical viewpoint. An analysis using analogies between selected biblical texts and operation concepts of learning-based approaches is performed. Thus, analogies for supervised, unsupervised, and reinforcement learning-based approaches are created. Through the analogies, the root of the automatic control design problems, i.e. forming objective functions, on a theological level is explored. The analysis leads to three consequences, which are related to the difficulty of forming control objective, the difficulty of considering human objectives in control, and the necessity of viewing systems in all their complexity. The paper proposes the application of the consequences in an illustrative route selection vehicle control example. A multi-layer control concept involving the consequences of the analysis is proposed, with which some ethical challenges of the selected control problem can be handled. © 2022 Studia UBB Theologia Reformata Transylvanica.
KW  - automated vehicle control
KW  - biblical analogies
KW  - ethical challenges
KW  - machine learning
PB  - Babes-Bolyai University
SN  - 15825418 (ISSN)
LA  - English
J2  - Stud. Univ. Babes-Bolyai, Theol. Reform. Transylv.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: B. Németh; The Faculty of Theology, Károli Gáspár University of the Reformed Church, Hungary; email: nemeth.balazs@kjk.bme.hu
ER  -

TY  - CONF
AU  - Ndlovu, S.C.
AU  - Mthunzi-Kufa, P.
TI  - Machine Learning in Biophotonics: Progress and Challenges
PY  - 2023
T2  - Proceedings of SPIE - The International Society for Optical Engineering
VL  - 12446
C7  - 1244614
DO  - 10.1117/12.2662811
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159417127&doi=10.1117%2f12.2662811&partnerID=40&md5=5a3f615b1d078faa85d9b5cd076e5124
AD  - Council for Scientific and Industrial Research Meiring Naudé Rd, Brummeria, Pretoria, 0184, South Africa
AD  - University of KwaZulu-Natal Meiring Naudé Rd, Brummeria, Pretoria, 0184, South Africa
AB  - Machine learning has paved the way for many breakthroughs in recent years, from the development of powerful computers to advances in image and voice recognition. The technique is applied to a wide range of problems, yet it is particularly useful in solving biomedical questions such as how cells function, get infected by microorganisms, and respond to drugs. Separately, biophotonics is the science involving the mixing of life and light sciences, where numerous health challenges have been studied. However, regardless of outstanding achievements and contributions to life sciences and medicine, there are significant challenges and exciting opportunities in both biophotonics and machine learning. In this article, we provide an overview of machine learning in biophotonics and its applications in cell imaging, diagnosis, and treatment. In biophotonics, machine learning is used to classify, detect, and segment cells or pathogens. Our group aims to use machine learning to detect, classify, and differentiate normal cells from diseased cells for diagnostics at point-of-care settings. It is noteworthy that we focus on the challenges of working with high-dimensional data such as continuous spectra from optical imaging and spectral features from cell imaging. The resulting increase in technological advances has led to an ever-increasing need for easy access, faster, and more efficient methods of healthcare. Applications of machine learning in biophotonics are already providing the answer in many ways, and through it, there is the potential to fully transform healthcare in the future. © 2023 SPIE.
KW  - Annotated data
KW  - Biomedical applications
KW  - Biophotonics
KW  - Classification
KW  - Data availability
KW  - Data quality
KW  - Deep learning
KW  - Ethical considerations
KW  - Explainable AI
KW  - Generative models
KW  - Image analysis
KW  - Interdisciplinary collaboration
KW  - Interpretability
KW  - Machine learning
KW  - Overfitting
KW  - Regulatory compliance
KW  - Reinforcement learning
KW  - Transfer learning
KW  - Cells
KW  - Clustering algorithms
KW  - Cytology
KW  - Diagnosis
KW  - Health care
KW  - Learning systems
KW  - Medical applications
KW  - Regulatory compliance
KW  - Reinforcement learning
KW  - Annotated data
KW  - Bio photonics
KW  - Biomedical applications
KW  - Data availability
KW  - Data quality
KW  - Deep learning
KW  - Ethical considerations
KW  - Explainable AI
KW  - Generative model
KW  - Image-analysis
KW  - Interdisciplinary collaborations
KW  - Interpretability
KW  - Machine-learning
KW  - Overfitting
KW  - Reinforcement learnings
KW  - Transfer learning
KW  - Deep learning
A2  - Hemmer P.R.
A2  - Migdall A.L.
PB  - SPIE
SN  - 0277786X (ISSN); 978-151065997-1 (ISBN)
LA  - English
J2  - Proc SPIE Int Soc Opt Eng
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: S.C. Ndlovu; Council for Scientific and Industrial Research Meiring Naudé Rd, Pretoria, Brummeria, 0184, South Africa; email: SNdlovu2@csir.ci.za; Conference name: Quantum Computing, Communication, and Simulation III 2023; Conference date: 29 January 2023 through 2 February 2023; Conference code: 187946; CODEN: PSISD
ER  -

TY  - CONF
AU  - Kasirzadeh, A.
AU  - Evans, C.
TI  - User Tampering in Reinforcement Learning Recommender Systems
PY  - 2023
T2  - AIES 2023 - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society
SP  - 58
EP  - 69
DO  - 10.1145/3600211.3604669
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172357979&doi=10.1145%2f3600211.3604669&partnerID=40&md5=799ac343804d2feac22eb741385ca0bc
AD  - The Alan Turing Institute, The University of Edinburgh, Edinburgh, United Kingdom
AD  - The Australian National University, Canberra, Australia
AB  - In this paper, we introduce new formal methods and provide empirical evidence to highlight a unique safety concern prevalent in reinforcement learning (RL)-based recommendation algorithms - 'user tampering.' User tampering is a situation where an RL-based recommender system may manipulate a media user's opinions through its suggestions as part of a policy to maximize long-term user engagement. We use formal techniques from causal modeling to critically analyze prevailing solutions proposed in the literature for implementing scalable RL-based recommendation systems, and we observe that these methods do not adequately prevent user tampering. Moreover, we evaluate existing mitigation strategies for reward tampering issues, and show that these methods are insufficient in addressing the distinct phenomenon of user tampering within the context of recommendations. We further reinforce our findings with a simulation study of an RL-based recommendation system focused on the dissemination of political content. Our study shows that a Q-learning algorithm consistently learns to exploit its opportunities to polarize simulated users with its early recommendations in order to have more consistent success with subsequent recommendations that align with this induced polarization. Our findings emphasize the necessity for developing safer RL-based recommendation systems and suggest that achieving such safety would require a fundamental shift in the design away from the approaches we have seen in the recent literature.  © 2023 Owner/Author.
KW  - AI Ethics
KW  - AI Safety
KW  - Recommendation Systems
KW  - Recommender Systems
KW  - Reinforcement Learning
KW  - Value Alignment
KW  - Formal methods
KW  - Learning algorithms
KW  - Learning systems
KW  - Recommender systems
KW  - User profile
KW  - AI ethic
KW  - AI safety
KW  - Causal modeling
KW  - Formal techniques
KW  - Mitigation strategy
KW  - Recommendation algorithms
KW  - Reinforcement learnings
KW  - Safety concerns
KW  - User engagement
KW  - Value alignment
KW  - Reinforcement learning
PB  - Association for Computing Machinery, Inc
SN  - 979-840070231-0 (ISBN)
LA  - English
J2  - AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Conference name: 2023 AAAI / ACM Conference on Artificial Intelligence, Ethics, and Society, AIES 2023; Conference date: 8 August 2023 through 10 August 2023; Conference code: 192626
ER  -

TY  - CONF
AU  - Roberts, J.S.
AU  - Montoya, L.N.
TI  - Contextualizing Artificially Intelligent Morality: A Meta-ethnography of Theoretical, Political and Applied Ethics
PY  - 2023
T2  - Lecture Notes in Networks and Systems
VL  - 652 LNNS
SP  - 482
EP  - 501
DO  - 10.1007/978-3-031-28073-3_35
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150957642&doi=10.1007%2f978-3-031-28073-3_35&partnerID=40&md5=9966d8da6566f7682fbbac5a67afb928
AD  - Accel AI Institute, San Francisco, CA, United States
AB  - In this meta-ethnography, we explore three different angles of ethical artificial intelligence (AI) design implementation including the philosophical ethical viewpoint, the technical perspective, and framing through a political lens. Our qualitative research includes a literature review which highlights the cross referencing of these angles through discussing the value and drawbacks of contrastive top-down, bottom-up, and hybrid approaches previously published. The novel contribution to this framework is the political angle, which constitutes ethics in AI either being determined by corporations and governments and imposed through policies or law (coming from the top), or ethics being called for by the people (coming from the bottom), as well as top-down, bottom-up, and hybrid technicalities of how AI is developed within a moral construct and in consideration of its users, with expected and unexpected consequences and long-term impact in the world. There is a focus on reinforcement learning as an example of a bottom-up applied technical approach and AI ethics principles as a practical top-down approach. This investigation includes real-world case studies to impart a global perspective, as well as philosophical debate on the ethics of AI and theoretical future thought experimentation based on historical fact, current world circumstances, and possible ensuing realities. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
KW  - Artificial intelligence
KW  - Ethics
KW  - Politics
KW  - Reinforcement learning
KW  - Ethical technology
KW  - Bottom up
KW  - Bottom up approach
KW  - Design implementation
KW  - Hybrid approach
KW  - Literature reviews
KW  - Long-term impacts
KW  - Politic
KW  - Qualitative research
KW  - Reinforcement learnings
KW  - Topdown
KW  - Reinforcement learning
A2  - Arai K.
PB  - Springer Science and Business Media Deutschland GmbH
SN  - 23673370 (ISSN); 978-303128072-6 (ISBN)
LA  - English
J2  - Lect. Notes Networks Syst.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: J.S. Roberts; Accel AI Institute, San Francisco, United States; email: info@accel.ai; Conference name: 8th Future of Information and Computing Conference, FICC 2023; Conference date: 2 March 2023 through 3 March 2023; Conference code: 291059
ER  -

TY  - JOUR
AU  - Marvi, M.
AU  - Aijaz, A.
AU  - Qureshi, A.
AU  - Khurram, M.
TI  - Development of an AI-Enabled Q-Agent for Making Data Offloading Decisions in a Multi-RAT Wireless Network
PY  - 2024
T2  - Journal of Computer Networks and Communications
VL  - 2024
C7  - 9571987
DO  - 10.1155/2024/9571987
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184892579&doi=10.1155%2f2024%2f9571987&partnerID=40&md5=29a648686b840bd816cbba61e66636d7
AD  - Department of Computer Science and Information Technology, NED University of Engineering and Technology, Karachi, Pakistan
AD  - Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Bristol, United Kingdom
AD  - Department of Computer Science, National University of Computer and Emerging Sciences, Karachi, Pakistan
AD  - Department of Computer and Information Systems Engineering, NED University of Engineering and Technology, Karachi, Pakistan
AB  - Data offloading is considered as a potential candidate for alleviating congestion on wireless networks and for improving user experience. However, due to the stochastic nature of the wireless networks, it is important to take optimal actions under different conditions such that the user experience is enhanced and congestion on heavy-loaded radio access technologies (RATs) is reduced by offloading data through lower loaded RATs. Since artificial intelligence (AI)-based techniques can learn optimal actions and adapt to different conditions, in this work, we develop an AI-enabled Q-agent for making data offloading decisions in a multi-RAT wireless network. We employ a model-free Q-learning algorithm for training of the Q-agent. We use stochastic geometry as a tool for estimating the average data rate offered by the network in a given region by considering the effect of interference. We use the Markov process for modeling users' mobility, that is, estimating the probability that a user is currently located in a region given its previous location. The user equipment (UE) plays the role of a Q-agent responsible for taking sequence of actions such that the long-term discounted cost for using network service is minimized. Q-agent performance has been evaluated and compared with the existing data offloading policies. The results suggest that the existing policies offer the best performance under specific situations. However, the Q-agent has learned to take near-optimal actions under different conditions. Thus, the Q-agent offers performance which is close to the best under different conditions. © 2024 Murk Marvi et al.
KW  - Learning algorithms
KW  - Markov processes
KW  - Mobile telecommunication systems
KW  - Reinforcement learning
KW  - Stochastic systems
KW  - Wireless networks
KW  - Condition
KW  - Learn+
KW  - Model free
KW  - Multi-radio access
KW  - Optimal actions
KW  - Performance
KW  - Q-learning algorithms
KW  - Radio access technologies
KW  - Stochastic nature
KW  - Users' experiences
KW  - Rats
PB  - Hindawi Limited
SN  - 20907141 (ISSN)
LA  - English
J2  - J. Comput. Netw. Commun.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: M. Marvi; Department of Computer Science and Information Technology, NED University of Engineering and Technology, Karachi, Pakistan; email: marvi@cloud.neduet.edu.pk
ER  -

TY  - CONF
AU  - Morgunov, E.F.
AU  - Alfimtsev, A.N.
TI  - The «Stag Hunt» Social Dilemma in Multi-Agent Reinforcement Learning
PY  - 2024
T2  - Proceedings of the 2024 6th International Youth Conference on Radio Electronics, Electrical and Power Engineering, REEPE 2024
DO  - 10.1109/REEPE60449.2024.10479770
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191255836&doi=10.1109%2fREEPE60449.2024.10479770&partnerID=40&md5=5dc01e6a93382ddfded650bd37219c39
AD  - Bauman Moscow State Technical University, Department of Informatics and Control Systems, Moscow, Russian Federation
AB  - This paper discusses the identification and resolution of social dilemmas in multi-agent reinforcement learning. A multi-agent environment was developed to simulate the movement of autonomous cars on a circular two-lane road, with two agent cars independently trained by a neural network. An independent Q-learning algorithm (IQN) was used for effective environment learning, allowing each agent to find its own strategy to improve learning efficiency. During the agents' training process, the «stag hunt» social dilemma was identified, which arose from agents refusing to cooperate with each other due to fear of betrayal. Experimental results showed that the IQN algorithm effectively resolves social dilemmas in various multi-agent scenarios. © 2024 IEEE.
KW  - deep learning
KW  - deep Q-network
KW  - moral AI
KW  - reinforcement learning
KW  - social dilemma
KW  - Autonomous agents
KW  - Deep learning
KW  - Learning algorithms
KW  - Multi agent systems
KW  - Autonomous car
KW  - Deep learning
KW  - Deep Q-network
KW  - Moral AI
KW  - Multi-agent environment
KW  - Multi-agent reinforcement learning
KW  - Reinforcement learnings
KW  - Social dilemmas
KW  - Two agents
KW  - Two-lane roads
KW  - Reinforcement learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835038289-1 (ISBN)
LA  - English
J2  - Proc. Int. Youth Conf. Radio Electron., Electr. Power Eng., REEPE
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 6th International Youth Conference on Radio Electronics, Electrical and Power Engineering, REEPE 2024; Conference date: 29 February 2024 through 2 March 2024; Conference code: 198733
ER  -

TY  - CONF
AU  - Shukla, A.
AU  - Pokhariya, H.S.
AU  - Michaelson, J.
AU  - Laxminarayanamma, K.
AU  - Kumar, M.
AU  - Krishna, O.
TI  - Distributed Deep Reinforcement Learning for Autonomous Iot Healthcare Devices in the Cloud
PY  - 2023
T2  - International Conference on Artificial Intelligence for Innovations in Healthcare Industries, ICAIIHI 2023
DO  - 10.1109/ICAIIHI57871.2023.10488976
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191504342&doi=10.1109%2fICAIIHI57871.2023.10488976&partnerID=40&md5=f85166c18c0f1c0dc71f64b664024ab7
AD  - Gla University, Department of Electronics and Communication, Mathura, India
AD  - Graphic Era Deemed to Be University, Department of Computer Science & Engineering, Uttarakhand, Dehradun, India
AD  - Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Chennai, India
AD  - Institute of Aeronautical Engineering, Department of Information Technology, Telangana, Hyderabad, India
AD  - Lloyd Institute of Engineering and Technology, Greater Noida, India
AD  - Lloyd Law College, Greater Noida, India
AB  - The ethical and philosophical problems concerning the cooperation of AI systems and human artists are also examined in this study. In addressing authorship, agency, and the very essence of creation, the changing position of artists as co-creators with intelligent algorithms is explored. It also looks at how AI can question and change conventional ideas of creative competence. Additionally, this study looks into how AI affects the promotion and distribution of art. AI-driven marketing tactics provide improved targeting of customers, personalized experiences, and optimal promotional efforts by utilizing insights based on data and predictive analytics. The study focuses on how these developments transform the relationships between galleries and artists and their patrons, ultimately fostering a more varied and inclusive art scene. The system's potential in many healthcare scenarios has been validated through simulations and practical experiments, which have received excellent feedback from healthcare providers. A critical review emphasizes the need to address practical deployment issues and data security concerns, while also highlighting the exciting convergence of IoT healthcare devices and DDRL. The paper ends with suggestions for more research, highlighting the significance of ethical issues, user interface improvements, and real-world validation. Through the smooth integration of DDRL-enhanced Internet of Things (IoT) medical equipment into clinical practice, our research eventually improves patient care and transforms the delivery of healthcare.  © 2023 IEEE.
KW  - data processing
KW  - healthcare
KW  - Internet of Things
KW  - learning process
KW  - technology
A2  - Swarnkar S.K.
A2  - Rathore Y.K.
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835033091-5 (ISBN)
LA  - English
J2  - Int. Conf. Artif. Intell. Innov. Healthc. Ind., ICAIIHI
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 1st International Conference on Artificial Intelligence for Innovations in Healthcare Industries, ICAIIHI 2023; Conference date: 29 December 2023 through 30 December 2023; Conference code: 198912
ER  -

TY  - JOUR
AU  - Watson, D.S.
TI  - On the Philosophy of Unsupervised Learning
PY  - 2023
T2  - Philosophy and Technology
VL  - 36
IS  - 2
C7  - 28
DO  - 10.1007/s13347-023-00635-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154026890&doi=10.1007%2fs13347-023-00635-6&partnerID=40&md5=2d917edf19b8c8e3347cfacc75d95706
AD  - Department of Informatics, King’s College London, London, United Kingdom
AB  - Unsupervised learning algorithms are widely used for many important statistical tasks with numerous applications in science and industry. Yet despite their prevalence, they have attracted remarkably little philosophical scrutiny to date. This stands in stark contrast to supervised and reinforcement learning algorithms, which have been widely studied and critically evaluated, often with an emphasis on ethical concerns. In this article, I analyze three canonical unsupervised learning problems: clustering, abstraction, and generative modeling. I argue that these methods raise unique epistemological and ontological questions, providing data-driven tools for discovering natural kinds and distinguishing essence from contingency. This analysis goes some way toward filling the lacuna in contemporary philosophical discourse on unsupervised learning, as well as bringing conceptual unity to a heterogeneous field more often described by what it is not (i.e., supervised or reinforcement learning) than by what it is. I submit that unsupervised learning is not just a legitimate subject of philosophical inquiry but perhaps the most fundamental branch of all AI. However, an uncritical overreliance on unsupervised methods poses major epistemic and ethical risks. I conclude by advocating for a pragmatic, error-statistical approach that embraces the opportunities and mitigates the challenges posed by this powerful class of algorithms. © 2023, The Author(s).
KW  - Abstraction
KW  - Clustering
KW  - Epistemology
KW  - Generative modeling
KW  - Machine learning
KW  - Metaphysics
PB  - Springer Science and Business Media B.V.
SN  - 22105433 (ISSN)
LA  - English
J2  - Philos. Technol.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 7; Correspondence Address: D.S. Watson; Department of Informatics, King’s College London, London, United Kingdom; email: david.watson@kcl.ac.uk
ER  -

TY  - JOUR
AU  - Blais, M.-A.
AU  - Akhloufi, M.A.
TI  - Drone Swarm Coordination Using Reinforcement Learning for Efficient Wildfires Fighting
PY  - 2024
T2  - SN Computer Science
VL  - 5
IS  - 3
C7  - 314
DO  - 10.1007/s42979-024-02650-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187642211&doi=10.1007%2fs42979-024-02650-6&partnerID=40&md5=35c57c95122fa922ed20450db7cf7f96
AD  - Dept. of Computer Science, Université de Moncton, Moncton, NB, Canada
AD  - Perception, Robotics, and Intelligent Machines (PRIME), Université de Moncton, Moncton, NB, Canada
AB  - Natural events, such as wildfires, pose a serious threat to the human population and cause significant environmental and economic damage. As climate change increases the frequency and intensity of extreme natural events, more efficient solutions are required to mitigate their impacts. One proposed solution is the usage of a swarm of drones and unmanned ground vehicles to extinguish wildfires. Controlling a swarm of drones is a challenging task that requires a complex control system capable of providing a good coordination between multiple vehicles. Artificial intelligence, particularly reinforcement learning, has been proposed as a solution for the autonomous control of a swarm. In this paper, we propose a conceptual system based on reinforcement learning and swarm robotics to efficiently combat wildfires. Our system consists of an overwatch drone and multiple payload drones which are responsible for carrying fire suppressants. The goal of our research is to guide a swarm of payload drones to a wildfire while preserving a tight swarm grouping. Each drone is individually controlled and aware of the position of the goal and the positions of other agents. To compare different algorithms, we tested a DQN, Rainbow DQN and FQF using a simulation and four AirSim scenarios. With our approach, we achieve impressive results in controlling a swarm of drones towards a common goal. Our research demonstrates the possibility of utilizing reinforcement learning to control a swarm of drones for wildfire fighting. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd 2024.
KW  - Drones
KW  - Intelligent system
KW  - Reinforcement learning
KW  - Swarm
KW  - Wildfires
PB  - Springer
SN  - 2662995X (ISSN)
LA  - English
J2  - SN COMPUT. SCI.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: M.-A. Blais; Dept. of Computer Science, Université de Moncton, Moncton, Canada; email: emb9357@umoncton.ca
ER  -

TY  - JOUR
AU  - Vouros, G.A.
TI  - Explainable Deep Reinforcement Learning: State of the Art and Challenges
PY  - 2022
T2  - ACM Computing Surveys
VL  - 55
IS  - 5
C7  - 92
DO  - 10.1145/3527448
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146307412&doi=10.1145%2f3527448&partnerID=40&md5=ffb5edf983cae1174db3f221f90141ef
AD  - University of Piraeus, Gr Lambraki 126, Piraeus, 18534, Greece
AB  - Interpretability, explainability, and transparency are key issues to introducing artificial intelligence methods in many critical domains. This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability, and fairness, and has important consequences toward keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. Although the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article, we aim to provide a review of state-of-the-art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators - that is, of those who make the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state-of-the-art methods, categorizing them into classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes by identifying open questions and important challenges.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
KW  - Deep learning
KW  - deep reinforcement learning
KW  - explainability
KW  - interpretability
KW  - transparency
KW  - Autonomous agents
KW  - Decision making
KW  - Deep learning
KW  - Ethical technology
KW  - Learning systems
KW  - Reinforcement learning
KW  - Artificial intelligence methods
KW  - Deep learning
KW  - Deep reinforcement learning
KW  - Explainability
KW  - Interpretability
KW  - Key Issues
KW  - Reinforcement learning method
KW  - Reinforcement learnings
KW  - State of the art
KW  - State-of-the-art methods
KW  - Transparency
PB  - Association for Computing Machinery
SN  - 03600300 (ISSN)
LA  - English
J2  - ACM Comput Surv
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 22; Correspondence Address: G.A. Vouros; University of Piraeus, Piraeus, Gr Lambraki 126, 18534, Greece; email: georgev@unipi.gr; CODEN: ACSUE
ER  -

TY  - CONF
AU  - Briganti, G.
TI  - ARTIFICIAL INTELLIGENCE IN PSYCHIATRY
PY  - 2023
T2  - Psychiatria Danubina
VL  - 35
SP  - 15
EP  - 19
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175593135&partnerID=40&md5=86ebcbbf1f95b22025980b009dd5f29f
AD  - Chair of Artificial Intelligence and Digital Medicine, Department of Neuroscience, Faculty of Medicine, University of Mons, Mons, Belgium
AD  - Department of Clinical Sciences, Faculty of Medicine, Université de Liège, Liège, Belgium
AD  - Laboratoire de Psychologie Médicale et Addictologie, CHU Brugmann, Faculté de Médecine, Université libre de Bruxelles, Brussels, Belgium
AB  - Artificial Intelligence (AI) has emerged as a powerful tool in various fields, including psychiatry. This paper explores the potential of AI in the diagnosis, treatment, and understanding of psychiatric conditions. We delve into the role of AI in psychiatry, discussing its applications, challenges, and future directions. We explore how AI techniques such as classification, hypothesis generation, and prediction are being used in psychiatry, with a specific focus on the detection and prediction of psychiatric conditions. We also discuss the ethical considerations and challenges in implementing AI in psychiatry and look towards the future of AI in this field. The paper highlights the potential of AI to enhance our understanding of psychiatric conditions, improve patient care, and drive innovation in psychiatric research. However, it also underscores the need for robust ethical frameworks and stringent data protection measures to ensure the responsible and effective use of AI in psychiatry. © 2023 Medicinska Naklada Zagreb. All rights reserved.
KW  - causality
KW  - diagnosis
KW  - machine learning
KW  - network models
KW  - Artificial Intelligence
KW  - Humans
KW  - Mental Disorders
KW  - Psychiatry
KW  - artificial intelligence
KW  - artificial neural network
KW  - ChatGPT
KW  - Conference Paper
KW  - confidentiality
KW  - data privacy
KW  - deep learning
KW  - ecological momentary assessment
KW  - feature extraction
KW  - human
KW  - informed consent
KW  - large language model
KW  - mental disease
KW  - natural language processing
KW  - neuroimaging
KW  - psychiatry
KW  - reinforcement learning (machine learning)
KW  - risk assessment
KW  - artificial intelligence
KW  - mental disease
PB  - Medicinska Naklada Zagreb
SN  - 03535053 (ISSN)
C2  - 37800199
LA  - English
J2  - Psychiatr. Danub.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: G. Briganti; Chair of Artificial Intelligence and Digital Medicine, Department of Neuroscience, Faculty of Medicine, University of Mons, Mons, Avenue du Champs de Mars 6, 7000, Belgium; email: giovanni.briganti@hotmail.com; CODEN: PSYDE
ER  -

TY  - JOUR
AU  - Ray, P.P.
TI  - Benchmarking, ethical alignment, and evaluation framework for conversational AI: Advancing responsible development of ChatGPT
PY  - 2023
T2  - BenchCouncil Transactions on Benchmarks, Standards and Evaluations
VL  - 3
IS  - 3
C7  - 100136
DO  - 10.1016/j.tbench.2023.100136
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173649842&doi=10.1016%2fj.tbench.2023.100136&partnerID=40&md5=5289a6a2306d6ae654d4b21093a7346c
AD  - Department of Computer Applications, Sikkim University, Sikkim, Gangtok, 737102, India
AB  - Conversational AI systems like ChatGPT have seen remarkable advancements in recent years, revolutionizing human–computer interactions. However, evaluating the performance and ethical implications of these systems remains a challenge. This paper delves into the creation of rigorous benchmarks, adaptable standards, and an intelligent evaluation methodology tailored specifically for ChatGPT. We meticulously analyze several prominent benchmarks, including GLUE, SuperGLUE, SQuAD, CoQA, Persona-Chat, DSTC, BIG-Bench, HELM and MMLU illuminating their strengths and limitations. This paper also scrutinizes the existing standards set by OpenAI, IEEE's Ethically Aligned Design, the Montreal Declaration, and Partnership on AI's Tenets, investigating their relevance to ChatGPT. Further, we propose adaptive standards that encapsulate ethical considerations, context adaptability, and community involvement. In terms of evaluation, we explore traditional methods like BLEU, ROUGE, METEOR, precision–recall, F1 score, perplexity, and user feedback, while also proposing a novel evaluation approach that harnesses the power of reinforcement learning. Our proposed evaluation framework is multidimensional, incorporating task-specific, real-world application, and multi-turn dialogue benchmarks. We perform feasibility analysis, SWOT analysis and adaptability analysis of the proposed framework. The framework highlights the significance of user feedback, integrating it as a core component of evaluation alongside subjective assessments and interactive evaluation sessions. By amalgamating these elements, this paper contributes to the development of a comprehensive evaluation framework that fosters responsible and impactful advancement in the field of conversational AI. © 2023 The Authors
KW  - Adaptive standards
KW  - Benchmarks
KW  - ChatGPT
KW  - Conversational AI
KW  - Evaluation framework
KW  - Intelligent evaluation
KW  - Human computer interaction
KW  - Philosophical aspects
KW  - Reinforcement learning
KW  - Adaptive standard
KW  - AI systems
KW  - Benchmark
KW  - ChatGPT
KW  - Conversational AI
KW  - Ethical implications
KW  - Evaluation framework
KW  - Intelligent evaluation
KW  - Performance
KW  - User feedback
KW  - Benchmarking
PB  - Elsevier B.V.
SN  - 27724859 (ISSN)
LA  - English
J2  - BenchCounc. Trans. Benchmarks, Stand. Evaluation
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5
ER  -

TY  - CONF
AU  - Han, G.
AU  - Choi, J.
AU  - Lee, H.
AU  - Kim, J.
TI  - Reinforcement Learning-Based Black-Box Model Inversion Attacks
PY  - 2023
T2  - Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition
VL  - 2023-June
SP  - 20504
EP  - 20513
DO  - 10.1109/CVPR52729.2023.01964
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173918285&doi=10.1109%2fCVPR52729.2023.01964&partnerID=40&md5=e37d7cbdfec36a9c79b4c9136a3708c6
AD  - School of Electrical Engineering, Kaist, South Korea
AB  - Model inversion attacks are a type of privacy attack that reconstructs private data used to train a machine learning model, solely by accessing the model. Recently, white-box model inversion attacks leveraging Generative Adversarial Networks (GANs) to distill knowledge from public datasets have been receiving great attention because of their excellent attack performance. On the other hand, current black-box model inversion attacks that utilize GANs suffer from issues such as being unable to guarantee the completion of the attack process within a predetermined number of query accesses or achieve the same level of performance as white-box attacks. To overcome these limitations, we propose a reinforcement learning-based black-box model inversion attack. We formulate the latent space search as a Markov Decision Process (MDP) problem and solve it with reinforcement learning. Our method utilizes the confidence scores of the generated images to provide rewards to an agent. Finally, the private data can be reconstructed using the latent vectors found by the agent trained in the MDP. The experiment results on various datasets and models demonstrate that our attack successfully recovers the private information of the target model by achieving state-of-the-art attack performance. We emphasize the importance of studies on privacy-preserving machine learning by proposing a more advanced black-box model inversion attack. © 2023 IEEE.
KW  - accountability
KW  - ethics in vision
KW  - fairness
KW  - privacy
KW  - Transparency
PB  - IEEE Computer Society
SN  - 10636919 (ISSN); 979-835030129-8 (ISBN)
LA  - English
J2  - Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2; Conference name: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023; Conference date: 18 June 2023 through 22 June 2023; Conference code: 192023; CODEN: PIVRE
ER  -

TY  - CONF
AU  - Sequeira, P.
AU  - Gervasio, M.
TI  - IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit Based on Analyses of Interestingness
PY  - 2023
T2  - Communications in Computer and Information Science
VL  - 1901 CCIS
SP  - 373
EP  - 396
DO  - 10.1007/978-3-031-44064-9_20
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176907546&doi=10.1007%2f978-3-031-44064-9_20&partnerID=40&md5=fe00aca29350ac125fb90202e6286961
AD  - SRI International, 333 Ravenswood Ave., Menlo Park, 94025, CA, United States
AB  - In recent years, advances in deep learning have resulted in a plethora of successes in the use of reinforcement learning (RL) to solve complex sequential decision tasks with high-dimensional inputs. However, existing systems lack the necessary mechanisms to provide humans with a holistic view of their competence, presenting an impediment to their adoption, particularly in critical applications where the decisions an agent makes can have significant consequences. Yet, existing RL-based systems are essentially competency-unaware in that they lack the necessary interpretation mechanisms to allow human operators to have an insightful, holistic view of their competency. Towards more explainable Deep RL (xDRL), we propose a new framework based on analyses of interestingness. Our tool provides various measures of RL agent competence stemming from interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit. We showcase the use of our framework by applying the proposed pipeline in a set of scenarios of varying complexity. We empirically assess the capability of the approach in identifying agent behavior patterns and competency-controlling conditions, and the task elements mostly responsible for an agent’s competence, based on global and local analyses of interestingness. Overall, we show that our framework can provide agent designers with insights about RL agent competence, both their capabilities and limitations, enabling more informed decisions about interventions, additional training, and other interactions in collaborative human-machine settings. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
KW  - Applications of xAI
KW  - Explainable AI
KW  - Global and Local Explanations
KW  - Interestingness Analysis
KW  - Reinforcement Learning
KW  - Deep learning
KW  - Learning systems
KW  - Application of xAI
KW  - Decision task
KW  - Explainable AI
KW  - Global and local explanation
KW  - Holistic view
KW  - Interestingness
KW  - Interestingness analyse
KW  - Reinforcement learning agent
KW  - Reinforcement learnings
KW  - Sequential decisions
KW  - Reinforcement learning
A2  - Longo L.
PB  - Springer Science and Business Media Deutschland GmbH
SN  - 18650929 (ISSN); 978-303144063-2 (ISBN)
LA  - English
J2  - Commun. Comput. Info. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: P. Sequeira; SRI International, Menlo Park, 333 Ravenswood Ave., 94025, United States; email: pedro.sequeira@sri.com; Conference name: 1st World Conference on eXplainable Artificial Intelligence, xAI 2023; Conference date: 26 July 2023 through 28 July 2023; Conference code: 303319
ER  -

TY  - JOUR
AU  - Erdődi, L.
AU  - Zennaro, F.M.
TI  - The Agent Web Model: modeling web hacking for reinforcement learning
PY  - 2022
T2  - International Journal of Information Security
VL  - 21
IS  - 2
SP  - 293
EP  - 309
DO  - 10.1007/s10207-021-00554-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107769457&doi=10.1007%2fs10207-021-00554-7&partnerID=40&md5=79890e019eec8a1d18b56f95b046b533
AD  - Department of Informatics, University of Oslo, Oslo, 0316, Norway
AB  - Website hacking is a frequent attack type used by malicious actors to obtain confidential information, modify the integrity of web pages or make websites unavailable. The tools used by attackers are becoming more and more automated and sophisticated, and malicious machine learning agents seem to be the next development in this line. In order to provide ethical hackers with similar tools, and to understand the impact and the limitations of artificial agents, we present in this paper a model that formalizes web hacking tasks for reinforcement learning agents. Our model, named Agent Web Model, considers web hacking as a capture-the-flag style challenge, and it defines reinforcement learning problems at seven different levels of abstraction. We discuss the complexity of these problems in terms of actions and states an agent has to deal with, and we show that such a model allows to represent most of the relevant web vulnerabilities. Aware that the driver of advances in reinforcement learning is the availability of standardized challenges, we provide an implementation for the first three abstraction layers, in the hope that the community would consider these challenges in order to develop intelligent web hacking agents. © 2021, The Author(s).
KW  - Agent Web Model
KW  - Capture the flag
KW  - Penetration testing
KW  - Reinforcement learning
KW  - Intelligent agents
KW  - Learning systems
KW  - Personal computing
KW  - Websites
KW  - Abstraction layer
KW  - Artificial agents
KW  - Capture the flag
KW  - Confidential information
KW  - Intelligent web
KW  - Levels of abstraction
KW  - Machine learning agents
KW  - Reinforcement learning agent
KW  - Reinforcement learning
PB  - Springer Science and Business Media Deutschland GmbH
SN  - 16155262 (ISSN)
LA  - English
J2  - Int. J. Inf. Secur.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 6; Correspondence Address: L. Erdődi; Department of Informatics, University of Oslo, Oslo, 0316, Norway; email: laszloe@ifi.uio.no
ER  -

TY  - JOUR
AU  - Geng, B.
TI  - Retracted Article: Traffic prediction and transmission scheduling of artificial intelligence-driven cognitive wireless networks (INTERNATIONAL JOURNAL OF COMPUTERS AND APPLICATIONS, (2023), 45, 1, (96–104))
PY  - 2023
T2  - International Journal of Computers and Applications
VL  - 45
IS  - 1
SP  - 96
EP  - 104
DO  - 10.1080/1206212X.2019.1706812
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077147601&doi=10.1080%2f1206212X.2019.1706812&partnerID=40&md5=0ec2d47312d8b99137a2ecf1380f51f9
AD  - Department of Primary Education, North Sichuan College of Preschool Teacher Education, Guangyuan, China
AB  - We, the Editor and Publisher of the International Journal of Computers and Applications, have retracted the following article which was part of the Special Issue on Advanced Security Techniques for Cloud Computing and Big Data - New Directions: Bin Geng (2019) Traffic prediction and transmission scheduling of artificial intelligence-driven cognitive wireless networks, International Journal of Computers and Applications, DOI: 10.1080/1206212X.2019.1706812 Since publication, it came to our attention that the articles published in this Special Issue were not reviewed fully in line with the journal's peer review standards and policy. We did not find any evidence of misconduct by the authors. However, in order to ensure full assessment has been conducted, we sought expert advice on the validity and quality of the published articles from independent peer reviewers. Following this post publication peer review, the Editor has determined that the articles do not meet the required scholarly standards to remain published in the journal, and therefore has taken the decision to retract. The concerns raised have been shared with the authors and they have been given the opportunity to respond. The authors have been informed about the retraction of the article.   We have been informed in our decision-making by our policy on publishing ethics and integrity and the COPE guidelines on retractions. The retracted articles will remain online to maintain the scholarly record, but they will be digitally watermarked on each page as ‘Retracted’. © 2019 Informa UK Limited, trading as Taylor & Francis Group.
KW  - Computational complexity
KW  - Forecasting
KW  - Iterative methods
KW  - Learning algorithms
KW  - Markov processes
KW  - Optimal systems
KW  - Reinforcement learning
KW  - Scheduling
KW  - Scheduling algorithms
KW  - Wireless networks
KW  - Cognitive network
KW  - Deep learning
KW  - Optimal strategies
KW  - Q-learning
KW  - Relay node
KW  - System state
KW  - Traffic loads
KW  - Traffic prediction
KW  - Traffic transmission
KW  - Transmission scheduling
KW  - Routing algorithms
PB  - Taylor and Francis Ltd.
SN  - 1206212X (ISSN)
LA  - English
J2  - Int J Comput Appl
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: B. Geng; Department of Primary Education, North Sichuan College of Preschool Teacher Education, Guangyuan, Sichuan, 628017, China; email: gengbin_qq@126.com; CODEN: IJCAF
ER  -

TY  - CONF
AU  - Hussain, M.
TI  - Can AlphaGo be apt subjects for Praise/Blame for "Move 37"?
PY  - 2023
T2  - AIES 2023 - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society
SP  - 977
EP  - 979
DO  - 10.1145/3600211.3604730
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173612369&doi=10.1145%2f3600211.3604730&partnerID=40&md5=46444f8d9415a125676c421a3e19b20c
AD  - Department of Humanities and Social Sciences, Indian Institute of Technology Dharwad, Dharwad, Karnataka, India
AB  - This paper examines whether machines (algorithms/programs/ AI systems) are apt subjects for praise or blame for some actions or performances. I consider "Move 37"of AlphaGo as a case study. DeepMind's AlphaGo is an AI algorithm developed to play the game of Go. The AlphaGo utilizes Deep Neural Networks. As AlphaGo is trained through reinforcement learning, the AI algorithm can improve itself over a period of time. Such AI models can go beyond the intended task and perform novel and unpredictable functions. There is a surprise element associated with "Move 37". "Move 37"not only surprises the Go players, the programmers, but also whoever is informed of this unpredicted move. Does someone or something deserve praise or blame for the surprise? If so, who or what deserves the praise or blame for "Move 37"? The programmer cannot be praised for "Move 37", which is either surprising or was not intended or imagined at all. At the same time, would we accept that neither the algorithm deserves praise for the unpredicted move that the algorithm allowed the program to make? From this, would we accept that since neither the programmer nor the algorithm/AI system deserves the praise, there is such a good or exciting move for which no one or nothing could be praised? Would we say this unpredictable move is a move for which no one deserves praise or blame? Wouldn't there be at least a few who were surprised by the unpredictable move? Should we say that for this pleasant surprise, no one deserves praise? Nonetheless, for us, specifically regarding the particular unpredictable move, we firmly find it counterintuitive to say that there is an exciting move for which no one deserves praise. The surprise element is the result of the property that belongs to the algorithm. It seems quite difficult for us to accept that no one deserves praise for "Move 37"or for similar moves. Therefore, someone or something deserves praise which is a matter of scrutiny.  © 2023 Owner/Author.
KW  - "Move 37"
KW  - AlphaGo
KW  - Artificial Moral Agency(AMA)
KW  - Blame and Praise
KW  - Causal Responsibility
KW  - Machine morality
KW  - Moral Responsibility
KW  - Reinforcement learning
KW  - "move 37"
KW  - AI algorithms
KW  - AI systems
KW  - Alphago
KW  - Artificial moral agency
KW  - Blame and praise
KW  - Causal responsibility
KW  - Machine algorithm
KW  - Machine morality
KW  - Moral responsibility
KW  - Deep neural networks
PB  - Association for Computing Machinery, Inc
SN  - 979-840070231-0 (ISBN)
LA  - English
J2  - AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: M. Hussain; Department of Humanities and Social Sciences, Indian Institute of Technology Dharwad, Karnataka, Dharwad, India; email: 193101002@iitdh.ac.in; Conference name: 2023 AAAI / ACM Conference on Artificial Intelligence, Ethics, and Society, AIES 2023; Conference date: 8 August 2023 through 10 August 2023; Conference code: 192626
ER  -

TY  - JOUR
AU  - Martín-Guerrero, J.D.
AU  - Lamata, L.
TI  - Quantum Machine Learning: A tutorial
PY  - 2022
T2  - Neurocomputing
VL  - 470
SP  - 457
EP  - 461
DO  - 10.1016/j.neucom.2021.02.102
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111544681&doi=10.1016%2fj.neucom.2021.02.102&partnerID=40&md5=492cd9ffa3c7f1ec904427a3e2e06dd2
AD  - Departament d'Enginyeria Electrònica, ETSE-UV, Universitat de València (UV), Spain
AD  - Departamento de Física Atómica, Molecular y Nuclear, Universidad de Sevilla, Sevilla, 41080, Spain
AB  - This tutorial provides an overview of Quantum Machine Learning (QML), a relatively novel discipline that brings together concepts from Machine Learning (ML), Quantum Computing (QC) and Quantum Information (QI). The great development experienced by QC, partly due to the involvement of giant technological companies as well as the popularity and success of ML have been responsible of making QML one of the main streams for researchers working on fuzzy borders between Physics, Mathematics and Computer Science. A possible, although arguably coarse, classification of QML methods may be based on those approaches that make use of ML in a quantum experimentation environment and those others that take advantage of QC and QI to find out alternative and enhanced solutions to problems driven by data, oftentimes offering a considerable speedup and improved performances as a result of tackling problems from a complete different standpoint. Several examples will be provided to illustrate both classes of methods. © 2021 The Authors
KW  - Computational speed-up
KW  - Quantum autoencoders
KW  - Quantum clustering
KW  - Quantum computing
KW  - Quantum reinforcement learning
KW  - Quantum-inspired learning algorithms
KW  - Quantum computers
KW  - Quantum optics
KW  - Reinforcement learning
KW  - Coarse classification
KW  - Computational speed-up
KW  - Machine-learning
KW  - Quantum autoencoder
KW  - Quantum clustering
KW  - Quantum Computing
KW  - Quantum Information
KW  - Quantum machines
KW  - Quantum reinforcement learning
KW  - Quantum-inspired learning algorithm
KW  - Article
KW  - Boltzmann machine
KW  - data extraction
KW  - Hopfield neural network
KW  - information retrieval
KW  - machine learning
KW  - measurement accuracy
KW  - quantum autoencoders
KW  - quantum clustering
KW  - quantum computing
KW  - quantum information
KW  - quantum machine learning
KW  - quantum reinforcement learning
KW  - Learning algorithms
PB  - Elsevier B.V.
SN  - 09252312 (ISSN)
LA  - English
J2  - Neurocomputing
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 30; Correspondence Address: J.D. Martín-Guerrero; Departament d'Enginyeria Electrònica, ETSE-UV, Universitat de València (UV), Spain; email: jose.d.martin@uv.es; CODEN: NRCGE
ER  -

TY  - CONF
AU  - Alberta, M.
AU  - Lee, D.
AU  - Khan, O.
TI  - Can Robots Be Responsible: A Bumper Theory Approach to Robot Moral Conditioning
PY  - 2023
T2  - Proceedings - 2023 17th IEEE International Conference on Robotic Computing, IRC 2023
SP  - 284
EP  - 287
DO  - 10.1109/IRC59093.2023.00053
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190066455&doi=10.1109%2fIRC59093.2023.00053&partnerID=40&md5=79e3b793428250a7f5f76006da8fb435
AD  - Department of Communication University of the Pacific (UOP), Stockton, 95211, CA, United States
AD  - Dep’t of Computer Engineering SOECS, University of the Pacific, Stockton, 95211, CA, United States
AD  - Department of Computer Sci SOECS, UOP, Stockton, CA, United States
AB  - This paper examines the main unresolved theoretical controversy question, centered on the gap issue when responsibility meets ambiguity. Our scientific inquiry aims to discuss what is missing in this issue and promote a dialogue that hopes to begin to resolve it. We are proposing a moral framework and planning a series of experiments that, when applied, may start the process of resolving the responsibility gap to better understand moral cognition. The controversy surrounding morality in autonomous robots or vehicles, synonymous with the responsibility gap problem, is being investigated by bumper theory related to the severity of casualty, indicating an immoral level, with a moral rulebook. The experiment proposes moral conditioning to drive the robot’s behavior to avoid collision and update the level of morality of the robot. Our scientific method should also promote a dialogue that contends robots can be prescribed a moral rulebook that could give the robot human-like moral cognition. As an experiment to validate robots enabled to morally be conditioned, robots are developed to detect humans, cars, or non-humans using machine learning. A fast single-stage YOLO is used for human detection and multitask convolutional neural network MTCNN is used for detecting human-faces. ©2023 IEEE.
KW  - artificial intelligence
KW  - CNN
KW  - deep learning
KW  - human-robot interaction
KW  - machine learning
KW  - moral conditioning
KW  - reinforcement learning
KW  - robot cognition
KW  - self-driving or autonomous vehicle
KW  - YOLO
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Human robot interaction
KW  - Intelligent robots
KW  - Learning systems
KW  - Reinforcement learning
KW  - Autonomous Vehicles
KW  - Deep learning
KW  - Humans-robot interactions
KW  - Machine-learning
KW  - Moral conditioning
KW  - Reinforcement learnings
KW  - Robot cognition
KW  - Self drivings
KW  - Self-driving or autonomous vehicle
KW  - YOLO
KW  - Autonomous vehicles
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835039574-7 (ISBN)
LA  - English
J2  - Proc. - IEEE Int. Conf. Robot. Comput., IRC
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: M. Alberta; Department of Communication University of the Pacific (UOP), Stockton, 95211, United States; email: malberta@pacific.edu; Conference name: 17th IEEE International Conference on Robotic Computing, IRC 2023; Conference date: 11 December 2023 through 13 December 2023; Conference code: 198411
ER  -

TY  - CONF
AU  - Patel, J.
TI  - A Comprehensive Review of Innovations in Artificial Intelligence and Enhancing Machine Learning Model Precision and Efficiency
PY  - 2023
T2  - 2023 14th International Conference on Computing Communication and Networking Technologies, ICCCNT 2023
DO  - 10.1109/ICCCNT56998.2023.10306844
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179854429&doi=10.1109%2fICCCNT56998.2023.10306844&partnerID=40&md5=98ef98d0efd0a43264a655e21a1f126c
AD  - Sardar Vallabhbhai Patel Institute of Technology, Department of Information Technology (IT), Gujarat, India
AB  - This paper presents a comprehensive review of recent innovations in artificial intelligence and algorithm development aimed at enhancing machine learning precision and efficiency. The review covers various techniques, including deep learning, reinforcement learning, and transfer learning, and their applications in different domains. The paper also discusses recent advancements in hardware and software infrastructure that have improved the efficiency of machine learning algorithms. The goal of this paper is to provide a comprehensive overview of the current state of the art in machine learning and to identify promising directions for future research. Furthermore, the paper highlights the challenges and limitations of existing techniques and provides insights into possible solutions. It also discusses the ethical implications of using machine learning in various domains, such as healthcare, finance, and social media. The review identifies the need for more transparent and explainable algorithms and emphasizes the importance of addressing bias and fairness issues in machine learning. © 2023 IEEE.
KW  - Artificial intelligence
KW  - Deep learning
KW  - IoT
KW  - neural network
KW  - NLP
KW  - recommendation system
KW  - Deep learning
KW  - Internet of things
KW  - Learning algorithms
KW  - Learning systems
KW  - Reinforcement learning
KW  - Transfer learning
KW  - Algorithms development
KW  - Deep learning
KW  - IoT
KW  - Learning reinforcements
KW  - Machine learning models
KW  - Machine-learning
KW  - Model efficiency
KW  - Modeling precision
KW  - Neural-networks
KW  - Reinforcement learnings
KW  - Efficiency
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835033509-5 (ISBN)
LA  - English
J2  - Int. Conf. Comput. Commun. Netw. Technol., ICCCNT
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: J. Patel; Sardar Vallabhbhai Patel Institute of Technology, Department of Information Technology (IT), Gujarat, India; email: jamnesh.patel15@gmail.com; Conference name: 14th International Conference on Computing Communication and Networking Technologies, ICCCNT 2023; Conference date: 6 July 2023 through 8 July 2023; Conference code: 194774
ER  -

TY  - CONF
AU  - Tsuchiya, Y.
AU  - Mori, Y.
AU  - Egi, M.
TI  - Explainable Reinforcement Learning Based on Q-Value Decomposition by Expected State Transitions
PY  - 2023
T2  - CEUR Workshop Proceedings
VL  - 3433
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166474332&partnerID=40&md5=8420611a0481c46252f139035a5bdd6c
AD  - Hitachi, Ltd., 1-280, Higashi-koigakubo, Kokubunzi, Tokyo, 185-8601, Japan
AB  - Because of the high control and planning performance, Reinforcement Learning (RL) is increasingly being adopted in a wide range of applications, including socially responsible tasks. This has led to a rapid advancement in the research of Explainable RL, and one commonly-used approach is past-oriented explanation; highlighting the factors in the current state that drive an agent to take an action. However, the agent is trained in anticipation of future rewards, hence, it is crucial to provide future-oriented explanations that demonstrate what the agent expects the action to achieve. To address the challenges of computational complexity and comprehensiveness of the explanation, we propose a novel method for interpreting the intention behind the agent's actions by introducing state-wise critics, which are additional neural networks that estimate the Q-value for each set of state transitions defined by users. By incorporating the RL model as the target network within the critics' loss function, the resulting explanation is aligned with the Q-value estimated by the agent. To evaluate the validity of the critics, we conducted an experiment of resource pre-allocation to measure the power outage risks. The results indicate that a relatively small number of critics are sufficient even with many combinations of states.  © 2023 Copyright for this paper by its authors.
KW  - Explainable AI
KW  - Future-oriented explanation
KW  - Interpretability
KW  - Q-value
KW  - Reinforcement learning
KW  - State transition
KW  - Risk assessment
KW  - 'current
KW  - Explainable AI
KW  - Future-oriented explanation
KW  - Interpretability
KW  - Novel methods
KW  - Performance
KW  - Q-values
KW  - Reinforcement learnings
KW  - State transitions
KW  - Value decomposition
KW  - Reinforcement learning
A2  - Martin A.
A2  - Fill H.-G.
A2  - Gerber A.
A2  - Hinkelmann K.
A2  - Lenat D.
A2  - Stolle R.
A2  - van Harmelen F.
PB  - CEUR-WS
SN  - 16130073 (ISSN)
LA  - English
J2  - CEUR Workshop Proc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: Y. Tsuchiya; Hitachi, Ltd., Tokyo, 1-280, Higashi-koigakubo, Kokubunzi, 185-8601, Japan; email: yuta.tsuchiya.gf@hitachi.com; Conference name: AAAI 2023 Spring Symposium on Challenges Requiring the Combination of Machine Learning and Knowledge Engineering, AAAI-MAKE 2023; Conference date: 27 March 2023 through 29 March 2023; Conference code: 190483
ER  -

TY  - CONF
AU  - Xue, T.
AU  - Wu, M.
AU  - Lu, L.
AU  - Wang, H.
AU  - Dong, H.
AU  - Chen, B.
TI  - Learning Gradient Fields for Scalable and Generalizable Irregular Packing
PY  - 2023
T2  - Proceedings - SIGGRAPH Asia 2023 Conference Papers, SA 2023
C7  - 105
DO  - 10.1145/3610548.3618235
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181764292&doi=10.1145%2f3610548.3618235&partnerID=40&md5=185ce1a254ab5d2027ae59a9e9b36aa2
AD  - Shandong University, China
AD  - Peking University, China
AB  - The packing problem, also known as cutting or nesting, has diverse applications in logistics, manufacturing, layout design, and atlas generation. It involves arranging irregularly shaped pieces to minimize waste while avoiding overlap. Recent advances in machine learning, particularly reinforcement learning, have shown promise in addressing the packing problem. In this work, we delve deeper into a novel machine learning-based approach that formulates the packing problem as conditional generative modeling. To tackle the challenges of irregular packing, including object validity constraints and collision avoidance, our method employs the score-based diffusion model to learn a series of gradient fields. These gradient fields encode the correlations between constraint satisfaction and the spatial relationships of polygons, learned from teacher examples. During the testing phase, packing solutions are generated using a coarse-to-fine refinement mechanism guided by the learned gradient fields. To enhance packing feasibility and optimality, we introduce two key architectural designs: multi-scale feature extraction and coarse-to-fine relation extraction. We conduct experiments on two typical industrial packing domains, considering translations only. Empirically, our approach demonstrates spatial utilization rates comparable to, or even surpassing, those achieved by the teacher algorithm responsible for training data generation. Additionally, it exhibits some level of generalization to shape variations. We are hopeful that this method could pave the way for new possibilities in solving the packing problem.  © 2023 ACM.
KW  - arrangement
KW  - gradient field
KW  - irregular packing
KW  - neural networks
KW  - Personnel training
KW  - Reinforcement learning
KW  - Arrangement
KW  - Coarse to fine
KW  - Diverse applications
KW  - Gradient fields
KW  - Irregular packing
KW  - Layout designs
KW  - Machine-learning
KW  - Neural-networks
KW  - Packing problems
KW  - Teachers'
KW  - Extraction
A2  - Spencer S.N.
PB  - Association for Computing Machinery, Inc
SN  - 979-840070315-7 (ISBN)
LA  - English
J2  - Proc. - SIGGRAPH Asia Conf. Pap., SA
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: L. Lu; Shandong University, China; email: lulin.linda@gmail.com; Conference name: 2023 SIGGRAPH Asia 2023 Conference Papers, SA 2023; Conference date: 12 December 2023 through 15 December 2023; Conference code: 195643
ER  -

TY  - CONF
AU  - Nithya, T.
AU  - Nivas Kumar, V.
AU  - Gayathri, S.
AU  - Deepa, S.
AU  - Varun, C.M.
AU  - Siva Subramanian, R.
TI  - A Comprehensive Survey of Machine Learning: Advancements, Applications, and Challenges
PY  - 2023
T2  - Proceedings of the 2023 2nd International Conference on Augmented Intelligence and Sustainable Systems, ICAISS 2023
SP  - 354
EP  - 361
DO  - 10.1109/ICAISS58487.2023.10250547
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173618577&doi=10.1109%2fICAISS58487.2023.10250547&partnerID=40&md5=66ac199ed7f6d2e19a07cd1b14769127
AD  - Rajalakshmi Institute of Technology, Dept of CSE, India
AD  - Jawahar Engineering College, Dept of Ece, India
AD  - S.A Engineering College, Dept of CSE, India
AD  - Rmd Engineering College, Dept of Csbs, India
AD  - R.M.K. Engineering College, Dept of Csbs, India
AD  - R.M.K. College of Engg and Tech, Dept of CSE, India
AB  - This article provides a comprehensive review of the subject of machine learning. In addition to covering the subject in depth, the essay also investigates the many applications, difficulties, and techniques that are used in this rapidly evolving field. This article makes an attempt to synthesize and synthesize the most recent research and knowledge on machine learning. It does so by offering insights into fundamental principles and methodologies, as well as fresh advancements and practical applications. This study analyzes how machine learning can be used in different fields, such as image recognition, natural language processing, and recommendation systems. Machine Learning (ML) techniques, such as supervised learning, unsupervised learning, and reinforcement learning are analyzed. This study also investigates the limitations and challenges presented by machine learning, including interpretability, data quality, and ethical considerations. This study provides a comprehensive overview on the topic, which will pave way for more research and advancements in the field of machine learning. © 2023 IEEE.
KW  - Machine Learning
KW  - Reinforcement Learning
KW  - Supervised Learning
KW  - Unsupervised Learning
KW  - Image recognition
KW  - Learning algorithms
KW  - Learning systems
KW  - Natural language processing systems
KW  - Supervised learning
KW  - Unsupervised learning
KW  - Data quality
KW  - Ethical considerations
KW  - Fundamental principles
KW  - Interpretability
KW  - Machine learning techniques
KW  - Machine-learning
KW  - On-machines
KW  - Recent researches
KW  - Reinforcement learnings
KW  - Reinforcement learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835032579-9 (ISBN)
LA  - English
J2  - Proc. Int. Conf. Augment. Intell. Sustain. Syst., ICAISS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5; Correspondence Address: T. Nithya; Rajalakshmi Institute of Technology, Dept of CSE, India; email: nithya.t@ritchennai.edu.in; Conference name: 2nd International Conference on Augmented Intelligence and Sustainable Systems, ICAISS 2023; Conference date: 23 August 2023 through 25 August 2023; Conference code: 192825
ER  -

TY  - CONF
AU  - Yang, C.
AU  - Li, X.
AU  - Baracaldo, N.
AU  - Shah, N.
AU  - He, C.
AU  - Lyu, L.
AU  - Sun, L.
AU  - Avestimehr, S.
TI  - The 1st International Workshop on Federated Learning with Graph Data (FedGraph)
PY  - 2022
T2  - International Conference on Information and Knowledge Management, Proceedings
SP  - 5179
EP  - 5180
DO  - 10.1145/3511808.3557495
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140932443&doi=10.1145%2f3511808.3557495&partnerID=40&md5=d560a37b60578b51fefdc7fd0f2ecb77
AD  - Emory University, Atlanta, GA, United States
AD  - University of British Columbia, Vancouver, BC, Canada
AD  - Ibm Research, San Jose, CA, United States
AD  - Snap Research, Seattle, WA, United States
AD  - FedML Inc, Los Angeles, CA, United States
AD  - Sony Ai, Tokyo, Japan
AD  - Lehigh University, Bethlehem, PA, United States
AD  - University of Southern California, Los Angeles, CA, United States
AB  - The field of graph data mining, one of the most important AI research areas, has been revolutionized by graph neural networks (GNNs), which benefit from training on real-world graph data with millions to billions of nodes and links. Unfortunately, the training data and process of GNNs involving graphs beyond millions of nodes are extremely costly on a centralized server, if not impossible. Moreover, due to the increasing concerns about data privacy, emerging data from realistic applications are naturally fragmented, forming distributed private graphs of multiple "data silos", among which direct transferring of data is forbidden. The nascent field of federated learning (FL), which aims to enable individual clients to jointly train their models while keeping their local data decentralized and completely private, is a promising paradigm for large-scale distributed and private training of GNNs. øurs aims to bring together researchers from different backgrounds with a common interest in how to extend current FL algorithms to operate with graph data models such as GNNs. FL is an extremely hot topic of large commercial interest and has been intensively explored for machine learning with visual and textual data. The exploration from graph mining researchers and industrial practitioners is timely catching up just recently. There are many unexplored challenges and opportunities, which urges the establishment of an organized and open community to collaboratively advance the science behind it. The prospective participants of this workshop will include researchers and practitioners from both graph mining and federated learning communities, whose interests include, but are not limited to: graph analysis and mining, heterogeneous network modeling, complex data mining, large-scale machine learning, distributed systems, optimization, meta-learning, reinforcement learning, privacy, robustness, explainability, fairness, ethics, and trustworthiness. © 2022 Owner/Author.
KW  - data privacy
KW  - distributed learning
KW  - federated learning
KW  - graph mining
KW  - graph neural networks
KW  - Data mining
KW  - Graph neural networks
KW  - Graph theory
KW  - Heterogeneous networks
KW  - Network security
KW  - Reinforcement learning
KW  - Distributed learning
KW  - Federated learning
KW  - Graph data
KW  - Graph mining
KW  - Graph neural networks
KW  - International workshops
KW  - Nodes and links
KW  - Real-world graphs
KW  - Research areas
KW  - Training process
KW  - Data privacy
PB  - Association for Computing Machinery
SN  - 978-145039236-5 (ISBN)
LA  - English
J2  - Int Conf Inf Knowledge Manage
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: C. Yang; Emory University, Atlanta, United States; email: j.carlyang@emory.edu; Conference name: 31st ACM International Conference on Information and Knowledge Management, CIKM 2022; Conference date: 17 October 2022 through 21 October 2022; Conference code: 183495
ER  -

TY  - JOUR
AU  - Greene, T.
AU  - Shmueli, G.
AU  - Ray, S.
TI  - Taking the Person Seriously: Ethically Aware IS Research in the Era of Reinforcement Learning-Based Personalization
PY  - 2023
T2  - Journal of the Association for Information Systems
VL  - 24
IS  - 6
C7  - 6
SP  - 1527
EP  - 1561
DO  - 10.17705/1jais.00800
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176908068&doi=10.17705%2f1jais.00800&partnerID=40&md5=3680717e897cca5d89204e0568517470
AD  - Department of Digitalization, Copenhagen Business School, Denmark
AD  - Institute of Service Science, National Tsing Hua University, Taiwan
AB  - Advances in reinforcement learning and implicit data collection on large-scale commercial platforms mark the beginning of a new era of personalization aimed at the adaptive control of human user environments. We present five emergent features of this new paradigm of personalization that endanger persons and societies at scale and analyze their potential to reduce personal autonomy, destabilize social and political systems, and facilitate mass surveillance and social control, among other concerns. We argue that current data protection laws, most notably the European Union’s General Data Protection Regulation, are limited in their ability to adequately address many of these issues. Nevertheless, we believe that IS researchers are well-situated to engage with and investigate this new era of personalization. We propose three distinct directions for ethically aware reinforcement learning-based personalization research uniquely suited to the strengths of IS researchers across the sociotechnical spectrum. © 2023 by the Association for Information Systems.
KW  - AI Ethics
KW  - Data Protection
KW  - Digital Platforms
KW  - Personalization
KW  - Reinforcement Learning
KW  - Sociotechnical
KW  - Data privacy
KW  - E-learning
KW  - Ethical technology
KW  - Laws and legislation
KW  - Learning systems
KW  - Adaptive Control
KW  - AI ethic
KW  - Data collection
KW  - Digital platforms
KW  - Human users
KW  - Large-scales
KW  - Personal autonomy
KW  - Personalizations
KW  - Reinforcement learnings
KW  - Sociotechnical
KW  - Reinforcement learning
PB  - Association for Information Systems
SN  - 15583457 (ISSN)
LA  - English
J2  - J. Assoc. Inf. Syst.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 3
ER  -

TY  - CONF
AU  - Gilbert, T.K.
AU  - Lambert, N.
AU  - Dean, S.
AU  - Zick, T.
AU  - Snoswell, A.
AU  - Mehta, S.
TI  - Reward Reports for Reinforcement Learning
PY  - 2023
T2  - AIES 2023 - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society
SP  - 84
EP  - 130
DO  - 10.1145/3600211.3604698
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173611301&doi=10.1145%2f3600211.3604698&partnerID=40&md5=48f1b75dfdbe5bcd67b205d9a5a59bd1
AD  - Digital Life Initiative, Cornell Tech, New York, NY, United States
AD  - HuggingFace, Berkeley, CA, United States
AD  - Cornell University, Ithaca, NY, United States
AD  - Harvard Law School, Boston, MA, United States
AD  - Centre for Automated Decision-Making and Society, Queensland University of Technology Brisbane, QLD, Australia
AD  - Columbia University, New York, NY, United States
AB  - Building systems that are good for society in the face of complex societal effects requires a dynamic approach. Recent approaches to machine learning (ML) documentation have demonstrated the promise of discursive frameworks for deliberation about these complexities. However, these developments have been grounded in a static ML paradigm, leaving the role of feedback and post-deployment performance unexamined. Meanwhile, recent work in reinforcement learning has shown that the effects of feedback and optimization objectives on system behavior can be wide-ranging and unpredictable. In this paper we sketch a framework for documenting deployed and iteratively updated learning systems, which we call Reward Reports. Taking inspiration from technical concepts in reinforcement learning, we outline Reward Reports as living documents that track updates to design choices and assumptions behind what a particular automated system is optimizing for. They are intended to track dynamic phenomena arising from system deployment, rather than merely static properties of models or data. After presenting the elements of a Reward Report, we discuss a concrete example: Meta's BlenderBot 3 chatbot. Several others for game-playing (DeepMind's MuZero), content recommendation (MovieLens), and traffic control (Project Flow) are included in the appendix.  © 2023 ACM.
KW  - disaggregated evaluation
KW  - documentation
KW  - ethical considerations
KW  - reporting
KW  - Reward function
KW  - Automation
KW  - Learning systems
KW  - Reinforcement learning
KW  - Traffic control
KW  - Building systems
KW  - Disaggregated evaluation
KW  - Documentation
KW  - Dynamic approaches
KW  - Ethical considerations
KW  - Learning paradigms
KW  - Machine-learning
KW  - Reinforcement learnings
KW  - Reporting
KW  - Reward function
KW  - Feedback
PB  - Association for Computing Machinery, Inc
SN  - 979-840070231-0 (ISBN)
LA  - English
J2  - AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Conference name: 2023 AAAI / ACM Conference on Artificial Intelligence, Ethics, and Society, AIES 2023; Conference date: 8 August 2023 through 10 August 2023; Conference code: 192626
ER  -

TY  - CONF
AU  - Rasmus, M.
AU  - Kopertowski, Z.
AU  - Kozdrowski, S.
TI  - AI Application in Next Generation Programmable Networks
PY  - 2022
T2  - 2022 30th International Conference on Software, Telecommunications and Computer Networks, SoftCOM 2022
DO  - 10.23919/SoftCOM55329.2022.9911332
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141618532&doi=10.23919%2fSoftCOM55329.2022.9911332&partnerID=40&md5=fa564630d835291da09edec783c7c3ac
AD  - Orange Labs, Polska Orange Polska, Warsaw, Poland
AD  - Warsaw University of Technology, Institute of Computer Science, Warsaw, Poland
AB  - Artificial intelligence (AI) algorithms can provide an effective solution for dynamic and automated network resource management in Software Defined Networking (SDN). In this contribution, we propose an auto-configuration enabler inside the next-generation Internet of Things (IoT) architecture proposed in the ASSIST-IoT project, for network resource allocation. The AI algorithm is responsible for controlling intent-based routing in an SDN network. This paper focuses on the problem of optimal intent switching between two designated paths using a Deep-Q-Learning approach based on an artificial neural network. The AI algorithm was trained to maximise the total throughput in the network and to use the network efficiently. The presented results confirm the validity of the applied AI approach to the problem of improving network performance in next-generation networks for economically and technically efficient implementation in the evaluation of IoT network systems.  © 2022 University of Split, FESB.
KW  - artificial intelligence
KW  - deep-Q-learning
KW  - internet of things
KW  - programmable networks
KW  - software defined networking
KW  - Application programs
KW  - Deep neural networks
KW  - Learning algorithms
KW  - Next generation networks
KW  - Reinforcement learning
KW  - Resource allocation
KW  - Software defined networking
KW  - Artificial intelligence algorithms
KW  - Auto-configuration
KW  - Deep-Q-learning
KW  - Effective solution
KW  - Internet of things architectures
KW  - Network resource management
KW  - Next generation Internet
KW  - Programmable network
KW  - Q-learning
KW  - Software-defined networkings
KW  - Internet of things
A2  - Begusic D.
A2  - Rozic N.
A2  - Radic J.
A2  - Saric M.
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-953290117-7 (ISBN)
LA  - English
J2  - Int. Conf. Softw., Telecommun. Comput. Networks, SoftCOM
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2; Conference name: 30th International Conference on Software, Telecommunications and Computer Networks, SoftCOM 2022; Conference date: 22 September 2022 through 24 September 2022; Conference code: 183585
ER  -

TY  - CONF
AU  - Ning, L.
AU  - Li, Y.
AU  - Zhou, M.
AU  - Song, H.
AU  - Dong, H.
TI  - A Deep Reinforcement Learning Approach to High-speed Train Timetable Rescheduling under Disturbances
PY  - 2019
T2  - 2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019
C7  - 8917180
SP  - 3469
EP  - 3474
DO  - 10.1109/ITSC.2019.8917180
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076811874&doi=10.1109%2fITSC.2019.8917180&partnerID=40&md5=d6b57fe0c03f5430fbf061ae624bec79
AD  - Beijing Jiaotong University, State Key Laboratory of Rail Traffic Control and Safety, Beijing, 100044, China
AD  - Beijing Jiaotong University, School of the Computer and Information Technology, Beijing, 100044, China
AB  - Train timetable rescheduling (TTR) aims to address the recovery of train operation order in reordering and retiming strategies during disturbances. Considering this problem, this paper introduces a deep reinforcement learning (DRL) approach to minimize the average total delay for all trains along the railway line. Specifically, the detailed train operation in block sections and stations is illustrated to establish a learning environment involving its state sets, action sets, and the reward function. The learning agent is responsible for adjusting running times, dwell times and departure sequences for trains and conflicts are resolved simultaneously. Numerical experiments are performed on an adapted timetable carried out on the Beijing-Shanghai high-speed railway line. The experimental results indicate that the proposed approach reduces the average total delay by 46.38% in real time, compared to the First-Come-First-Served (FCFS) method. © 2019 IEEE.
KW  - Conflict resolution
KW  - Deep reinforcement learning
KW  - Train timetable rescheduling
KW  - Computer aided instruction
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Machine learning
KW  - Railroad cars
KW  - Railroad transportation
KW  - Railroads
KW  - Reinforcement learning
KW  - Scheduling
KW  - Conflict Resolution
KW  - First come first served
KW  - High speed railway lines
KW  - High speed train (HST)
KW  - Learning environments
KW  - Numerical experiments
KW  - Reinforcement learning approach
KW  - Train timetables
KW  - Deep learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-153867024-8 (ISBN)
LA  - English
J2  - IEEE Intell. Transp. Syst. Conf., ITSC
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 60; Conference name: 2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019; Conference date: 27 October 2019 through 30 October 2019; Conference code: 155652
ER  -

TY  - JOUR
AU  - Whittlestone, J.
AU  - Arulkumaran, K.
AU  - Crosby, M.
TI  - The societal implications of deep reinforcement learning
PY  - 2021
T2  - Journal of Artificial Intelligence Research
VL  - 70
SP  - 1003
EP  - 1030
DO  - 10.1613/JAIR.1.12360
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103670175&doi=10.1613%2fJAIR.1.12360&partnerID=40&md5=01488f0f66b61ebc42b41ac3d77bb044
AD  - Leverhulme Centre for the Future of Intelligence, University of Cambridge, United Kingdom
AD  - Imperial College London, United Kingdom
AD  - Leverhulme Centre for the Future of Intelligence, Imperial College London, United Kingdom
AB  - Deep Reinforcement Learning (DRL) is an avenue of research in Artificial Intelligence (AI) that has received increasing attention within the research community in recent years, and is beginning to show potential for real-world application. DRL is one of the most promising routes towards developing more autonomous AI systems that interact with and take actions in complex real-world environments, and can more flexibly solve a range of problems for which we may not be able to precisely specify a correct 'answer'. This could have substantial implications for people's lives: for example by speeding up automation in various sectors, changing the nature and potential harms of online influence, or introducing new safety risks in physical infrastructure. In this paper, we review recent progress in DRL, discuss how this may introduce novel and pressing issues for society, ethics, and governance, and highlight important avenues for future research to better understand DRL's societal implications. © 2021 AI Access Foundation. All rights reserved.
KW  - Behavioral research
KW  - Reinforcement learning
KW  - AI systems
KW  - Potential harm
KW  - Real world environments
KW  - Real-world
KW  - Recent progress
KW  - Research communities
KW  - Safety risks
KW  - Societal implications
KW  - Deep learning
PB  - AI Access Foundation
SN  - 10769757 (ISSN)
LA  - English
J2  - J Artif Intell Res
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 19; CODEN: JAIRF
ER  -

TY  - CONF
AU  - Agrawal, M.
AU  - Peterson, J.C.
AU  - Griffiths, T.L.
TI  - Using Machine Learning to Guide Cognitive Modeling: A Case Study in Moral Reasoning
PY  - 2019
T2  - Proceedings of the 41st Annual Meeting of the Cognitive Science Society: Creativity + Cognition + Computation, CogSci 2019
SP  - 1318
EP  - 1323
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099467528&partnerID=40&md5=427f3b9875858c6ec4376d653917f6e9
AD  - Department of Psychology, Princeton University, United States
AD  - Department of Computer Science, Princeton University, United States
AD  - Department of Psychology and Computer Science, Princeton University, United States
AB  - Large-scale behavioral datasets enable researchers to use complex machine learning algorithms to better predict human behavior, yet this increased predictive power does not always lead to a better understanding of the behavior in question. In this paper, we outline a data-driven, iterative procedure that allows cognitive scientists to use machine learning to generate models that are both interpretable and accurate. We demonstrate this method in the domain of moral decision-making, where standard experimental approaches often identify relevant principles that influence human judgments, but fail to generalize these findings to “real world” situations that place these principles in conflict. The recently released Moral Machine dataset allows us to build a powerful model that can predict the outcomes of these conflicts while remaining simple enough to explain the basis behind human decisions. © Cognitive Science Society: Creativity + Cognition + Computation, CogSci 2019.All rights reserved.
KW  - machine learning
KW  - moral psychology
KW  - Behavioral research
KW  - Iterative methods
KW  - Large dataset
KW  - Learning algorithms
KW  - Reinforcement learning
KW  - Case-studies
KW  - Cognitive model
KW  - Complex machines
KW  - Human behaviors
KW  - Large-scales
KW  - Machine learning algorithms
KW  - Machine-learning
KW  - Moral psychology
KW  - Moral reasoning
KW  - Predictive power
KW  - Decision making
PB  - The Cognitive Science Society
SN  - 0991196775 (ISBN); 978-099119677-7 (ISBN)
LA  - English
J2  - Proc. Annu. Meet. Cogn. Sci. Soc.: Creativity Cogn. Comput., CogSci
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 3; Conference name: 41st Annual Meeting of the Cognitive Science Society: Creativity + Cognition + Computation, CogSci 2019; Conference date: 24 July 2019 through 27 July 2019; Conference code: 182811
ER  -

TY  - CONF
AU  - Kowalczuk, Z.
AU  - Cybulski, J.
AU  - Czubenko, M.
TI  - JamesBot - An intelligent agent playing StarCraft II
PY  - 2019
T2  - 2019 24th International Conference on Methods and Models in Automation and Robotics, MMAR 2019
C7  - 8864611
SP  - 105
EP  - 110
DO  - 10.1109/MMAR.2019.8864611
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074235345&doi=10.1109%2fMMAR.2019.8864611&partnerID=40&md5=7474dd8f3a279514947e5817725de1c7
AD  - Faculty of Electronics Telecommunications and Informatics, Gdansk Univesity of Technology, Gdansk, Poland
AB  - The most popular method for optimizing a certain strategy based on a reward is Reinforcement Learning (RL). Lately, a big challenge for this technique are computer games such as StarCraft II which is a real-time strategy game, created by Blizzard. The main idea of this game is to fight between agents and control objects on the battlefield in order to defeat the enemy. This work concerns creating an autonomous bot using reinforced learning, in particular, the Q-Learning algorithm for playing StarCraft. JamesBot consists of three parts. State Manager processes relevant information from the environment. Decision Manager consists of a table implementation of the Q-Learning algorithm, which assigns actions to states, and the epsilon-greedy strategy, which determines the behavior of the bot. In turn, Action Manager is responsible for executing commands. Testing bots involves fighting the default (simple) agent built into the game. Although JamesBot played better than the default (random) agent, it failed to gain the ability to defeat the opponent. The obtained results, however, are quite promising in terms of the possibilities of further development. © 2019 IEEE.
KW  - Machine learning
KW  - Q-Learning
KW  - Reinforcement learning
KW  - StarCraft II
KW  - Computer games
KW  - Decision tables
KW  - Interactive computer graphics
KW  - Learning systems
KW  - Machine learning
KW  - Managers
KW  - Reinforcement learning
KW  - Robotics
KW  - Storms
KW  - Control objects
KW  - Greedy strategies
KW  - Q-learning
KW  - Q-learning algorithms
KW  - Real-time strategy games
KW  - Reinforced learning
KW  - StarCraft II
KW  - Learning algorithms
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-172810933-6 (ISBN)
LA  - English
J2  - Int. Conf. Methods Model. Autom. Robot., MMAR
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Conference name: 24th International Conference on Methods and Models in Automation and Robotics, MMAR 2019; Conference date: 26 August 2019 through 29 August 2019; Conference code: 152762
ER  -

TY  - JOUR
AU  - Lauffenburger, J.C.
AU  - Yom-Tov, E.
AU  - Keller, P.A.
AU  - McDonnell, M.E.
AU  - Bessette, L.G.
AU  - Fontanet, C.P.
AU  - Sears, E.S.
AU  - Kim, E.
AU  - Hanken, K.
AU  - Joseph Buckley, J.
AU  - Barlev, R.A.
AU  - Haff, N.
AU  - Choudhry, N.K.
TI  - REinforcement learning to improve non-adherence for diabetes treatments by Optimising Response and Customising Engagement (REINFORCE): Study protocol of a pragmatic randomised trial
PY  - 2021
T2  - BMJ Open
VL  - 11
IS  - 12
C7  - e052091
DO  - 10.1136/bmjopen-2021-052091
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121008254&doi=10.1136%2fbmjopen-2021-052091&partnerID=40&md5=1127131d0d2d8e167fbfd272c3bb7ff9
AD  - Center for Healthcare Delivery Sciences, Department of Medicine, Brigham and Women's Hospital and Harvard Medical School, Boston, MA, United States
AD  - Division of Pharmacoepidemiology and Pharmacoeconomics, Department of Medicine, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, United States
AD  - Microsoft Research, Microsoft Herzeliya, Israel
AD  - Tuck School of Business, Dartmouth College, Hanover, NH, United States
AD  - Endocrinology, Diabetes and Hypertension, Department of Medicine, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, United States
AD  - Division of Sleep Medicine, Department of Medicine, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, United States
AB  - Introduction Achieving optimal diabetes control requires several daily self-management behaviours, especially adherence to medication. Evidence supports the use of text messages to support adherence, but there remains much opportunity to improve their effectiveness. One key limitation is that message content has been generic. By contrast, reinforcement learning is a machine learning method that can be used to identify individuals' patterns of responsiveness by observing their response to cues and then optimising them accordingly. Despite its demonstrated benefits outside of healthcare, its application to tailoring communication for patients has received limited attention. The objective of this trial is to test the impact of a reinforcement learning-based text messaging programme on adherence to medication for patients with type 2 diabetes. Methods and analysis In the REinforcement learning to Improve Non-adherence For diabetes treatments by Optimising Response and Customising Engagement (REINFORCE) trial, we are randomising 60 patients with suboptimal diabetes control treated with oral diabetes medications to receive a reinforcement learning intervention or control. Subjects in both arms will receive electronic pill bottles to use, and those in the intervention arm will receive up to daily text messages. The messages will be individually adapted using a reinforcement learning prediction algorithm based on daily adherence measurements from the pill bottles. The trial's primary outcome is average adherence to medication over the 6-month follow-up period. Secondary outcomes include diabetes control, measured by glycated haemoglobin A1c, and self-reported adherence. In sum, the REINFORCE trial will evaluate the effect of personalising the framing of text messages for patients to support medication adherence and provide insight into how this could be adapted at scale to improve other self-management interventions. Ethics and dissemination This study was approved by the Mass General Brigham Institutional Review Board (IRB) (USA). Findings will be disseminated through peer-reviewed journals, clinicaltrials.gov reporting and conferences. Trial registration number Clinicaltrials.gov (NCT04473326).  © 
KW  - clinical trials
KW  - diabetes & endocrinology
KW  - public health
KW  - Diabetes Mellitus, Type 2
KW  - Glycated Hemoglobin A
KW  - Humans
KW  - Medication Adherence
KW  - Randomized Controlled Trials as Topic
KW  - Self-Management
KW  - Text Messaging
KW  - insulin
KW  - oral antidiabetic agent
KW  - glycosylated hemoglobin
KW  - adult
KW  - Article
KW  - clinical trial protocol
KW  - controlled study
KW  - diabetes control
KW  - electronic health record
KW  - feedback system
KW  - female
KW  - follow up
KW  - health behavior
KW  - human
KW  - insulin treatment
KW  - major clinical study
KW  - male
KW  - medication compliance
KW  - non insulin dependent diabetes mellitus
KW  - patient compliance
KW  - pragmatic trial
KW  - randomized controlled trial
KW  - reinforcement learning (machine learning)
KW  - self care
KW  - self monitoring
KW  - self report
KW  - social reinforcement
KW  - text messaging
KW  - medication compliance
KW  - randomized controlled trial (topic)
KW  - self care
KW  - text messaging
PB  - BMJ Publishing Group
SN  - 20446055 (ISSN)
C2  - 34862289
LA  - English
J2  - BMJ Open
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5; Correspondence Address: J.C. Lauffenburger; Center for Healthcare Delivery Sciences, Department of Medicine, Brigham and Women's Hospital and Harvard Medical School, Boston, United States; email: jlauffenburger@bwh.harvard.edu; J.C. Lauffenburger; Center for Healthcare Delivery Sciences, Department of Medicine, Brigham and Women's Hospital and Harvard Medical School, Boston, United States; email: jlauffenburger@bwh.harvard.edu
ER  -

TY  - CONF
AU  - Marot, A.
AU  - Donnot, B.
AU  - Dulac-Arnold, G.
AU  - Kelly, A.
AU  - O'Sullivan, A.
AU  - Viebahn, J.
AU  - Awad, M.
AU  - Guyon, I.
AU  - Panciatici, P.
AU  - Romero, C.
TI  - Learning to run a Power Network Challenge: a Retrospective Analysis
PY  - 2020
T2  - Proceedings of Machine Learning Research
VL  - 133
SP  - 112
EP  - 132
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162682629&partnerID=40&md5=c00f496feb5e42bb53daf27f521eb189
AD  - AI Lab, Reseau de Transport d'Electricite (RTE), France
AD  - Google Research, Brain Team, Google, France
AD  - Electric Power Research Institute (EPRI), Ireland
AD  - UCL Energy Institute, University College of London, United Kingdom
AD  - Digital and Process Excellence, TenneT
AD  - Electrical & Computer Engineering Department, American University of Beirut, Lebanon
AD  - Chalearn, University Paris-Saclay, France
AD  - R&D Department, Reseau de Transport d'Electricite (RTE), France
AD  - CELEC EP Transelectric, Ecuador
AB  - Power networks, responsible for transporting electricity across large geographical regions, are complex infrastructures on which modern life critically depend. Variations in demand and production profiles, with increasing renewable energy integration, as well as the high voltage network technology, constitute a real challenge for human operators when optimizing electricity transportation while avoiding blackouts. Motivated to investigate the potential of Artificial Intelligence methods in enabling adaptability in power network operation, we have designed a L2RPN challenge to encourage the development of reinforcement learning solutions to key problems present in the next-generation power networks. The NeurIPS 2020 competition was well received by the international community attracting over 300 participants worldwide. The main contribution of this challenge is our proposed comprehensive'Grid2Op' framework, and associated benchmark, which plays realistic sequential network operations scenarios. The Grid2Op framework, which is open-source and easily re-usable, allows users to define new environments with its companion GridAlive ecosystem. Grid2Op relies on existing non-linear physical power network simulators and let users create a series of perturbations and challenges that are representative of two important problems: a) the uncertainty resulting from the increased use of unpredictable renewable energy sources, and b) the robustness required with contingent line disconnections. In this paper, we give the highlights of the NeurIPS 2020 competition. We present the benchmark suite and analyse the winning solutions, including one super-human performance demonstration. We propose our organizational insights for a successful competition and conclude on open research avenues. Given the challenge success, we expect our work will foster research to create more sustainable solutions for power network operations. © 2021 A. Marot et al.
KW  - Competition
KW  - Decision-making
KW  - Power Networks
KW  - Reinforcement Learning
KW  - Benchmarking
KW  - Electric network analysis
KW  - Geographical regions
KW  - Learning systems
KW  - Reinforcement learning
KW  - Renewable energy resources
KW  - Complex infrastructures
KW  - Decisions makings
KW  - High voltage networks
KW  - Network operations
KW  - Network technologies
KW  - Power networks
KW  - Production profiles
KW  - Reinforcement learnings
KW  - Renewable energy integrations
KW  - Retrospective analysis
KW  - Decision making
A2  - Escalante H.J.
A2  - Hofmann K.
PB  - ML Research Press
SN  - 26403498 (ISSN)
LA  - English
J2  - Proc. Mach. Learn. Res.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 16; Conference name: 34th Demonstration and Competition Track at the 34th Annual Conference on Neural Information Processing Systems, NeurIPS 2020; Conference date: 6 December 2020 through 12 December 2020; Conference code: 189195
ER  -

TY  - JOUR
AU  - Lopez-Gazpio, I.
TI  - Gaining Student Engagement Through Project-Based Learning: A Competitive 2D Game Construction Case Study
PY  - 2022
T2  - IEEE Access
VL  - 10
SP  - 1881
EP  - 1892
DO  - 10.1109/ACCESS.2021.3139764
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122584370&doi=10.1109%2fACCESS.2021.3139764&partnerID=40&md5=648063871a13e9494cd04af1c676e09a
AD  - Department of Informatic Electronic and Communication Technologies, University of Deusto, Donostia-San Sebastian, 20012, Spain
AB  - In this work we consider an open artificial intelligence game as a matter of study within the lectures of artificial intelligence to combat lack of motivation and increase engagement within the classroom. During formation, students in computer science can deal with moderately complex projects, nevertheless, dealing with such problems is relegated to the Degree Final Project. In this investigation we show the procedural steps of how project-based learning combined with game construction can effectively be used to promote engagement in informatic lectures at university. For the task, we build a 2D game engine and propose students to enroll in factitious research teams with the aim of programming intelligent agents that play the game employing artificial intelligence techniques. The intended principal outcome is to show evidence of the application of project-based learning in artificial intelligence within the lectures, and how it can be combined with game construction to increase motivation in the classroom. Project-based learning has the students learn, organize, and solve challenges while students themselves remain their own responsible for the investigation and process of work. We propose to follow a series of sequential phases that conform a set of milestones that incorporate a project-based learning approach to the lectures. Through this work we show that the use of project-based learning combined with game construction provides reliable evidence that a much deeper understanding about artificial intelligence is attained by students participating in the challenge. Student evaluation questionnaires and final grade results attained by students indicate that students remained more engaged during the semester in comparison to previous semesters in which lack of motivation was reported.  © 2013 IEEE.
KW  - Action research
KW  - active learning computing education
KW  - cross-disciplinary skills
KW  - engagement
KW  - game construction
KW  - project-based learning (PBL)
KW  - reinforcement learning
KW  - Computer games
KW  - Intelligent agents
KW  - Job analysis
KW  - Motivation
KW  - Reinforcement learning
KW  - Surveys
KW  - Action research
KW  - Active Learning
KW  - Active learning computing education
KW  - Computing education
KW  - Cross-disciplinary
KW  - Cross-disciplinary skill
KW  - Engagement
KW  - Face
KW  - Game
KW  - Game construction
KW  - Learning (artificial intelligence)
KW  - Project based learning
KW  - Project-based learning
KW  - Task analysis
KW  - Students
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 21693536 (ISSN)
LA  - English
J2  - IEEE Access
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 4; Correspondence Address: I. Lopez-Gazpio; Department of Informatic Electronic and Communication Technologies, University of Deusto, Donostia-San Sebastian, 20012, Spain; email: inigo.lopezgazpio@deusto.es
ER  -

TY  - CONF
AU  - Xenou, K.
AU  - Chalkiadakis, G.
AU  - Afantenos, S.
TI  - Deep Reinforcement Learning in Strategic Board Game Environments
PY  - 2019
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 11450 LNAI
SP  - 233
EP  - 248
DO  - 10.1007/978-3-030-14174-5_16
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063425583&doi=10.1007%2f978-3-030-14174-5_16&partnerID=40&md5=7ecea14aa2fafccec9707df78134fb26
AD  - School of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece
AD  - Institut de recherche en informatique de Toulouse (IRIT), Université Paul Sabatier, Toulouse, France
AB  - In this paper we propose a novel Deep Reinforcement Learning (DRL) algorithm that uses the concept of “action-dependent state features”, and exploits it to approximate the Q-values locally, employing a deep neural network with parallel Long Short Term Memory (LSTM) components, each one responsible for computing an action-related Q-value. As such, all computations occur simultaneously, and there is no need to employ “target” networks and experience replay, which are techniques regularly used in the DRL literature. Moreover, our algorithm does not require previous training experiences, but trains itself online during game play. We tested our approach in the Settlers Of Catan multi-player strategic board game. Our results confirm the effectiveness of our approach, since it outperforms several competitors, including the state-of-the-art jSettler heuristic algorithm devised for this particular domain. © 2019, Springer Nature Switzerland AG.
KW  - Deep Reinforcement Learning
KW  - Strategic board games
KW  - Deep neural networks
KW  - Heuristic algorithms
KW  - Long short-term memory
KW  - Machine learning
KW  - Reinforcement learning
KW  - Action-dependent state
KW  - Board games
KW  - Game plays
KW  - Q-values
KW  - State of the art
KW  - Training experiences
KW  - Multi agent systems
A2  - Slavkovik M.
PB  - Springer Verlag
SN  - 03029743 (ISSN); 978-303014173-8 (ISBN)
LA  - English
J2  - Lect. Notes Comput. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 11; Correspondence Address: G. Chalkiadakis; School of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece; email: gehalk@intelligence.tuc.gr; Conference name: 16th European Conference on Multi-Agent Systems, EUMAS 2018; Conference date: 6 December 2018 through 7 December 2018; Conference code: 223799
ER  -

TY  - CONF
AU  - Ho, J.
AU  - Wang, C.-M.
AU  - Chuang, C.-H.
AU  - King, C.-T.
AU  - Feng, C.-W.
AU  - Chou, T.-H.
AU  - Chen, Y.-M.
AU  - Yang, Y.-H.
AU  - Hsiao, Y.-C.
TI  - Bootstrapping Human-Autonomy Collaborations by using Brain-Computer Interface of SSVEP for Multi-Agent Deep Reinforcement Learning
PY  - 2022
T2  - Proceedings of the 2022 IEEE International Conference on Human-Machine Systems, ICHMS 2022
DO  - 10.1109/ICHMS56717.2022.9980765
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146305588&doi=10.1109%2fICHMS56717.2022.9980765&partnerID=40&md5=af2a24dadbbe3adb4208679e795a8bcb
AD  - Institute of Information Science, Academia Sinica, Taipei, 115, Taiwan
AD  - Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu, 30013, Taiwan
AD  - National Tsing Hua University, College of Education, Hsinchu, 30013, Taiwan
AD  - National Tsing Hua University, Department of Computer Science, Hsinchu, 30013, Taiwan
AD  - National Taiwan University, College of Eecs, Taipei, 106, Taiwan
AB  - Human-Autonomy Teaming (HAT) has become one of the emerging AI trends due to the advances in sophisticated machine design that allows closer cooperation with humans while performing moral, reasonable, and applicable tasks as humans' most exemplary assistants. Based on HAT's pursuing the collective goal and sharing the authority between humans and machines, our research aims at answering whether humans' brain-computer interface (BCI) helps achieve efficient collaborations of human with Reinforcement Learning (RL) agents. How can it efficiently facilitate human-in-the-loop guidance to bootstrap the training of the agents? This study proposes a BCI-based system that interacts with RL agents as a human-in-the-loop teaming integration. The neural responses elicited by the Steady-State Visual Evoked Potential in BCI facilitate the collaboration of learning agents with humans and accomplish this goal in a game simulation environment. The results of our proposed system, NeuroRL, show significant improvement by reducing the non-stationarity of exploitations and explorations in the RL agents. With BCI-assisted human-in-the-loop, the rewards can be optimized during the early investigations to achieve more efficient convergence in the training. The novel design proposed in this study can extend the development of the emerging HAT field and knowledge-based RL systems for various applications in dynamic environments.  © 2022 IEEE.
KW  - brain-computer interface
KW  - human-AI system
KW  - human-in-the-loop
KW  - reinforcement learning
KW  - Autonomous agents
KW  - Brain computer interface
KW  - Deep learning
KW  - Intelligent agents
KW  - Knowledge based systems
KW  - Learning systems
KW  - Machine design
KW  - Multi agent systems
KW  - AI systems
KW  - Collective goals
KW  - Cooperation with human
KW  - Human brain
KW  - Human-AI system
KW  - Human-in-the-loop
KW  - Multi agent
KW  - Reinforcement learning agent
KW  - Reinforcement learnings
KW  - Sophisticated machines
KW  - Reinforcement learning
A2  - Kaber D.
A2  - Guerrieri A.
A2  - Fortino G.
A2  - Nurnberger A.
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-166545238-0 (ISBN)
LA  - English
J2  - Proc. IEEE Int. Conf. Hum.-Mach. Syst., ICHMS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: C.-M. Wang; Institute of Information Science, Academia Sinica, Taipei, 115, Taiwan; email: cmwang@iis.sinica.edu.tw; C.-H. Chuang; Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu, 30013, Taiwan; email: ch.chuang@mx.nthu.edu.tw; C.-T. King; Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu, 30013, Taiwan; email: king@cs.nthu.edu.tw; Conference name: 3rd IEEE International Conference on Human-Machine Systems, ICHMS 2022; Conference date: 17 November 2022 through 19 November 2022; Conference code: 185382
ER  -

TY  - JOUR
AU  - Fragkos, G.
AU  - Minwalla, C.
AU  - Tsiropoulou, E.E.
AU  - Plusquellic, J.
TI  - Enhancing Privacy in PUF-Cash through Multiple Trusted Third Parties and Reinforcement Learning
PY  - 2022
T2  - ACM Journal on Emerging Technologies in Computing Systems
VL  - 18
IS  - 1
C7  - 7
DO  - 10.1145/3441139
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123945842&doi=10.1145%2f3441139&partnerID=40&md5=937ecbebfffba2c884e243ead5a710e9
AD  - University of New Mexico, P.O. Box 1212, Albuquerque, 43017-6221, NM, United States
AD  - Bank of Canada, 234 Wellington St., Ottawa, K1A0G9, Canada
AB  - Electronic cash (e-Cash) is a digital alternative to physical currency such as coins and bank notes. Suitably constructed, e-Cash has the ability to offer an anonymous offline experience much akin to cash, and in direct contrast to traditional forms of payment such as credit and debit cards. Implementing security and privacy within e-Cash, i.e., preserving user anonymity while preventing counterfeiting, fraud, and double spending, is a non-trivial challenge. In this article, we propose major improvements to an e-Cash protocol, termed PUF-Cash, based on physical unclonable functions (PUFs). PUF-Cash was created as an offline-first, secure e-Cash scheme that preserved user anonymity in payments. In addition, PUF-Cash supports remote payments; an improvement over traditional currency. In this work, a novel multi-trusted-third-party exchange scheme is introduced, which is responsible for "blinding"Alice's e-Cash tokens; a feature at the heart of preserving her anonymity. The exchange operations are governed by machine learning techniques which are uniquely applied to optimize user privacy, while remaining resistant to identity-revealing attacks by adversaries and trusted authorities. Federation of the single trusted third party into multiple entities distributes the workload, thereby improving performance and resiliency within the e-Cash system architecture. Experimental results indicate that improvements to PUF-Cash enhance user privacy and scalability.  © 2021 Association for Computing Machinery.
KW  - Digital currency
KW  - Electronic money
KW  - Internet of things
KW  - Networks
KW  - Security
KW  - Blockchain
KW  - Cryptography
KW  - E-learning
KW  - Electronic money
KW  - Learning systems
KW  - Reinforcement learning
KW  - Credit cards
KW  - Debit cards
KW  - Electronic-cash
KW  - Network
KW  - Offline
KW  - Reinforcement learnings
KW  - Security
KW  - Trusted third parties
KW  - User anonymity
KW  - User privacy
KW  - Internet of things
PB  - Association for Computing Machinery
SN  - 15504832 (ISSN)
LA  - English
J2  - ACM J. Emerg. Technologies Comput. Syst.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5
ER  -

TY  - CONF
AU  - Lover, J.
AU  - Gjaerum, V.B.
AU  - Lekkas, A.M.
TI  - Explainable AI methods on a deep reinforcement learning agent for automatic docking
PY  - 2021
T2  - IFAC-PapersOnLine
VL  - 54
IS  - 16
SP  - 146
EP  - 152
DO  - 10.1016/j.ifacol.2021.10.086
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120875018&doi=10.1016%2fj.ifacol.2021.10.086&partnerID=40&md5=b92ee0582ecdd8ca70e4e676226c717d
AD  - Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, NO-7491, Norway
AB  - Artifical neural networks (ANNs) have made their way into marine robotics in the last years, where they are used in control and perception systems, to name a few examples. At the same time, the black-box nature of ANNs is responsible for key challenges related to interpretability and trustworthiness, which need to be addressed if ANNs are to be deployed safely in real-life operations. In this paper, we implement three XAI methods to provide explanations to the decisions made by a deep reinforcement learning agent: Kernel SHAP, LIME and Linear Model Trees (LMTs). The agent was trained via Proximal Policy Optimization (PPO) to perform automatic docking on a fully-actuated vessel. We discuss the properties and suitability of the three methods, and juxtapose them with important attributes of the docking agent to provide context to the explanations. © 2021 The Authors.
KW  - Autonomous ships
KW  - Deep reinforcement learning
KW  - Docking
KW  - Explainable artificial intelligence
KW  - Marine control systems
KW  - Deep neural networks
KW  - Reinforcement learning
KW  - Artifical neural networks
KW  - Automatic docking
KW  - Black boxes
KW  - Docking
KW  - Explainable artificial intelligence
KW  - In-control
KW  - Marine control systems
KW  - Marine robotics
KW  - Perception systems
KW  - Reinforcement learning agent
KW  - Lime
PB  - Elsevier B.V.
SN  - 24058963 (ISSN)
LA  - English
J2  - IFAC-PapersOnLine
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 9; Correspondence Address: J. Lover; Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, NO-7491, Norway; email: loverjakob@gmail.com; V.B. Gjaerum; Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, NO-7491, Norway; email: vilde.gjarum@ntnu.no; A.M. Lekkas; Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, NO-7491, Norway; email: anastasios.lekkas@ntnu.no; Conference name: 13th IFAC Conference on Control Applications in Marine Systems, Robotics, and Vehicles CAMS 2021; Conference date: 22 September 2021 through 24 September 2021; Conference code: 146758
ER  -

TY  - CONF
AU  - Rodriguez-Soto, M.
AU  - Lopez-Sanchez, M.
AU  - Rodriguez-Aguilar, J.A.
TI  - Guaranteeing the Learning of Ethical Behaviour through Multi-Objective Reinforcement Learning
PY  - 2021
T2  - ALA 2021 - Adaptive and Learning Agents Workshop at AAMAS 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164790058&partnerID=40&md5=8c6e41a552a28873676f41c65dd8937a
AD  - Artificial Intelligence, Research Institute (IIIA-CSIC), Bellaterra, Spain
AD  - Universitat de Barcelona (UB), Barcelona, Spain
AB  - AI research is being challenged with ensuring that autonomous agents behave ethically, namely in alignment with moral values. A common approach, founded on the exploitation of Reinforcement Learning techniques, is to design environments that incentivise agents to learn an ethical behaviour. However, to the best of our knowledge, current approaches do not offer theoretical guarantees that an agent will learn an ethical behaviour. Here, we advance along this direction by proposing a novel way of designing environments wherein it is formally guaranteed that an agent learns to behave ethically while pursuing its individual objective. Our theoretical results develop within the formal framework of Multi- Objective Reinforcement Learning to ease the handling of an agent's individual and ethical objectives. As a further contribution, we leverage on our theoretical results to introduce an algorithm that automates the design of ethical environments. © 2021 Association for Computing Machinery.
KW  - Moral Decision Making
KW  - Multi-Objective Reinforcement Learning
KW  - Value Alignment
KW  - Autonomous agents
KW  - Ethical technology
KW  - Learning systems
KW  - Reinforcement learning
KW  - Decisions makings
KW  - Design environment
KW  - Ethical behavior
KW  - Learn+
KW  - Moral decision making
KW  - Multi objective
KW  - Multi-objective reinforcement learning
KW  - Reinforcement learning techniques
KW  - Reinforcement learnings
KW  - Value alignment
KW  - Decision making
PB  - AAMAS
LA  - English
J2  - ALA - Adapt. Learn. Agents Workshop AAMAS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2; Conference name: Adaptive and Learning Agents Workshop, ALA 2021 at AAMAS 2021; Conference date: 3 May 2021 through 4 May 2021; Conference code: 192414
ER  -

TY  - CONF
AU  - Hasanzadeh Mofrad, M.
AU  - Melhem, R.
AU  - Hammoud, M.
TI  - Revolver: Vertex-Centric Graph Partitioning Using Reinforcement Learning
PY  - 2018
T2  - IEEE International Conference on Cloud Computing, CLOUD
VL  - 2018-July
C7  - 8457880
SP  - 818
EP  - 821
DO  - 10.1109/CLOUD.2018.00111
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057475359&doi=10.1109%2fCLOUD.2018.00111&partnerID=40&md5=b1fe2147708c520cfcb030214914ad70
AD  - Department of Computer Science, University of Pittsburgh, Pittsburgh, United States
AD  - Carnegie Mellon University in Qatar, Doha, Qatar
AB  - Big graph analytics is gaining a widespread momentum across different fields, including biology, computer vision, social networks, recommendation systems and transportation logistics, to mention just a few. Distributed systems for graph analytics are utilized as a mean to process big graphs. To distribute and balance computation and communication loads within a distributed graph analytics system, graph partitioning algorithms can be leveraged. In this paper, we propose Revolver, a machine learning-based graph partitioning algorithm. In particular, Revolver uses reinforcement learning and label propagation to efficiently and effectively carry out the task of graph partitioning. It employs a vertex-centric approach where each vertex in a graph is associated with an autonomous agent responsible for assigning a suitable partition to the vertex. In addition, it uses label propagation to evaluate the decency of partitioning. Evaluation results show that Revolver can produce highly balanced and localized partitions compared to three popular and state-of-the-art graph partitioning algorithms. © 2018 IEEE.
KW  - Graph partitioning
KW  - Label propagation
KW  - Learning automata
KW  - Reinforcement learning
KW  - Autonomous agents
KW  - Cloud computing
KW  - Reinforcement learning
KW  - Communication load
KW  - Distributed systems
KW  - Evaluation results
KW  - Graph Partitioning
KW  - Graph partitioning algorithms
KW  - Label propagation
KW  - Learning Automata
KW  - Transportation-logistics
KW  - Graph theory
PB  - IEEE Computer Society
SN  - 21596182 (ISSN); 978-153867235-8 (ISBN)
LA  - English
J2  - IEEE Int. Conf. Cloud Comput., CLOUD
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 9; Conference name: 11th IEEE International Conference on Cloud Computing, CLOUD 2018; Conference date: 2 July 2018 through 7 July 2018; Conference code: 139614
ER  -

TY  - JOUR
AU  - Zhou, M.
AU  - Yu, Y.
AU  - Qu, X.
TI  - Development of an Efficient Driving Strategy for Connected and Automated Vehicles at Signalized Intersections: A Reinforcement Learning Approach
PY  - 2020
T2  - IEEE Transactions on Intelligent Transportation Systems
VL  - 21
IS  - 1
C7  - 8848852
SP  - 433
EP  - 443
DO  - 10.1109/TITS.2019.2942014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076984591&doi=10.1109%2fTITS.2019.2942014&partnerID=40&md5=b39c9af66db92b7bea47bbf8a4024192
AD  - Tencent Holdings Limited, Shenzhen, 518057, China
AD  - School of Civil and Environmental Engineering, University of Technology Sydney, Sydney, 2007, Australia
AD  - Department of Architecture and Civil Engineering, Chalmers University of Technology, Gothenburg, 41296, Sweden
AB  - The concept of Connected and Automated Vehicles (CAVs) enables instant traffic information to be shared among vehicle networks. With this newly proposed concept, a vehicle's driving behaviour will no longer be solely based on the driver's limited and incomplete observation. By taking advantages of the shared information, driving behaviours of CAVs can be improved greatly to a more responsible, accurate and efficient level. This study proposed a reinforcement-learning-based car following model for CAVs in order to obtain an appropriate driving behaviour to improve travel efficiency, fuel consumption and safety at signalized intersections in real-time. The result shows that by specifying an effective reward function, a controller can be learned and works well under different traffic demands as well as traffic light cycles with different durations. This study reveals a great potential of emerging reinforcement learning technologies in transport research and applications. © 2000-2011 IEEE.
KW  - car-following
KW  - deep deterministic policy gradient
KW  - intersection
KW  - machine learning
KW  - Neural network
KW  - reinforcement learning
KW  - traffic light
KW  - traffic oscillation
KW  - Deep neural networks
KW  - Highway traffic control
KW  - Intersections
KW  - Learning algorithms
KW  - Learning systems
KW  - Machine learning
KW  - Neural networks
KW  - Street traffic control
KW  - Traffic signs
KW  - Vehicles
KW  - Car following
KW  - Car following models
KW  - Incomplete observation
KW  - Policy gradient
KW  - Reinforcement learning approach
KW  - Signalized intersection
KW  - Traffic information
KW  - Traffic light
KW  - Reinforcement learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 15249050 (ISSN)
LA  - English
J2  - IEEE Trans. Intell. Transp. Syst.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 182; Correspondence Address: X. Qu; Department of Architecture and Civil Engineering, Chalmers University of Technology, Gothenburg, 41296, Sweden; email: drxiaoboqu@gmail.com
ER  -

TY  - CONF
AU  - Kedzierski, B.
AU  - Willetts, I.
TI  - Accelerating Time to Competency in an Industry 5.0 World
PY  - 2021
T2  - AIChE Annual Meeting, Conference Proceedings
VL  - 2021-November
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136231501&partnerID=40&md5=6a3a7ce11356681dc26eb27a299858c9
AD  - HumanWRKS LLC, Houston, TX, United States
AD  - AVEVA Software LLC, Carlsbad, CA, United States
AB  - Industry 4.0 is about smart production, IoT, sensors and drones. Industry 5.0 will be about connecting smart production to human cognition, collaboration and creativity. To this end, progress will be judged on not just the connection of data between the Internet of Things (IoT), but how that connectivity enhances the human experience and shortens the time to achieve environmental safety competencies. This paper discusses how companies such as Shell are deploying more personalized worker experiences to serve the new industrial agenda of achieving the goals of both, the enterprise as well as continually ensure the welfare of humans. Specifically, the paper discusses how: Experiential simulation-based learning strategies such as Operator Training Simulators have become more mobile, on-demand and collaborative in order to drive more personalized worker experiences AI infused microlearning tailored to industrial operations staff is used to build upon an individual's learning profile, identify strengths and weaknesses and provide timely reinforcement learning content Virtual Reality platforms can be used to address behavioral needs, where frontline staff can be trained and evaluated on decision-making and real-time choices when faced with unexpected or infrequent situations in the simulated plant. Using real-life scenarios, these solutions can help instill responsible behavior and boost safety performance across both assets and teams Microsoft Power Apps now allow citizen development of solutions by those closest to the opportunity. The age of mass personalization within an Industry 5.0 world is here and will undoubtedly seeks to achieve large-scale impact, one individual at a time. © 2021 American Institute of Chemical Engineers. All rights reserved.
KW  - Decision making
KW  - Industry 4.0
KW  - Personnel training
KW  - Reinforcement learning
KW  - Simulation platform
KW  - Virtual addresses
KW  - Virtual reality
KW  - Environmental safety
KW  - Human cognition
KW  - Human creativity
KW  - Industrial operations
KW  - Learning strategy
KW  - Micro-learning
KW  - On demands
KW  - Operator training simulator
KW  - Simulation-based learning
KW  - Workers'
KW  - Internet of things
PB  - American Institute of Chemical Engineers
SN  - 978-171385283-4 (ISBN)
LA  - English
J2  - AIChE Annu. Meet. Conf. Proc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 2021 AIChE Annual Meeting; Conference date: 15 November 2021 through 19 November 2021; Conference code: 181244
ER  -

TY  - CONF
AU  - Cortés, E.C.
AU  - Ghosh, D.
TI  - An invitation to system-wide algorithmic fairness
PY  - 2020
T2  - AIES 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society
SP  - 236
EP  - 241
DO  - 10.1145/3375627.3375860
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082165523&doi=10.1145%2f3375627.3375860&partnerID=40&md5=0ac9ababcebf1e25183df9a114efc72e
AD  - Pennsylvania State University, State College, PA, United States
AD  - University of Colorado, Anschutz Medical Campus, Aurora, CO, United States
AB  - We propose a framework for analyzing and evaluating system-wide algorithmic fairness. The core idea is to use simulation techniques in order to extend the scope of current fairness assessments by incorporating context and feedback to a phenomenon of interest. By doing so, we expect to better understand the interaction among the social behavior giving rise to discrimination, automated decision making tools, and fairness-inspired statistical constraints. In particular, we invite the community to use agent based models as an explanatory tool for causal mechanisms of population level properties. We also propose embedding these into a reinforcement learning algorithm to find optimal actions for meaningful change. As an incentive for taking a system-wide approach , we show through a simple model of predictive policing and trials that if we limit our attention to one portion of the system, we may determine some blatantly unfair practices as fair, and be blind to overall unfairness. © 2020 Copyright held by the owner/author(s).
KW  - Agent based modeling
KW  - Ethical ai
KW  - Fairness
KW  - Recidivism
KW  - Computational methods
KW  - Decision making
KW  - Ethical aspects
KW  - Reinforcement learning
KW  - Simulation platform
KW  - Agent-based model
KW  - Automated decision making
KW  - Evaluating systems
KW  - Fairness
KW  - Recidivism
KW  - Simulation technique
KW  - Statistical constraints
KW  - System wide approaches
KW  - Autonomous agents
PB  - Association for Computing Machinery, Inc
SN  - 978-145037110-0 (ISBN)
LA  - English
J2  - AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 6; Conference name: 3rd AAAI/ACM Conference on AI, Ethics, and Society, AIES 2020, co-located with AAAI 2020; Conference date: 7 February 2020 through 8 February 2020; Conference code: 158220
ER  -

TY  - JOUR
AU  - Henderson, P.
AU  - Hu, J.
AU  - Romoff, J.
AU  - Brunskill, E.
AU  - Jurafsky, D.
AU  - Pineau, J.
TI  - Towards the systematic reporting of the energy and carbon footprints of machine learning
PY  - 2020
T2  - Journal of Machine Learning Research
VL  - 21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098464023&partnerID=40&md5=80a4388648f46937227186c95f2f22f8
AD  - Stanford University, Stanford, CA, United States
AD  - Facebook, Menlo Park, CA, United States
AD  - Mila, McGill University, Montreal, QC, Canada
AD  - Facebook AI Research, Mila, McGill University, Montreal, QC, Canada
AB  - Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms. © 2020 Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, Joelle Pineau. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v21/20-312.html.
KW  - Climate change
KW  - Deep learning
KW  - Energy efficiency
KW  - Green computing
KW  - Reinforcement learning
KW  - Carbon footprint
KW  - Energy efficiency
KW  - Energy utilization
KW  - Environmental impact
KW  - Learning systems
KW  - Reinforcement learning
KW  - Carbon emissions
KW  - Case-studies
KW  - Climate impacts
KW  - Energy efficient
KW  - Energy efficient algorithms
KW  - Machine learning research
KW  - Real time
KW  - Learning algorithms
PB  - Microtome Publishing
SN  - 15324435 (ISSN)
LA  - English
J2  - J. Mach. Learn. Res.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 165
ER  -

TY  - CONF
AU  - Wu, Q.
AU  - Wang, H.
AU  - Wang, H.
TI  - Learning by Exploration: New Challenges in Real-World Environments
PY  - 2020
T2  - Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
SP  - 3575
EP  - 3576
DO  - 10.1145/3394486.3406484
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090405998&doi=10.1145%2f3394486.3406484&partnerID=40&md5=fd40440c65fda5070def4ba553e3d779
AD  - University of Virginia, Charlottesville, VA, United States
AB  - Learning is a predominant theme for any intelligent system, humans, or machines. Moving beyond the classical paradigm of learning from past experience, e.g., offline supervised learning from given labels, a learner needs to actively collect exploratory feedback to learn from the unknowns, i.e., learning through exploration. This tutorial will introduce the learning by exploration paradigm, which is the key ingredient in many interactive online learning problems, including the multi-armed bandit and, more generally, reinforcement learning problems. In this tutorial, we will first motivate the need for exploration in machine learning algorithms and highlight its importance in many real-world problems where online sequential decision making is involved. In real-world application scenarios, considerable challenges arise in such a learning problem, including sample complexity, costly and even outdated feedback, and ethical considerations of exploration (such as fairness and privacy). We will introduce several classical exploration strategies and then highlight the aforementioned three fundamental challenges in the learning from exploration paradigm and introduce the recent research development on addressing them, respectively. © 2020 Owner/Author.
KW  - learning by exploration
KW  - multi-armed bandit
KW  - reinforcement learning
KW  - Data mining
KW  - Decision making
KW  - Intelligent systems
KW  - Learning systems
KW  - Reinforcement learning
KW  - Application scenario
KW  - Ethical considerations
KW  - Exploration strategies
KW  - Multi armed bandit
KW  - Real world environments
KW  - Real-world problem
KW  - Recent researches
KW  - Sequential decision making
KW  - Learning algorithms
PB  - Association for Computing Machinery
SN  - 978-145037998-4 (ISBN)
LA  - English
J2  - Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2; Conference name: 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2020; Conference date: 23 August 2020 through 27 August 2020; Conference code: 162480
ER  -

TY  - JOUR
AU  - Samma, H.
AU  - Lahasan, B.
TI  - Optimized Two-Stage Ensemble Model for Mammography Mass Recognition
PY  - 2020
T2  - IRBM
VL  - 41
IS  - 4
SP  - 195
EP  - 204
DO  - 10.1016/j.irbm.2020.01.005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078796799&doi=10.1016%2fj.irbm.2020.01.005&partnerID=40&md5=a02015c448e8a006c4546547af8d42a6
AD  - Intelligent Biometric Group, School of Electrical and Electronic Engineering, Universiti Sains Malaysia, Engineering Campus, Nibong Tebal, Penang, 14300, Malaysia
AD  - Department of Computer Programming, Faculty of Education–Shabwa, University of Aden, Shabwa, Yemen
AB  - Objectives: Mammography mass recognition is considered as a very challenge pattern recognition problem due to the high similarity between normal and abnormal masses. Therefore, the main objective of this study is to develop an efficient and optimized two-stage recognition model to tackle this recognition task. Material and methods: Basically, the developed recognition model combines an ensemble of linear Support Vector Machine (SVM) classifiers with a Reinforcement Learning-based Memetic Particle Swarm Optimizer (RLMPSO) as RLMPSO-SVM recognition model. RLMPSO is used to construct a two-stage of an ensemble of linear SVM classifiers by performing simultaneous SVM parameters tuning, features selection, and training instances selection. The first stage of RLMPSO-SVM recognition model is responsible about recognizing the input ROI mammography masses as normal or abnormal mass pattern. Meanwhile, the second stage of RLMPSO-SVM model used to perform further recognition for abnormal ROIs as malignant or benign masses. In order to evaluate the effectiveness of RLMPSO-SVM, a total of 1187 normal ROIs, 111 malignant ROIs, and 135 benign ROIs were randomly selected from DDSM database images. Results: Reported results indicated that RLMPSO-SVM model was able to achieve performances of 97.57% sensitivity rate with 97.86% specificity rate for normal vs. abnormal recognition cases. For malignant vs. benign recognition performance it was reported of 97.81% sensitivity rate with 96.92% specificity rate. Conclusion: Reported results indicated that RLMPSO-SVM recognition model is an effective tool that could assist the radiologist during the diagnosis of the presented abnormalities in mammography images. The outcomes indicated that RLMPSO-SVM significantly outperformed various SVM-based models as well as other variants of computational intelligence models including multi-layer perceptron, naive Bayes classifier, and k-nearest neighbor. © 2020 AGBM
KW  - Ensemble model
KW  - Mammography mass recognition
KW  - Particle swarm optimizer
KW  - Support vector machine
KW  - article
KW  - Bayesian learning
KW  - body weight
KW  - cancer model
KW  - cancer size
KW  - cancer staging
KW  - comparative effectiveness
KW  - controlled study
KW  - human
KW  - intelligence
KW  - k nearest neighbor
KW  - linear support vector machine
KW  - mammography
KW  - molecular recognition
KW  - multilayer perceptron
KW  - particle swarm optimization
KW  - radiologist
KW  - randomized controlled trial
KW  - reinforcement learning (machine learning)
KW  - sensitivity and specificity
KW  - support vector machine
PB  - Elsevier Masson SAS
SN  - 19590318 (ISSN)
LA  - English
J2  - IRBM
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5; Correspondence Address: H. Samma; Intelligent Biometric Group, School of Electrical and Electronic Engineering, Universiti Sains Malaysia, Penang, Engineering Campus, Nibong Tebal, 14300, Malaysia; email: hussein.samma@usm.my
ER  -

TY  - JOUR
AU  - Gönül, S.
AU  - Namlı, T.
AU  - Coşar, A.
AU  - Toroslu, İ.H.
TI  - A reinforcement learning based algorithm for personalization of digital, just-in-time, adaptive interventions
PY  - 2021
T2  - Artificial Intelligence in Medicine
VL  - 115
C7  - 102062
DO  - 10.1016/j.artmed.2021.102062
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103689016&doi=10.1016%2fj.artmed.2021.102062&partnerID=40&md5=d930d475b60d5505d730a041b742080d
AD  - SRDC Corp., Silikon Blok Kat: 1 No: 16 SRDC Teknokent ODTÜ, Ankara, Turkey
AD  - Department of Computer Engineering, Middle East Technical University, Orta Doğu Teknik Üniversitesi Universiteler Mah. Dumlupinar Blv. No:1 06800, Ankara, Turkey
AB  - Suboptimal health related behaviors and habits; and resulting chronic diseases are responsible for majority of deaths globally. Studies show that providing personalized support to patients yield improved results by preventing and/or timely treatment of these problems. Digital, just-in-time and adaptive interventions are mobile phone-based notifications that are being utilized to support people wherever and whenever necessary in coping with their health problems. In this research, we propose a reinforcement learning-based mechanism to personalize interventions in terms of timing, frequency and preferred type(s). We simultaneously employ two reinforcement learning models, namely intervention-selection and opportune-moment-identification; capturing and exploiting changes in people's long-term and momentary contexts respectively. While the intervention-selection model adapts the intervention delivery with respect to type and frequency, the opportune-moment-identification model tries to find the most opportune moments to deliver interventions throughout a day. We propose two accelerator techniques over the standard reinforcement learning algorithms to boost learning performance. First, we propose a customized version of eligibility traces for rewarding past actions throughout an agent's trajectory. Second, we utilize the transfer learning method to reuse knowledge across multiple learning environments. We validate the proposed approach in a simulated experiment where we simulate four personas differing in their daily activities, preferences on specific intervention types and attitudes towards the targeted behavior. Our experiments show that the proposed approach yields better results compared to the standard reinforcement learning algorithms and successfully capture the simulated variations associated with the personas. © 2021 Elsevier B.V.
KW  - Intervention delivery optimization
KW  - Just-in-time adaptive interventions
KW  - Personalized health interventions
KW  - Personalized interventions
KW  - Reinforcement learning
KW  - Algorithms
KW  - Chronic Disease
KW  - Humans
KW  - Reinforcement, Psychology
KW  - Computer aided instruction
KW  - E-learning
KW  - Just in time production
KW  - Learning systems
KW  - Patient treatment
KW  - Reinforcement learning
KW  - Transfer learning
KW  - Eligibility traces
KW  - Identification model
KW  - Learning environments
KW  - Learning performance
KW  - Reinforcement learning models
KW  - Simulated experiments
KW  - Targeted behavior
KW  - Transfer learning methods
KW  - Article
KW  - behavior change
KW  - benchmarking
KW  - chronic disease
KW  - clinical indicator
KW  - controlled study
KW  - decision making
KW  - exercise
KW  - health behavior
KW  - health care delivery
KW  - human
KW  - intensive care
KW  - k means clustering
KW  - knowledge
KW  - learning environment
KW  - Markov decision process
KW  - mathematical model
KW  - personalized medicine
KW  - priority journal
KW  - random forest
KW  - reinforcement learning (machine learning)
KW  - reward
KW  - self care
KW  - Social Cognitive Theory
KW  - supervised machine learning
KW  - Theory of Planned Behavior
KW  - transfer of learning
KW  - algorithm
KW  - Learning algorithms
PB  - Elsevier B.V.
SN  - 09333657 (ISSN)
C2  - 34001322
LA  - English
J2  - Artif. Intell. Med.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 8; Correspondence Address: S. Gönül; SRDC Corp., Silikon Blok Kat: 1 No: 16 SRDC Teknokent ODTÜ, Ankara, Turkey; email: suat@srdc.com.tr; CODEN: AIMEE
ER  -

TY  - CONF
AU  - van Hasselt, H.
AU  - Madjiheurem, S.
AU  - Hessel, M.
AU  - Silver, D.
AU  - Barreto, A.
AU  - Borsa, D.
TI  - Expected Eligibility Traces
PY  - 2021
T2  - 35th AAAI Conference on Artificial Intelligence, AAAI 2021
VL  - 11B
SP  - 9997
EP  - 10005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109665191&partnerID=40&md5=1c540b8968b2739087cf37bfb10a68dc
AD  - DeepMind, United Kingdom
AD  - University College London, United Kingdom
AB  - The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state. In this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD(λ). Finally, we discuss possible extensions and connections to related ideas, such as successor features. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
KW  - Combinatorial optimization
KW  - 'current
KW  - Counterfactuals
KW  - Credit assignment
KW  - Credit assignment problems
KW  - Eligibility traces
KW  - Generalisation
KW  - Research questions
KW  - Temporal difference learning
KW  - Reinforcement learning
PB  - Association for the Advancement of Artificial Intelligence
SN  - 978-171383597-4 (ISBN)
LA  - English
J2  - AAAI Conf. Artif. Intell., AAAI
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 10; Conference name: 35th AAAI Conference on Artificial Intelligence, AAAI 2021; Conference date: 2 February 2021 through 9 February 2021; Conference code: 176953
ER  -

TY  - CONF
AU  - Turner, A.M.
AU  - Hadfield-Menell, D.
AU  - Tadepalli, P.
TI  - Conservative agency via attainable utility preservation
PY  - 2020
T2  - AIES 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society
SP  - 385
EP  - 391
DO  - 10.1145/3375627.3375851
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082170934&doi=10.1145%2f3375627.3375851&partnerID=40&md5=0db458dc7117c657d822ab64b4ea511a
AD  - Oregon State University, Corvallis, OR, United States
AD  - Uc Berkeley, Berkeley, CA, United States
AB  - Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior. © 2020 Copyright held by the owner/author(s).
KW  - AI alignment
KW  - Reinforcement learning
KW  - Reward specification
KW  - Side effects
KW  - Ethical aspects
KW  - Effective behaviors
KW  - Expensive equipments
KW  - Misspecification
KW  - Reward function
KW  - Side effect
KW  - Reinforcement learning
PB  - Association for Computing Machinery, Inc
SN  - 978-145037110-0 (ISBN)
LA  - English
J2  - AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 19; Conference name: 3rd AAAI/ACM Conference on AI, Ethics, and Society, AIES 2020, co-located with AAAI 2020; Conference date: 7 February 2020 through 8 February 2020; Conference code: 158220
ER  -

TY  - CONF
AU  - Wolf-Brenner, C.
TI  - MAKE US SMILE! AI AND THE VIOLATION OF HUMAN INTENTIONS
PY  - 2021
T2  - 34th Bled eConference: Digital Support from Crisis to Progressive Change, BLED 2021 - Proceedings
SP  - 67
EP  - 73
DO  - 10.18690/978-961-286-485-9.5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173566330&doi=10.18690%2f978-961-286-485-9.5&partnerID=40&md5=746d5f66fdb2ce7762ba68a61f21d6fe
AD  - Know-Center GmbH, Graz, Austria
AB  - In his book Superintelligence, Nick Bostrom points to several ways the development of Artificial Intelligence (AI) might fail, turn out to be malignant or even induce an existential catastrophe. He describes ‘Perverse Instantiations’ (PI) as cases, in which AI figures out how to satisfy some goal through unintended ways. For instance, AI could attempt to paralyze human facial muscles into constant smiles to achieve the goal of making humans smile. According to Bostrom, cases like this ought to be avoided since they include a violation of human designer’s intentions. However, AI finding solutions that its designers have not yet thought of and therefore could also not have intended is arguably one of the main reasons why we are so eager to use it on a variety of problems. In this paper, I aim to show that the concept of PI is quite vague, mostly due to ambiguities surrounding the term ‘intention’. Ultimately, this text aims to serve as a starting point for a further discussion of the research topic, the development of a research agenda and future improvement of the terminology. © 2021 34th Bled eConference: Digital Support from Crisis to Progressive Change, BLED 2021 - Proceedings. All rights reserved.
KW  - artificial intelligence
KW  - control problem
KW  - digital ethics
KW  - perverse instantiations
KW  - reinforcement learning
KW  - E-learning
KW  - Philosophical aspects
KW  - Control problems
KW  - Digital ethics
KW  - Facial muscles
KW  - Finding solutions
KW  - Human intentions
KW  - Perverse instantiation
KW  - Reinforcement learnings
KW  - Research agenda
KW  - Research topics
KW  - Superintelligence
KW  - Reinforcement learning
A2  - Andreja Pucihar A.
A2  - Borstnar M.K.
A2  - Bons R.
A2  - Cripps H.
A2  - Sheombar A.
A2  - Vidmar D.
PB  - University of Maribor Press
SN  - 978-961286485-9 (ISBN)
LA  - English
J2  - Bled eConference: Digit. Support Crisis Progress. Change, BLED - Proc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: C. Wolf-Brenner; Know-Center GmbH, Graz, Austria; email: christof.brenner@gmx.at; Conference name: 34th Bled eConference, BLED 2021; Conference date: 27 June 2021 through 30 June 2021; Conference code: 192380
ER  -

TY  - CONF
AU  - Miyashita, Y.
AU  - Sugawara, T.
TI  - Coordination structures generated by deep reinforcement learning in distributed task executions
PY  - 2019
T2  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
VL  - 4
SP  - 2129
EP  - 2131
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077026226&partnerID=40&md5=bca8bddf569c772aac4d709660db17ea
AD  - Computer Science and Communications Engineering, Waseda University, Tokyo, Japan
AB  - We investigate the coordination structures generated by deep Q-network (DQN) in a distributed task execution. Cooperation and coordination are the crucial issues in multi-agent systems, and very sophisticated design or learning is required in order to achieve effective structures or regimes of coordination. In this paper, we show the results that agents establish the division of labor in a bottom-up manner by determining their implicit responsible area when input structure for DQN is constituted by their own observation and absolute location. © 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.
KW  - Cooperation
KW  - Coordination
KW  - Divisional cooperation
KW  - Multi-agent deep reinforcement learning
KW  - Autonomous agents
KW  - Machine learning
KW  - Multi agent systems
KW  - Reinforcement learning
KW  - Cooperation
KW  - Cooperation and coordination
KW  - Coordination
KW  - Coordination structures
KW  - Distributed tasks
KW  - Division of labor
KW  - Divisional cooperation
KW  - Multi agent
KW  - Deep learning
PB  - International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)
SN  - 15488403 (ISSN); 978-151089200-2 (ISBN)
LA  - English
J2  - Proc. Int. Joint Conf. Auton. Agents Multiagent Syst., AAMAS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 18th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2019; Conference date: 13 May 2019 through 17 May 2019; Conference code: 155776
ER  -

TY  - JOUR
AU  - Haas, J.
TI  - Moral Gridworlds: A Theoretical Proposal for Modeling Artificial Moral Cognition
PY  - 2020
T2  - Minds and Machines
VL  - 30
IS  - 2
SP  - 219
EP  - 246
DO  - 10.1007/s11023-020-09524-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084153579&doi=10.1007%2fs11023-020-09524-9&partnerID=40&md5=3aa27a3d14d7f2a9bd1a5be6c260455b
AD  - Department of Philosophy, Rhodes College, Memphis, 38112, United States
AB  - I describe a suite of reinforcement learning environments in which artificial agents learn to value and respond to moral content and contexts. I illustrate the core principles of the framework by characterizing one such environment, or “gridworld,” in which an agent learns to trade-off between monetary profit and fair dealing, as applied in a standard behavioral economic paradigm. I then highlight the core technical and philosophical advantages of the learning approach for modeling moral cognition, and for addressing the so-called value alignment problem in AI. © 2020, Springer Nature B.V.
KW  - Artificial intelligence
KW  - Fairness
KW  - Machine ethics
KW  - Moral AI
KW  - Moral cognition
KW  - Moral psychology
KW  - Reinforcement learning
KW  - Computer aided instruction
KW  - Economic and social effects
KW  - Philosophical aspects
KW  - Alignment Problems
KW  - Artificial agents
KW  - Behavioral economics
KW  - Fair dealing
KW  - Learning approach
KW  - Trade off
KW  - Reinforcement learning
PB  - Springer
SN  - 09246495 (ISSN)
LA  - English
J2  - Minds Mach
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 6; Correspondence Address: J. Haas; Department of Philosophy, Rhodes College, Memphis, 38112, United States; email: juliashaas@gmail.com; CODEN: MMACE
ER  -

TY  - JOUR
AU  - Loftus, T.J.
AU  - Filiberto, A.C.
AU  - Li, Y.
AU  - Balch, J.
AU  - Cook, A.C.
AU  - Tighe, P.J.
AU  - Efron, P.A.
AU  - Upchurch, G.R.
AU  - Rashidi, P.
AU  - Li, X.
AU  - Bihorac, A.
TI  - Decision analysis and reinforcement learning in surgical decision-making
PY  - 2020
T2  - Surgery (United States)
VL  - 168
IS  - 2
SP  - 253
EP  - 266
DO  - 10.1016/j.surg.2020.04.049
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086519362&doi=10.1016%2fj.surg.2020.04.049&partnerID=40&md5=7d29d4bd81e8d80ec2c63acaf8f19207
AD  - Department of Surgery, University of Florida Health, Gainesville, FL, United States
AD  - NSF Center for Big Learning, University of Florida, Gainesville, FL, United States
AD  - Department of Medicine, University of California, San Francisco, CA, United States
AD  - Departments of Anesthesiology, Orthopedics, and Information Systems/Operations Management, University of Florida Health, Gainesville, FL, United States
AD  - Departments of Biomedical Engineering, Computer and Information Science and Engineering, and Electrical and Computer Engineering, University of Florida, Gainesville, FL, United States
AD  - Precision and Intelligence in Medicine, Department of Medicine, University of Florida Health, Gainesville, FL, United States
AB  - Background: Surgical patients incur preventable harm from cognitive and judgment errors made under time constraints and uncertainty regarding patients’ diagnoses and predicted response to treatment. Decision analysis and techniques of reinforcement learning theoretically can mitigate these challenges but are poorly understood and rarely used clinically. This review seeks to promote an understanding of decision analysis and reinforcement learning by describing their use in the context of surgical decision-making. Methods: Cochrane, EMBASE, and PubMed databases were searched from their inception to June 2019. Included were 41 articles about cognitive and diagnostic errors, decision-making, decision analysis, and machine-learning. The articles were assimilated into relevant categories according to Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews guidelines. Results: Requirements for time-consuming manual data entry and crude representations of individual patients and clinical context compromise many traditional decision-support tools. Decision analysis methods for calculating probability thresholds can inform population-based recommendations that jointly consider risks, benefits, costs, and patient values but lack precision for individual patient-centered decisions. Reinforcement learning, a machine-learning method that mimics human learning, can use a large set of patient-specific input data to identify actions yielding the greatest probability of achieving a goal. This methodology follows a sequence of events with uncertain conditions, offering potential advantages for personalized, patient-centered decision-making. Clinical application would require secure integration of multiple data sources and attention to ethical considerations regarding liability for errors and individual patient preferences. Conclusion: Traditional decision-support tools are ill-equipped to accommodate time constraints and uncertainty regarding diagnoses and the predicted response to treatment, both of which often impair surgical decision-making. Decision analysis and reinforcement learning have the potential to play complementary roles in delivering high-value surgical care through sound judgment and optimal decision-making. © 2020 Elsevier Inc.
KW  - Attitude to Health
KW  - Clinical Decision-Making
KW  - Decision Making, Shared
KW  - Decision Support Techniques
KW  - Decision Trees
KW  - Electronic Health Records
KW  - Humans
KW  - Machine Learning
KW  - Numbers Needed To Treat
KW  - Patient Preference
KW  - Patient-Centered Care
KW  - Surgical Procedures, Operative
KW  - adult
KW  - article
KW  - attention
KW  - decision support system
KW  - diagnostic error
KW  - Embase
KW  - female
KW  - human
KW  - male
KW  - Medline
KW  - patient preference
KW  - Preferred Reporting Items for Systematic Reviews and Meta-Analyses
KW  - probability
KW  - reinforcement learning (machine learning)
KW  - sound
KW  - systematic review
KW  - treatment response
KW  - uncertainty
KW  - attitude to health
KW  - clinical decision making
KW  - decision tree
KW  - electronic health record
KW  - machine learning
KW  - numbers needed to treat
KW  - patient care
KW  - surgery
PB  - Mosby Inc.
SN  - 00396060 (ISSN)
C2  - 32540036
LA  - English
J2  - Surgery
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 20; Correspondence Address: A. Bihorac; Department of Medicine, University of Florida Health, Division of Nephrology, Hypertension, and Renal Transplantation, Gainesville, PO Box 100224, 32610-0224, United States; email: abihorac@ufl.edu; CODEN: SURGA
ER  -

TY  - JOUR
AU  - Agarwal, A.
AU  - Edelman, S.
TI  - Functionally Effective Conscious AI Without Suffering
PY  - 2020
T2  - Journal of Artificial Intelligence and Consciousness
VL  - 7
IS  - 1
SP  - 39
EP  - 50
DO  - 10.1142/S2705078520300030
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118229657&doi=10.1142%2fS2705078520300030&partnerID=40&md5=5ccce03b76e9823942ea43c682bb177d
AD  - Department of Computer Science, Cornell University, United States
AD  - Department of Psychology, Cornell University, United States
AB  - Insofar as consciousness has a functional role in facilitating learning and behavioral control, the builders of autonomous Artificial Intelligence (AI) systems are likely to attempt to incorporate it into their designs. The extensive literature on the ethics of AI is concerned with ensuring that AI systems, and especially autonomous conscious ones, behave ethically. In contrast, our focus here is on the rarely discussed complementary aspect of engineering conscious AI: how to avoid condemning such systems, for whose creation we would be solely responsible, to unavoidable suffering brought about by phenomenal self-consciousness. We outline two complementary approaches to this problem, one motivated by a philosophical analysis of the phenomenal self, and the other by certain computational concepts in reinforcement learning. © 2020 World Scientific Publishing Company.
KW  - AI Ethics
KW  - Conscious AI
KW  - Phenomenal Self
KW  - Reinforcement Learning
KW  - Suffering
PB  - World Scientific
SN  - 27050785 (ISSN)
LA  - English
J2  - Journal of Artificial Intelligence and Consciousness
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 7; Correspondence Address: S. Edelman; Department of Psychology, Cornell University, United States; email: se37@cornell.edu
ER  -

TY  - JOUR
AU  - Xu, X.
AU  - Xu, Y.
AU  - Wang, M.-H.
AU  - Li, J.
AU  - Xu, Z.
AU  - Chai, S.
AU  - He, Y.
TI  - Data-Driven Game-Based Pricing for Sharing Rooftop Photovoltaic Generation and Energy Storage in the Residential Building Cluster under Uncertainties
PY  - 2021
T2  - IEEE Transactions on Industrial Informatics
VL  - 17
IS  - 7
C7  - 9166747
SP  - 4480
EP  - 4491
DO  - 10.1109/TII.2020.3016336
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097380049&doi=10.1109%2fTII.2020.3016336&partnerID=40&md5=7d8d97e5d7b72fef878a97355ce3a48f
AD  - Shenzhen Research Institute, Department of Electrical Engineering, The Hong Kong Polytechnic University, Hong Kong, 999077, Hong Kong
AD  - School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798, Singapore
AD  - College of Electrical and Information Engineering, Hunan University, Changsha, 410082, China
AB  - In this article, a novel machine learning based data-driven pricing method is proposed for sharing rooftop photovoltaic (PV) generation and energy storage in an electrically interconnected residential building cluster (RBC). In the studied problem, the energy sharing process is modeled by the leader-follower Stackelberg game where the owner of the rooftop PV system is responsible for pricing self-generated PV energy and operating ES devices. Meanwhile, local electricity consumers in the RBC choose their energy consumption with the given internal electricity prices. To track the stochastic rooftop PV panel outputs, the long short-term memory network based rolling-horizon prediction function is developed to dynamically predict future trends of PV generation. With system information, the predicted information is fed into a Q-learning based decision-making process to find near-optimal pricing strategies. The simulation results verify the effectiveness of the proposed approach in solving energy sharing problems with partial or uncertain information.  © 2005-2012 IEEE.
KW  - Energy sharing
KW  - energy storage (ES)
KW  - long short-term memory (LSTM) network
KW  - photovoltaic (PV) generation
KW  - pricing method
KW  - Q-learning algorithm
KW  - residential building cluster (RBC)
KW  - Stackelberg game
KW  - Decision making
KW  - Digital storage
KW  - Electric energy storage
KW  - Energy utilization
KW  - Housing
KW  - Learning systems
KW  - Photovoltaic cells
KW  - Reinforcement learning
KW  - Stochastic systems
KW  - Decision making process
KW  - Electricity consumers
KW  - Energy sharing process
KW  - Near-optimal pricing
KW  - Photovoltaic generation
KW  - Prediction function
KW  - Residential building
KW  - Uncertain informations
KW  - Costs
PB  - IEEE Computer Society
SN  - 15513203 (ISSN)
LA  - English
J2  - IEEE Trans. Ind. Inf.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 30; Correspondence Address: Z. Xu; Shenzhen Research Institute, Department of Electrical Engineering, The Hong Kong Polytechnic University, Hong Kong, 999077, Hong Kong; email: eezhaoxu@polyu.edu.hk
ER  -

TY  - JOUR
AU  - Costa, T.
AU  - Laan, A.
AU  - Heras, F.J.H.
AU  - de Polavieja, G.G.
TI  - Automated Discovery of Local Rules for Desired Collective-Level Behavior Through Reinforcement Learning
PY  - 2020
T2  - Frontiers in Physics
VL  - 8
C7  - 200
DO  - 10.3389/fphy.2020.00200
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087681592&doi=10.3389%2ffphy.2020.00200&partnerID=40&md5=516a5725623027cf0d91abfbe808fce8
AD  - Collective Behavior Laboratory, Champalimaud Research, Lisbon, Portugal
AB  - Complex global behavior patterns can emerge from very simple local interactions between many agents. However, no local interaction rules have been identified that generate some patterns observed in nature, for example the rotating balls, rotating tornadoes and the full-core rotating mills observed in fish collectives. Here we show that locally interacting agents modeled with a minimal cognitive system can produce these collective patterns. We obtained this result by using recent advances in reinforcement learning to systematically solve the inverse modeling problem: given an observed collective behavior, we automatically find a policy generating it. Our agents are modeled as processing the information from neighbor agents to choose actions with a neural network and move in an environment of simulated physics. Even though every agent is equipped with its own neural network, all agents have the same network architecture and parameter values, ensuring in this way that a single policy is responsible for the emergence of a given pattern. We find the final policies by tuning the neural network weights until the produced collective behavior approaches the desired one. By using modular neural networks with modules using a small number of inputs and outputs, we built an interpretable model of collective motion. This enabled us to analyse the policies obtained. We found a similar general structure for the four different collective patterns, not dissimilar to the one we have previously inferred from experimental zebrafish trajectories; but we also found consistent differences between policies generating the different collective pattern, for example repulsion in the vertical direction for the more three-dimensional structures of the sphere and tornado. Our results illustrate how new advances in artificial intelligence, and specifically in reinforcement learning, allow new approaches to analysis and modeling of collective behavior. © Copyright © 2020 Costa, Laan, Heras and de Polavieja.
KW  - collective behavior
KW  - deep learning
KW  - explainable artificial intelligence
KW  - interpretable artificial intelligence
KW  - multi agent reinforcement learning
KW  - Cognitive systems
KW  - Deep learning
KW  - Inverse problems
KW  - Learning systems
KW  - Multi agent systems
KW  - Network architecture
KW  - Tornadoes
KW  - Automated discovery
KW  - Collective behaviour
KW  - Deep learning
KW  - Explainable artificial intelligence
KW  - Interpretable artificial intelligence
KW  - Local interactions
KW  - Local rules
KW  - Multi-agent reinforcement learning
KW  - Neural-networks
KW  - Reinforcement learnings
KW  - Reinforcement learning
PB  - Frontiers Media SA
SN  - 2296424X (ISSN)
LA  - English
J2  - Front. Phys.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 10; Correspondence Address: G.G. de Polavieja; Collective Behavior Laboratory, Champalimaud Research, Lisbon, Portugal; email: gonzalo.polavieja@neuro.fchampalimaud.org
ER  -

TY  - JOUR
AU  - Sit, M.
AU  - Demiray, B.Z.
AU  - Xiang, Z.
AU  - Ewing, G.J.
AU  - Sermet, Y.
AU  - Demir, I.
TI  - A comprehensive review of deep learning applications in hydrology and water resources
PY  - 2020
T2  - Water Science and Technology
VL  - 82
IS  - 12
SP  - 2635
EP  - 2670
DO  - 10.2166/wst.2020.369
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093922286&doi=10.2166%2fwst.2020.369&partnerID=40&md5=adfb90d03dab73cd722c77c96f00743f
AD  - Interdisciplinary Graduate Program in Informatics, University of Iowa, Iowa City, United States
AD  - IIHR - Hydroscience and Engineering, University of Iowa, 100 C. Maxwell Stanley Hydraulics Laboratory, Iowa City, 52242-1585, IA, United States
AD  - Department of Computer Science, University of Iowa, Iowa City, United States
AD  - Department of Civil and Environmental Engineering, University of Iowa, Iowa City, United States
AD  - Department of Electrical and Computer Engineering, University of Iowa, Iowa City, United States
AB  - The global volume of digital data is expected to reach 175 zettabytes by 2025. The volume, variety and velocity of water-related data are increasing due to large-scale sensor networks and increased attention to topics such as disaster response, water resources management, and climate change. Combined with the growing availability of computational resources and popularity of deep learning, these data are transformed into actionable and practical knowledge, revolutionizing the water industry. In this article, a systematic review of literature is conducted to identify existing research that incorporates deep learning methods in the water sector, with regard to monitoring, management, governance and communication of water resources. The study provides a comprehensive review of state-of-the-art deep learning approaches used in the water industry for generation, prediction, enhancement, and classification tasks, and serves as a guide for how to utilize available deep learning methods for future water resources challenges. Key issues and challenges in the application of these techniques in the water domain are discussed, including the ethics of these technologies for decision-making in water resources management and governance. Finally, we provide recommendations and future directions for the application of deep learning models in hydrology and water resources. © IWA Publishing 2020
KW  - Artificial intelligence
KW  - Deep learning
KW  - Hydroscience
KW  - Machine learning
KW  - Review
KW  - Water
KW  - Climate Change
KW  - Deep Learning
KW  - Hydrology
KW  - Water Resources
KW  - Climate change
KW  - Decision making
KW  - Deep learning
KW  - Hydrology
KW  - Information management
KW  - Learning systems
KW  - Sensor networks
KW  - Water supply
KW  - water
KW  - Classification tasks
KW  - Computational resources
KW  - Disaster response
KW  - Hydrology and water resource
KW  - Large scale sensor network
KW  - Learning approach
KW  - Systematic Review
KW  - Water resources management
KW  - algorithm
KW  - communication
KW  - decision making
KW  - governance approach
KW  - hydrology
KW  - literature review
KW  - water management
KW  - water resource
KW  - aquatic environment
KW  - architecture
KW  - Article
KW  - artificial intelligence
KW  - artificial neural network
KW  - autoencoder
KW  - biotechnology
KW  - classification algorithm
KW  - convolution algorithm
KW  - convolutional neural network
KW  - decision making
KW  - deep belief network
KW  - deep learning
KW  - deep neural network
KW  - deep q network
KW  - Elman network
KW  - extreme learning machine
KW  - feed forward neural network
KW  - gated recurrent unit network
KW  - generative adversarial network
KW  - hydrology
KW  - long short term memory network
KW  - machine learning
KW  - nonlinear autoregressive model
KW  - nonlinear regression analysis
KW  - prediction
KW  - Q learning
KW  - recurrent neural network
KW  - regression analysis
KW  - reinforcement learning (machine learning)
KW  - reproducibility
KW  - restricted Boltzmann machine
KW  - systematic review (topic)
KW  - unsupervised machine learning
KW  - water availability
KW  - water management
KW  - water monitoring
KW  - water quality
KW  - water supply
KW  - climate change
KW  - hydrology
KW  - Water resources
PB  - IWA Publishing
SN  - 02731223 (ISSN)
C2  - 33341760
LA  - English
J2  - Water Sci. Technol.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 216; Correspondence Address: M. Sit; Interdisciplinary Graduate Program in Informatics, University of Iowa, Iowa City, United States; email: muhammed-sit@uiowa.edu; CODEN: WSTED
ER  -

TY  - CONF
AU  - Amendola, J.
AU  - Tannuri, E.A.
AU  - Cozman, F.G.
AU  - Reali Costa, A.H.
TI  - Port channel navigation subjected to environmental conditions using reinforcement learning
PY  - 2019
T2  - Proceedings of the International Conference on Offshore Mechanics and Arctic Engineering - OMAE
VL  - 7A-2019
C7  - V07AT06A042
DO  - 10.1115/OMAE2019-96120
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075809293&doi=10.1115%2fOMAE2019-96120&partnerID=40&md5=2cae97a398c70a88f42f7507f41335ce
AD  - University of Sao Paulo, Sao Paulo, Brazil
AB  - This paper proposes a machine learning agent for automatically navigating a vessel in a confined channel subject to environmental conditions. The agent is trained and tested using a Ship Maneuvering Simulator and is responsible for commanding the rudder, so as to keep the vessel inside the channel with minimum distance from the center line, and to reach the final part of the channel with a prescribed thruster rotation level. The algorithm is based on deep reinforcement learning method and uses an efficient state-space representation. The advantage of using reinforcement learning is that it does not require any expert to directly teach the agent how to behave under particular conditions. The novelty of this work is that: it does not require previous knowledge on the vessel dynamic model and the maneuvering scenario; it is robust against fluctuations of environmental forces such as wind and current; it considers discrete actions of rudder commands emulating the pilot actions in a real maneuver. The developed method is convenient for simulations in scenarios or areas that were never navigated before, in which no previous navigation data can be used to train a conventional supervised learning agent. One direct application for this work is the integration with a realistic fast-time maneuvering simulator for new ports or operations. Both training and validation experiments focused on the unsheltered approach channel of the Suape Port, in Brazil; these experiments were run in a SMH-USP maneuvering simulator (real environmental conditions measured on-site were employed in simulations). Copyright © 2019 ASME.
KW  - Arctic engineering
KW  - Deep learning
KW  - Machine learning
KW  - Ocean engineering
KW  - Offshore oil well production
KW  - Rudders
KW  - Simulators
KW  - State space methods
KW  - Approach channels
KW  - Channel navigation
KW  - Environmental conditions
KW  - Environmental forces
KW  - Machine learning agents
KW  - Particular condition
KW  - Reinforcement learning method
KW  - State space representation
KW  - Reinforcement learning
PB  - American Society of Mechanical Engineers (ASME)
SN  - 978-079185884-4 (ISBN)
LA  - English
J2  - Proc Int Conf Offshore Mech Arct Eng - OMAE
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 3; Correspondence Address: J. Amendola; University of Sao Paulo, Sao Paulo, Brazil; email: jose.amendola@usp.br; Conference name: ASME 2019 38th International Conference on Ocean, Offshore and Arctic Engineering, OMAE 2019; Conference date: 9 June 2019 through 14 June 2019; Conference code: 154931; CODEN: PIOSE
ER  -

TY  - CONF
AU  - Agafonov, A.
AU  - Yumaganov, A.
AU  - Myasnikov, V.
TI  - Adaptive Traffic Signal Control Based on Maximum Weighted Traffic Flow
PY  - 2022
T2  - 2022 8th International Conference on Information Technology and Nanotechnology, ITNT 2022
DO  - 10.1109/ITNT55410.2022.9848651
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137777254&doi=10.1109%2fITNT55410.2022.9848651&partnerID=40&md5=0ada6169f8fc7fe34587d8c0eec54857
AD  - Samara National Research University, Samara, Russian Federation
AD  - Samara National Research University, IPSI RAS - Branch of the FSRC Crystallography and Photonics RAS, Samara, Russian Federation
AB  - The paper proposes a method for adaptive traffic signal control. The proposed method includes two stages, each of which is responsible for a separate algorithm. At the first step, the predicted 'flow' of vehicles through the intersection along the given lane is estimated at the allowed traffic light signal. At the second step, a 'weighted' flow estimate is formed, which takes into account the waiting time for vehicles at the intersection. The next phase of the traffic signal is determined by the criterion of maximizing the weighted traffic flow through the intersection. An experimental study of the proposed algorithm was conducted both on synthetic and real-world traffic scenarios, including an isolated intersection, a highway, and a city area. Based on experimental results, we can conclude that the proposed algorithm outperforms baseline classical and reinforcement learning methods for traffic signal control in terms of average waiting time and average travel time.  © 2022 IEEE.
KW  - adaptive control
KW  - intelligent transportation systems
KW  - machine learning
KW  - traffic flow
KW  - traffic signal control
KW  - Adaptive control systems
KW  - Intelligent systems
KW  - Learning systems
KW  - Reinforcement learning
KW  - Street traffic control
KW  - Traffic signals
KW  - Adaptive Control
KW  - Adaptive traffic signal control
KW  - Intelligent transportation systems
KW  - Light signal
KW  - Machine-learning
KW  - Separate algorithm
KW  - Traffic flow
KW  - Traffic light
KW  - Traffic signal control
KW  - Waiting time
KW  - Travel time
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-166548571-5 (ISBN)
LA  - English
J2  - Int. Conf. Inf. Technol. Nanotechnol., ITNT
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5; Conference name: 8th International Conference on Information Technology and Nanotechnology, ITNT 2022; Conference date: 23 May 2022 through 27 May 2022; Conference code: 182151
ER  -

TY  - CONF
AU  - Qu, C.
AU  - Boubin, J.
AU  - Gafurov, D.
AU  - Zhou, J.
AU  - Aloysius, N.
AU  - Nguyen, H.
AU  - Calyam, P.
TI  - UAV Swarms in Smart Agriculture: Experiences and Opportunities
PY  - 2022
T2  - Proceedings - 2022 IEEE 18th International Conference on e-Science, eScience 2022
SP  - 148
EP  - 158
DO  - 10.1109/eScience55777.2022.00029
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145438688&doi=10.1109%2feScience55777.2022.00029&partnerID=40&md5=20f968a8ca575c0273cd2d222e1210d1
AD  - University of Missouri, Columbia, United States
AD  - Binghamton University, United States
AB  - Smart agriculture benefits from unmanned aerial vehicles (UAV), and in-field sensors to collect data used to make responsible crop management decisions which sustainably increase yields. In addition, smart agriculture relies on machine learning algorithms, creative networking solutions, and edge and cloud computing resources to collect, transfer, and process agricultural data. UAV can carry a wide array of sensors, maneuver rapidly throughout the field, apply treatments for some crop health problems, and can be flown by software. UAV, however, have small batteries and limited carrying capacities which keep missions short. In this paper, we provide an overview of state-of-the-art UAV swarm technology for smart agriculture, and present experiences from real-world agricultural UAV swarm case studies. We describe how quick mapping of large areas such as crop fields necessitates multiple UAV missions, potentially using multiple UAV simultaneously as a swarm. We detail how swarms of UAV have added advantages over a single UAV deployment. They can coordinate to map areas in parallel, leverage multiple sensor types, target areas for close inspection, and diagnose and treat problems rapidly. UAV swarms come with additional implementation difficulties beyond single UAV. We list challenges to implementers in terms of Resource allocation, compute orchestration, multi-agent mission planning and swarm goal definition. We also describe recent advances in edge computing, machine learning, and autonomy in orchestration and resource management techniques for swarm deployments. Finally, we conclude with research opportunities that future work can address to improve swarm performance, scale, and adoption for smart agriculture.  © 2022 IEEE.
KW  - computation offloading
KW  - intelligent orchestration
KW  - multi-sensor network
KW  - smart agriculture
KW  - UAV swarms
KW  - Agricultural robots
KW  - Antennas
KW  - computation offloading
KW  - Learning algorithms
KW  - Multi agent systems
KW  - Reinforcement learning
KW  - Resource allocation
KW  - Sensor networks
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Computation offloading
KW  - Crop managements
KW  - Edge computing
KW  - Field sensors
KW  - In-field
KW  - Intelligent orchestration
KW  - Multi-sensor networks
KW  - Smart agricultures
KW  - Unmanned aerial vehicle swarm
KW  - Crops
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-166546124-5 (ISBN)
LA  - English
J2  - Proc. - IEEE Int. Conf. e-Sci., eScience
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 13; Correspondence Address: C. Qu; University of Missouri, Columbia, United States; email: cqy78@mail.missouri.edu; D. Gafurov; University of Missouri, Columbia, United States; email: dgvkh@mail.missouri.edu; Conference name: 18th IEEE International Conference on e-Science, eScience 2022; Conference date: 10 October 2022 through 14 October 2022; Conference code: 185105
ER  -

TY  - CONF
AU  - Alkoby, S.
AU  - Rath, A.
AU  - Stone, P.
TI  - Teaching social behavior through human reinforcement for ad hoc teamwork-the star framework
PY  - 2019
T2  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
VL  - 3
SP  - 1773
EP  - 1775
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074999964&partnerID=40&md5=1550d96100f579b7313ce335000c753e
AD  - University of Texas at Austin, Austin, TX, United States
AB  - As AI technology continues to develop, more and more agents will become capable of long term autonomy alongside people. Thus, a recent line of research has studied the problem of teaching autonomous agents the concept of ethics and human social norms. Most existing work considers the case of an individual agent attempting to learn a predefined set of rules. In reality however, social norms are not always pre-defined and are very difficult to represent algorithmically. Moreover, the basic idea behind the social norms concept is ensuring that one's actions do not negatively influence others' utilities, which is inherently a multiagent concept. Thus, here we investigate a way to teach agents, as a team, how to act according to human social norms. In this research, we introduce the star framework used to teach an ad hoc team of agents to act in accordance with human social norms. Using a hybrid team (agents and people), when taking an action considered to be socially unacceptable, the agents receive negative feedback from the human teammate(s) who has(have) an awareness of the team's norms. We view star as an important step towards teaching agents to act more consistently with respect to human morality. © 2019 International Foundation for Autonomous Agents and Multiagent Systems. All rights reserved.
KW  - Ad hoc
KW  - Reinforcement learning
KW  - Social norms
KW  - Behavioral research
KW  - Multi agent systems
KW  - Reinforcement learning
KW  - Stars
KW  - Ad hoc
KW  - Ad-hoc teams
KW  - AI Technologies
KW  - Individual agent
KW  - Set of rules
KW  - Social behavior
KW  - Social norm
KW  - Teaching agents
KW  - Autonomous agents
PB  - International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)
SN  - 15488403 (ISSN); 978-151089200-2 (ISBN)
LA  - English
J2  - Proc. Int. Joint Conf. Auton. Agents Multiagent Syst., AAMAS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5; Conference name: 18th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2019; Conference date: 13 May 2019 through 17 May 2019; Conference code: 155776
ER  -

TY  - CONF
AU  - Holzinger, A.
AU  - Goebel, R.
AU  - Fong, R.
AU  - Moon, T.
AU  - Müller, K.-R.
AU  - Samek, W.
TI  - xxAI - Beyond Explainable Artificial Intelligence
PY  - 2022
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 13200 LNAI
SP  - 3
EP  - 10
DO  - 10.1007/978-3-031-04083-2_1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128906717&doi=10.1007%2f978-3-031-04083-2_1&partnerID=40&md5=8cf203855f78513aff15d563922a3324
AD  - Human-Centered AI Lab, University of Natural Resources and Life Sciences, Vienna, Austria
AD  - Medical University Graz, Graz, Austria
AD  - xAI Lab, Alberta Machine Intelligence Institute, Edmonton, Canada
AD  - Princeton University, Princeton, United States
AD  - Seoul National University, Seoul, South Korea
AD  - Department of Artificial Intelligence, Korea University, Seoul, South Korea
AD  - Max Planck Institute for Informatics, Saarbrücken, Germany
AD  - Machine Learning Group, Technical University of Berlin, Berlin, Germany
AD  - Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, Berlin, Germany
AD  - BIFOLD – Berlin Institute for the Foundations of Data and Learning, Berlin, Germany
AB  - The success of statistical machine learning from big data, especially of deep learning, has made artificial intelligence (AI) very popular. Unfortunately, especially with the most successful methods, the results are very difficult to comprehend by human experts. The application of AI in areas that impact human life (e.g., agriculture, climate, forestry, health, etc.) has therefore led to an demand for trust, which can be fostered if the methods can be interpreted and thus explained to humans. The research field of explainable artificial intelligence (XAI) provides the necessary foundations and methods. Historically, XAI has focused on the development of methods to explain the decisions and internal mechanisms of complex AI systems, with much initial research concentrating on explaining how convolutional neural networks produce image classification predictions by producing visualizations which highlight what input patterns are most influential in activating hidden units, or are most responsible for a model’s decision. In this volume, we summarize research that outlines and takes next steps towards a broader vision for explainable AI in moving beyond explaining classifiers via such methods, to include explaining other kinds of models (e.g., unsupervised and reinforcement learning models) via a diverse array of XAI techniques (e.g., question-and-answering systems, structured explanations). In addition, we also intend to move beyond simply providing model explanations to directly improving the transparency, efficiency and generalization ability of models. We hope this volume presents not only exciting research developments in explainable AI but also a guide for what next areas to focus on within this fascinating and highly relevant research field as we enter the second decade of the deep learning revolution. This volume is an outcome of the ICML 2020 workshop on “XXAI: Extending Explainable AI Beyond Deep Models and Classifiers.” © 2022, The Author(s).
KW  - Artificial intelligence
KW  - Explainability
KW  - Explainable AI
KW  - Machine learning
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Reinforcement learning
KW  - Artificial intelligence systems
KW  - Classification prediction
KW  - Convolutional neural network
KW  - Explainability
KW  - Explainable artificial intelligence
KW  - Human expert
KW  - Human lives
KW  - Images classification
KW  - Research fields
KW  - Statistical machine learning
KW  - Forestry
A2  - Holzinger A.
A2  - Goebel R.
A2  - Fong R.
A2  - Moon T.
A2  - Müller K.
A2  - Samek W.
PB  - Springer Science and Business Media Deutschland GmbH
SN  - 03029743 (ISSN); 978-303104082-5 (ISBN)
LA  - English
J2  - Lect. Notes Comput. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 16; Correspondence Address: A. Holzinger; Human-Centered AI Lab, University of Natural Resources and Life Sciences, Vienna, Austria; email: andreas.holzinger@human-centered.ai; Conference name: International Workshop on Extending Explainable AI Beyond Deep Models and Classifiers, xxAI 2020, held in Conjunction with ICML 2020; Conference date: 18 July 2020 through 18 July 2020; Conference code: 276949
ER  -

TY  - CONF
AU  - Rolf, M.
AU  - Crook, N.
AU  - Steil, J.
TI  - From social interaction to ethical AI: A developmental roadmap
PY  - 2018
T2  - 2018 Joint IEEE 8th International Conference on Development and Learning and Epigenetic Robotics, ICDL-EpiRob 2018
C7  - 8761023
SP  - 204
EP  - 211
DO  - 10.1109/DEVLRN.2018.8761023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064513268&doi=10.1109%2fDEVLRN.2018.8761023&partnerID=40&md5=4b62cb31179438eb854cbffac912132b
AD  - School of Engineering Computing and Maths, Oxford Brookes University, United Kingdom
AD  - Institute for Robotics and Process Control, Technische Universität, Braunschweig, Germany
AB  - AI and robot ethics have recently gained a lot of attention because adaptive machines are increasingly involved in ethically sensitive scenarios and cause incidents of public outcry. Much of the debate has been focused on achieving highest moral standards in handling ethical dilemmas on which not even humans can agree, which indicates that the wrong questions are being asked. We suggest to address this ethics debate strictly through the lens of what behavior seems socially acceptable, rather than idealistically ethical. Learning such behavior puts the debate into the very heart of developmental robotics. This paper poses a roadmap of computational and experimental questions to address the development of socially acceptable machines. We emphasize the need for social reward mechanisms and learning architectures that integrate these while reaching beyond limitations of plain reinforcement-learning agents. We suggest to use the metaphor of 'needs' to bridge rewards and higher level abstractions such as goals for both communication and action generation in a social context. We then suggest a series of experimental questions and possible platforms and paradigms to guide future research in the area. © 2018 IEEE.
KW  - Intelligent agents
KW  - Philosophical aspects
KW  - Robotics
KW  - Developmental robotics
KW  - Ethical dilemma
KW  - Higher-level abstraction
KW  - Learning architectures
KW  - Plain reinforcement
KW  - Social context
KW  - Social interactions
KW  - Through the lens
KW  - Reinforcement learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-153866110-9 (ISBN)
LA  - English
J2  - Jt. IEEE Int. Conf. Dev. Learn. Epigenetic Robot., ICDL-EpiRob
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5; Conference name: Joint 8th IEEE International Conference on Development and Learning and Epigenetic Robotics, ICDL-EpiRob 2018; Conference date: 16 September 2018 through 20 September 2018; Conference code: 149923
ER  -

TY  - CONF
AU  - Maske, H.
AU  - Chu, T.
AU  - Kalabič, U.
TI  - Large-scale traffic control using autonomous vehicles and decentralized deep reinforcement learning
PY  - 2019
T2  - 2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019
C7  - 8916776
SP  - 3816
EP  - 3821
DO  - 10.1109/ITSC.2019.8916776
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076821551&doi=10.1109%2fITSC.2019.8916776&partnerID=40&md5=06f9ab8de9e38be1acb3c0411d1f2c82
AD  - Mitsubishi Electric Research Laboratories, Cambridge, 02139, MA, United States
AD  - Uhana Inc, Palo Alto, 94306, CA, United States
AB  - In this work, we introduce a scalable, decentralized deep reinforcement learning (RL) scheme for optimizing vehicle traffic consisting of both autonomous and human-driven vehicles. The control inputs to the system are the following distance and lane placement of the autonomous vehicles and the human-vehicles are uncontrolled. One point of novelty of the scheme is that it is trained on images of traffic, where pixels are colored based on how much of them is occupied by human-driven or autonomous vehicles. Another point of novelty is in how the scheme achieves scalable decentralization; it does so by training multiple RL agents, each responsible for its own region of control, but able to at least partially observe neighboring agents' regions of control. In this way, the scheme is infinitely scalable because an RL agent does not need to communicate with its neighbors, and can be applied in conjunction with systems in which neighboring controllers are not RL-based.We perform a case study of two simulations on a two-lane oval highway in the Simulation of Urban MObility (SUMO) environment. In the first simulation, a single RL agent is applied so that its region of control coincides with an area of traffic congestion. In the second simulation, we apply multiple RL agents covering the entire network. The results of the first simulation show that the single RL agent is able to improve traffic congestion. The results of the second simulation show that the decentralized RL scheme is able to achieve even better results. © 2019 IEEE.
KW  - Deep learning
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Machine learning
KW  - Reinforcement learning
KW  - Traffic congestion
KW  - Control inputs
KW  - Two-lane
KW  - Urban mobility
KW  - Vehicle traffic
KW  - Autonomous vehicles
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-153867024-8 (ISBN)
LA  - English
J2  - IEEE Intell. Transp. Syst. Conf., ITSC
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 8; Conference name: 2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019; Conference date: 27 October 2019 through 30 October 2019; Conference code: 155652
ER  -

TY  - CONF
AU  - Fu, Y.
AU  - Chai, J.
AU  - Zhu, Y.
AU  - Zhao, D.
TI  - LILAC: Learning a Leader for Cooperative Reinforcement Learning
PY  - 2022
T2  - IEEE Conference on Computatonal Intelligence and Games, CIG
VL  - 2022-August
SP  - 49
EP  - 55
DO  - 10.1109/CoG51982.2022.9893619
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139137372&doi=10.1109%2fCoG51982.2022.9893619&partnerID=40&md5=189e900dd81544fa9e102b3000eff4d2
AD  - The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China
AD  - Wuhan University, Electronic Information School, Wuhan, China
AD  - University of Chinese Academy of Sciences, School of Artificial Intelligence, Beijing, China
AB  - In cooperative multi-agent reinforcement learning,role-based learning promises to reach satisfactory policy learning through the decomposition of complicated tasks using roles. Different roles are responsible for different aspects of the task. However, how this group of roles can be quickly identified is not clear. To address this problem, we propose a novel framework, LearnIng a LeAder for Cooperative reinforcement learning (LILAC), which introduces a leader to integrate information to assign roles. Leaders take a broad view of the whole task and feed the integrated information into a Gaussian mixture model to sample role embedding distribution. It enables LILAC to assign appropriate roles to different agents and improves cooperative performance. In order to evaluate the cooperation of multiple agents, a mixing network, inputted by individual local utility networks, is constructed to estimate the global action value. Two loss functions, temporal difference loss and mean divergence loss, are adopted by LILAC to learn network parameters and to encourage diversity of policies for different roles. By virtue of the leader module, LILAC outperforms the StarCraft II micromanagement benchmark in our experiments, especially on challenging tasks.  © 2022 IEEE.
KW  - deep learning
KW  - game AI
KW  - multi-agent reinforcement learning
KW  - role-based method
KW  - Deep learning
KW  - Learning systems
KW  - Multi agent systems
KW  - Reinforcement learning
KW  - Cooperative reinforcement learning
KW  - Deep learning
KW  - Embeddings
KW  - Game AI
KW  - Gaussian Mixture Model
KW  - Integrated informations
KW  - Multi-agent reinforcement learning
KW  - Policy learning
KW  - Role-based
KW  - Role-based method
KW  - Gaussian distribution
PB  - IEEE Computer Society
SN  - 23254270 (ISSN); 978-166545989-1 (ISBN)
LA  - English
J2  - IEEE Conf. Comput. Intell. Games, CIG
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: Y. Fu; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; email: yuqianfu@whu.edu.cn; J. Chai; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; email: chaijiajun2020@ia.ac.cn; Y. Zhu; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; email: yuanheng.zhu@ia.ac.cn; D. Zhao; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; email: dongbin.zhao@ia.ac.cn; Conference name: 2022 IEEE Conference on Games, CoG 2022; Conference date: 21 August 2022 through 24 August 2022; Conference code: 182931
ER  -

TY  - JOUR
AU  - Aguilera, A.
AU  - Figueroa, C.A.
AU  - Hernandez-Ramos, R.
AU  - Sarkar, U.
AU  - Cemballi, A.
AU  - Gomez-Pathak, L.
AU  - Miramontes, J.
AU  - Yom-Tov, E.
AU  - Chakraborty, B.
AU  - Yan, X.
AU  - Xu, J.
AU  - Modiri, A.
AU  - Aggarwal, J.
AU  - Jay Williams, J.
AU  - Lyles, C.R.
TI  - MHealth app using machine learning to increase physical activity in diabetes and depression: Clinical trial protocol for the DIAMANTE Study
PY  - 2020
T2  - BMJ Open
VL  - 10
IS  - 8
C7  - e034723
DO  - 10.1136/bmjopen-2019-034723
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089769786&doi=10.1136%2fbmjopen-2019-034723&partnerID=40&md5=e9593fcf22a21c05979b941efd1e8dd5
AD  - School of Social Welfare, University of California Berkeley, Berkeley, CA, United States
AD  - Ucsf Center for Vulnerable Populations, Division of General Internal Medicine San Francisco, Zuckerberg San Francisco General Hospital, San Francisco, CA, United States
AD  - Microsoft Research, Herzeliya, Israel
AD  - Centre for Quantitative Medicine, Duke-National University, Singapore Medical School, Singapore, Singapore
AD  - Department of Statistics and Applied Probability, National University of Singapore, Singapore, Singapore
AD  - Department of Biostatistics and Bioinformatics, Duke University, Durham, NC, United States
AD  - Computer Science University of Toronto, Toronto, ON, Canada
AB  - Introduction Depression and diabetes are highly disabling diseases with a high prevalence and high rate of comorbidity, particularly in low-income ethnic minority patients. Though comorbidity increases the risk of adverse outcomes and mortality, most clinical interventions target these diseases separately. Increasing physical activity might be effective to simultaneously lower depressive symptoms and improve glycaemic control. Self-management apps are a cost-effective, scalable and easy access treatment to increase physical activity. However, cutting-edge technological applications often do not reach vulnerable populations and are not tailored to an individual's behaviour and characteristics. Tailoring of interventions using machine learning methods likely increases the effectiveness of the intervention. Methods and analysis In a three-arm randomised controlled trial, we will examine the effect of a text-messaging smartphone application to encourage physical activity in low-income ethnic minority patients with comorbid diabetes and depression. The adaptive intervention group receives messages chosen from different messaging banks by a reinforcement learning algorithm. The uniform random intervention group receives the same messages, but chosen from the messaging banks with equal probabilities. The control group receives a weekly mood message. We aim to recruit 276 adults from primary care clinics aged 18-75 years who have been diagnosed with current diabetes and show elevated depressive symptoms (Patient Health Questionnaire depression scale-8 (PHQ-8) >5). We will compare passively collected daily step counts, self-report PHQ-8 and most recent haemoglobin A1c from medical records at baseline and at intervention completion at 6-month follow-up. Ethics and dissemination The Institutional Review Board at the University of California San Francisco approved this study (IRB: 17-22608). We plan to submit manuscripts describing our user-designed methods and testing of the adaptive learning algorithm and will submit the results of the trial for publication in peer-reviewed journals and presentations at (inter)-national scientific meetings. Trial registration number NCT03490253; pre-results. © Author(s) (or their employer(s)) 2020. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.
KW  - Depression & mood disorders
KW  - Diabetes & endocrinology
KW  - Health informatics
KW  - Telemedicine
KW  - Adolescent
KW  - Adult
KW  - Aged
KW  - Depression
KW  - Diabetes Mellitus
KW  - Ethnic Groups
KW  - Exercise
KW  - Humans
KW  - Machine Learning
KW  - Middle Aged
KW  - Minority Groups
KW  - Mobile Applications
KW  - Randomized Controlled Trials as Topic
KW  - San Francisco
KW  - Telemedicine
KW  - Young Adult
KW  - hemoglobin A1c
KW  - adult
KW  - aged
KW  - Article
KW  - clinical trial protocol
KW  - comorbidity
KW  - controlled study
KW  - depression
KW  - diabetes mellitus
KW  - ethnic group
KW  - female
KW  - follow up
KW  - human
KW  - machine learning
KW  - major clinical study
KW  - male
KW  - Patient Health Questionnaire 8
KW  - physical activity
KW  - primary medical care
KW  - randomized controlled trial
KW  - reinforcement learning (machine learning)
KW  - self care
KW  - self report
KW  - step count
KW  - text messaging
KW  - adolescent
KW  - California
KW  - depression
KW  - diabetes mellitus
KW  - exercise
KW  - machine learning
KW  - middle aged
KW  - minority group
KW  - mobile application
KW  - randomized controlled trial (topic)
KW  - telemedicine
KW  - young adult
PB  - BMJ Publishing Group
SN  - 20446055 (ISSN)
C2  - 32819981
LA  - English
J2  - BMJ Open
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 52; Correspondence Address: C.A. Figueroa; School of Social Welfare, University of California Berkeley, Berkeley, United States; email: c.a.fgueroa@berkeley.edu
ER  -

TY  - CONF
AU  - Philezwini Sithungu, S.
AU  - Marie Ehlers, E.
TI  - A Reinforcement Learning-Based Classification Symbiont Agent for Dynamic Difficulty Balancing
PY  - 2020
T2  - ACM International Conference Proceeding Series
SP  - 15
EP  - 23
DO  - 10.1145/3440840.3440856
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101783433&doi=10.1145%2f3440840.3440856&partnerID=40&md5=826efcaad997e72dbca7c444174148c4
AD  - Academy of Computer Science and Software Engineering, University of Johannesburg, Johannesburg, South Africa
AB  - AdaptiveSGA is a mechanism for achieving Adaptive Game AI-based Dynamic Difficulty Balancing in games. AdaptiveSGA is based on the Symbiotic Game Agent model and, therefore, leverages the advantages of biological symbiosis. Within the AdaptiveSGA architecture, the classification symbiont agent is responsible for the dynamic difficulty balancing component. Current work proposes the use of a classification symbiont agent that makes use of reinforcement learning to optimise dynamic difficulty balancing in order to match the opponent's skill. Current work also introduces three different types of decision-making algorithms that can be used by decision-making symbiont agents to display different kinds of behaviour. The ability to reproduce different kinds of NPC behaviour forms the adaptive game AI component of AdaptiveSGA. Experimental results showed that the reinforcement learning-based classification symbiont agent can achieve an even game with opponents and can further help minimise the number of draws. © 2020 ACM.
KW  - Classification symbiont agent
KW  - Dynamic difficulty balancing
KW  - Q-learning
KW  - Symbiotic game agent
KW  - Decision making
KW  - Intelligent computing
KW  - Adaptive game AI
KW  - Agent model
KW  - Decision-making algorithms
KW  - Reinforcement learning
PB  - Association for Computing Machinery
SN  - 978-145038808-5 (ISBN)
LA  - English
J2  - ACM Int. Conf. Proc. Ser.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 4; Correspondence Address: E. Marie Ehlers; Academy of Computer Science and Software Engineering, University of Johannesburg, Johannesburg, South Africa; email: emehlers@uj.ac.za; Conference name: 3rd International Conference on Computational Intelligence and Intelligent Systems, CIIS 2020; Conference date: 13 November 2020 through 15 November 2020; Conference code: 167083
ER  -

TY  - CONF
AU  - Afsar, M.M.
TI  - Intelligent multi-purpose healthcare bot facilitating shared decision making
PY  - 2019
T2  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
VL  - 4
SP  - 2393
EP  - 2395
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077021999&partnerID=40&md5=9c3964072bd0032435db3116dc9ff092
AD  - University of Calgary, Calgary, AB, Canada
AB  - Patient decision AIDS (PtDAs) have been promoted to facilitate personalized information retrieval and decision support; nonetheless, although promoted for more than 20 years, they have generally failed to gain a foothold in the general delivery of healthcare. Intelligent interactive agent technologies could address the design features necessary to facilitate support and shared-decision making. In this thesis, we develop and build a PtDA for Prostate cancer using intelligent agent technology. The proposed system, called ALAN, has a multi-layered architecture with three layers. While the first layer (User-Interface) is responsible to effectively interact with users (patients and physicians), the bottom layer (Data) handles requests regarding storing and retrieving the data. Unlike most existing bots, our core objective is to enable ALAN with learning abilities, which can evolve in the course of time and improve its behaviour with minimum distraction of the user. To this end, reinforcement learning and deep learning algorithms are employed in the main layer, i.e., Analytical Decision Making. This research is expected to have impact on delivery of personalized healthcare. Montreal. Canada. © 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.
KW  - Chatbots
KW  - Multi-agent systems
KW  - Patient decision AIDS
KW  - Reinforcement learning
KW  - Shared-decision making
KW  - Autonomous agents
KW  - Decision making
KW  - Decision support systems
KW  - Deep learning
KW  - Diseases
KW  - Health care
KW  - Intelligent agents
KW  - Learning algorithms
KW  - Machine learning
KW  - Reinforcement learning
KW  - User interfaces
KW  - Analytical decisions
KW  - Chatbots
KW  - Decision aids
KW  - Intelligent agent technology
KW  - Interactive agents
KW  - Personalized healthcare
KW  - Personalized information retrieval
KW  - Shared decision makings
KW  - Multi agent systems
PB  - International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)
SN  - 15488403 (ISSN); 978-151089200-2 (ISBN)
LA  - English
J2  - Proc. Int. Joint Conf. Auton. Agents Multiagent Syst., AAMAS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: M.M. Afsar; University of Calgary, Calgary, Canada; email: afsar@ieee.org; Conference name: 18th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2019; Conference date: 13 May 2019 through 17 May 2019; Conference code: 155776
ER  -

TY  - JOUR
AU  - Daglarli, E.
TI  - Computational modeling of prefrontal cortex for meta-cognition of a humanoid robot
PY  - 2020
T2  - IEEE Access
VL  - 8
C7  - 9103513
SP  - 98491
EP  - 98507
DO  - 10.1109/ACCESS.2020.2998396
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086075046&doi=10.1109%2fACCESS.2020.2998396&partnerID=40&md5=27438cd2defe5a9ed81a37b725e6cddd
AD  - Faculty of Computer and Informatics Engineering, Istanbul Technical University, Istanbul, 34469, Turkey
AB  - For robot intelligence and human-robot interaction (HRI), complex decision-making, interpretation, and adaptive planning processes are great challenges. These require recursive task processing and meta-cognitive reasoning mechanism. Naturally, the human brain realizes these cognitive skills by prefrontal cortex which is a part of the neocortex. Previous studies about neurocognitive robotics would not meet these requirements. Thus, it is aimed at developing a brain-inspired robot control architecture that performs spatial-temporal and emotional reasoning. In this study, we present a novel solution that covers a computational model of the prefrontal cortex for humanoid robots. Computational mechanisms are mainly placed on the bio-physical plausible neural structures embodied in different dynamics. The main components of the system are composed of several computational modules including dorsolateral, ventrolateral, anterior, and medial prefrontal regions. Also, it is responsible for organizing the working memory. A reinforcement meta-learning based explainable artificial intelligence (xAI) procedure is applied to the working memory regions of the computational prefrontal cortex model. Experimental evaluation and verification tests are processed by the developed software framework embodied in the humanoid robot platform. The humanoid robots' perceptual states and cognitive processes including emotion, attention, and intention-based reasoning skills can be observed and controlled via the developed software. Several interaction scenarios are implemented to monitor and evaluate the model's performance. © 2013 IEEE.
KW  - Artificial intelligence
KW  - brain modeling
KW  - cognitive robotics
KW  - human-robot interaction
KW  - Anthropomorphic robots
KW  - Brain
KW  - Computation theory
KW  - Decision making
KW  - Intelligent robots
KW  - Reinforcement learning
KW  - Robot programming
KW  - Software testing
KW  - Verification
KW  - Computational model
KW  - Experimental evaluation
KW  - Human robot Interaction (HRI)
KW  - Neural structures
KW  - Robot control architecture
KW  - Robot intelligences
KW  - Software frameworks
KW  - Verification tests
KW  - Human robot interaction
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 21693536 (ISSN)
LA  - English
J2  - IEEE Access
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 4; Correspondence Address: E. Daglarli; Faculty of Computer and Informatics Engineering, Istanbul Technical University, Istanbul, 34469, Turkey; email: evren.daglarli@itu.edu.tr
ER  -

TY  - CONF
AU  - Peschl, M.
AU  - Zgonnikov, A.
AU  - Oliehoek, F.A.
AU  - Siebert, L.C.
TI  - MORAL: Aligning AI with Human Norms through Multi-Objective Reinforced Active Learning
PY  - 2022
T2  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
VL  - 2
SP  - 1038
EP  - 1046
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134357119&partnerID=40&md5=0c2fe9165d7d23bdf1c8dabf1a87665a
AD  - Delft University of Technology, Delft, Netherlands
AB  - Inferring reward functions from demonstrations and pairwise preferences are auspicious approaches for aligning Reinforcement Learning (RL) agents with human intentions. However, state-of-the art methods typically focus on learning a single reward model, thus rendering it difficult to trade off different reward functions from multiple experts. We propose Multi-Objective Reinforced Active Learning (MORAL), a novel method for combining diverse demonstrations of social norms into a Pareto-optimal policy. Through maintaining a distribution over scalarization weights, our approach is able to interactively tune a deep RL agent towards a variety of preferences, while eliminating the need for computing multiple policies. We empirically demonstrate the effectiveness of MORAL in two scenarios, which model a delivery and an emergency task that require an agent to act in the presence of normative conflicts. Overall, we consider our research a step towards multi-objective RL with learned rewards, bridging the gap between current reward learning and machine ethics literature. © 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved
KW  - Active Learning
KW  - Inverse Reinforcement Learning
KW  - Multi-Objective Decision-Making
KW  - Value Alignment
KW  - Autonomous agents
KW  - Decision making
KW  - Economic and social effects
KW  - Inverse problems
KW  - Learning systems
KW  - Multi agent systems
KW  - Pareto principle
KW  - Active Learning
KW  - Human intentions
KW  - Inverse reinforcement learning
KW  - Multi objective
KW  - Multi objective decision making
KW  - Reinforcement learning agent
KW  - Reward function
KW  - State-of-the-art methods
KW  - Trade off
KW  - Value alignment
KW  - Reinforcement learning
PB  - International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)
SN  - 15488403 (ISSN); 978-171385433-3 (ISBN)
LA  - English
J2  - Proc. Int. Joint Conf. Auton. Agents Multiagent Syst., AAMAS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 3; Conference name: 21st International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2022; Conference date: 9 May 2022 through 13 May 2022; Conference code: 180727
ER  -

TY  - JOUR
AU  - Bouton, M.
AU  - Julian, K.D.
AU  - Nakhaei, A.
AU  - Fujimura, K.
AU  - Kochenderfer, M.J.
TI  - Decomposition methods with deep corrections for reinforcement learning
PY  - 2019
T2  - Autonomous Agents and Multi-Agent Systems
VL  - 33
IS  - 3
SP  - 330
EP  - 352
DO  - 10.1007/s10458-019-09407-z
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064926995&doi=10.1007%2fs10458-019-09407-z&partnerID=40&md5=f3f2740bdf159be1fe94d3feff730c04
AD  - Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, United States
AD  - Honda Research Institute, Mountain View, CA, United States
AB  - Decomposition methods have been proposed to approximate solutions to large sequential decision making problems. In contexts where an agent interacts with multiple entities, utility decomposition can be used to separate the global objective into local tasks considering each individual entity independently. An arbitrator is then responsible for combining the individual utilities and selecting an action in real time to solve the global problem. Although these techniques can perform well empirically, they rely on strong assumptions of independence between the local tasks and sacrifice the optimality of the global solution. This paper proposes an approach that improves upon such approximate solutions by learning a correction term represented by a neural network. We demonstrate this approach on a fisheries management problem where multiple boats must coordinate to maximize their catch over time as well as on a pedestrian avoidance problem for autonomous driving. In each problem, decomposition methods can scale to multiple boats or pedestrians by using strategies involving one entity. We verify empirically that the proposed correction method significantly improves the decomposition method and outperforms a policy trained on the full scale problem without utility decomposition. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.
KW  - Autonomous driving
KW  - Deep reinforcement learning
KW  - Multi-fidelity optimization
KW  - Utility fusion
KW  - Boats
KW  - Decision making
KW  - Deep learning
KW  - Machine learning
KW  - Problem solving
KW  - Approximate solution
KW  - Autonomous driving
KW  - Correction method
KW  - Decomposition methods
KW  - Fisheries management
KW  - Global solutions
KW  - Multi-fidelity optimization
KW  - Sequential decision making
KW  - Reinforcement learning
PB  - Springer New York LLC
SN  - 13872532 (ISSN)
LA  - English
J2  - Auton. Agents Multi-Agent Syst.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5; Correspondence Address: M. Bouton; Department of Aeronautics and Astronautics, Stanford University, Stanford, United States; email: boutonm@stanford.edu
ER  -

TY  - JOUR
AU  - Sharma, S.
AU  - Singh, B.
TI  - Weighted cooperative reinforcement learning-based energy-efficient autonomous resource selection strategy for underlay D2D communication
PY  - 2019
T2  - IET Communications
VL  - 13
IS  - 14
SP  - 2078
EP  - 2087
DO  - 10.1049/iet-com.2018.6028
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070953496&doi=10.1049%2fiet-com.2018.6028&partnerID=40&md5=eb66da600e69f53138980f6dd1ebd744
AD  - ECE Department, NIT, Kurukshetra, Haryana, India
AB  - Underlay Device-to-Device (D2D) communication is a key technology responsible for high data rate, ultra-low latency with high spectral and energy efficiency in 5G cellular networks. But to achieve its full potential, optimal channel allocation and effective co-channel interference management must be accomplished. To address this challenge, we propose a multi-agent reinforcement learning based autonomous channel selection scheme for D2D communication. The proposed scheme, Weighted Cooperative Q-Learning based Resource Selection (WCopQLRS), allows a D2D pair to learn to select a channel from the available resources autonomously. Learning process of each D2D transmitter involves cooperation from neighboring D2D agents by exchanging their latest Q-values. An additional parameter called cooperation range is used to determine the neighboring pairs whose Q-values can be used for learning the optimal policy. The limited prior information prevents a linear increase in the dimensions of Q-value matrix of each learning agent when the number of D2D pairs within the cell is huge. Though WCopQL-RS involves additional information exchange among agents as compared to independent learning but also provides improved system throughput and convergence speed. It is shown through simulation results that WCopQL-RS outperforms other existing schemes in terms of average D2D user throughput, energy consumption and fairness value. © The Institution of Engineering and Technology 2019.
KW  - 5G mobile communication systems
KW  - Autonomous agents
KW  - Cooperative communication
KW  - Energy efficiency
KW  - Energy utilization
KW  - Machine learning
KW  - Multi agent systems
KW  - Channel selection
KW  - Cooperative reinforcement learning
KW  - D2D communications
KW  - Deviceto-device (D2D) communication
KW  - Independent learning
KW  - Information exchanges
KW  - Multi-agent reinforcement learning
KW  - Resource selection
KW  - Reinforcement learning
PB  - Institution of Engineering and Technology
SN  - 17518628 (ISSN)
LA  - English
J2  - IET Commun.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 14; Correspondence Address: S. Sharma; ECE Department, NIT, Kurukshetra, India; email: sandeepika31@gmail.com
ER  -

TY  - JOUR
AU  - Lu, W.
AU  - Liu, D.
TI  - A2: Extracting cyclic switchings from DOB-nets for rejecting excessive disturbances: A2: Extracting cyclic switchings from DOB-nets for rejecting excessive disturbances
PY  - 2020
T2  - Neurocomputing
VL  - 400
SP  - 161
EP  - 172
DO  - 10.1016/j.neucom.2020.03.014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081974324&doi=10.1016%2fj.neucom.2020.03.014&partnerID=40&md5=9940590f2f63feb9202f39faff24074c
AD  - School of Mechanical Engineering and Automation, Harbin Institute of Technology, (Shenzhen), China
AD  - CAS, UTS, Australia
AB  - Reinforcement Learning (RL) is limited in practice by its poor explainability, which is responsible for insufficient trustiness from users, unsatisfied interpretation for human intervention, inadequate analysis for future improvement, etc. This paper seeks to partially characterize the interplay between dynamical environments and a previously-proposed Disturbance OBserver net (DOB-net). The DOB-net is trained via RL and offers optimal control for a set of Partially Observable Markovian Decision Processes (POMDPs). The transition function of each POMDP is largely determined by the environments (excessive external disturbances). This paper proposes an Attention-based Abstraction (A2) approach to extract a finite-state automaton, referred to as a Key Moore Machine Network (KMMN), to capture the switching mechanisms exhibited by the DOB-net in dealing with multiple such POMDPs. A2 first quantizes the controlled platform by learning continuous-discrete interfaces. Then it extracts the KMMN by finding the key hidden states and transitions that attract sufficient attention from the DOB-net. Within the resultant KMMN, three patterns of cyclic switchings (between key hidden states) are found, and saturated controls are shown synchronized with unknown disturbances. Interestingly, the found switchings have previously appeared in the control design for often-saturated systems. They are interpreted via an analogy to the discrete-event subsystem of hybrid control. © 2020
KW  - Disturbance rejection
KW  - Finite-state machine
KW  - Hybrid system
KW  - Multiple POMDPs
KW  - Reinforcement learning
KW  - Disturbance rejection
KW  - Finite automata
KW  - Hybrid systems
KW  - Disturbance observer
KW  - Dynamical environment
KW  - External disturbances
KW  - Future improvements
KW  - Markovian decision process
KW  - Multiple POMDPs
KW  - Transition functions
KW  - Unknown disturbance
KW  - Article
KW  - automation
KW  - environment
KW  - key moore machine network
KW  - Markov jump
KW  - model predictive control
KW  - policy
KW  - priority journal
KW  - recurrent neural network
KW  - reinforcement learning (machine learning)
KW  - Reinforcement learning
PB  - Elsevier B.V.
SN  - 09252312 (ISSN)
LA  - English
J2  - Neurocomputing
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: W. Lu; School of Mechanical Engineering and Automation, Harbin Institute of Technology, China; email: wenjie.lu@outlook.com; CODEN: NRCGE
ER  -

TY  - CONF
AU  - Chaput, R.
AU  - Duval, J.
AU  - Boissier, O.
AU  - Guillermin, M.
AU  - Hassas, S.
TI  - A Multi-Agent Approach to Combine Reasoning and Learning for an Ethical Behavior
PY  - 2021
T2  - AIES 2021 - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society
SP  - 13
EP  - 23
DO  - 10.1145/3461702.3462515
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112403429&doi=10.1145%2f3461702.3462515&partnerID=40&md5=262b4ef5b31bf03995b67d443164efb1
AD  - Univ Lyon, Université Lyon 1, Liris, UMR5205, Lyon, France
AB  - The recent field of Machine Ethics is experiencing rapid growth to answer the societal need for Artificial Intelligence (AI) algorithms imbued with ethical considerations, such as benevolence toward human users and actors. Several approaches already exist for this purpose, mostly either by reasoning over a set of predefined ethical principles (Top-Down), or by learning new principles (Bottom-Up). While both methods have their own advantages and drawbacks, only few works have explored hybrid approaches, such as using symbolic rules to guide the learning process for instance, combining the advantages of each. This paper draws upon existing works to propose a novel hybrid method using symbolic judging agents to evaluate the ethics of learning agents' behaviors, and accordingly improve their ability to ethically behave in dynamic multi-agent environments. Multiple benefits ensue from this separation between judging and learning agents: agents can evolve (or be updated by human designers) separately, benefiting from co-construction processes; judging agents can act as accessible proxies for non-expert human stakeholders or regulators; and finally, multiple points of view (one per judging agent) can be adopted to judge the behavior of the same agent, which produces a richer feedback. Our proposed approach is applied to an energy distribution problem, in the context of a Smart Grid simulator, with continuous and multi-dimensional states and actions. The experiments and results show the ability of learning agents to correctly adapt their behaviors to comply with the judging agents' rules, including when rules evolve over time. © 2021 Owner/Author.
KW  - ethical judgment
KW  - ethics
KW  - hybrid neural-symbolic learning
KW  - machine ethics
KW  - multi-agent learning
KW  - reinforcement learning
KW  - Combines
KW  - Intelligent agents
KW  - Multi agent systems
KW  - Philosophical aspects
KW  - Smart power grids
KW  - Co-construction process
KW  - Energy distributions
KW  - Ethical considerations
KW  - Ethical principles
KW  - Learning process
KW  - Multi-agent approach
KW  - Multi-agent environment
KW  - Multi-dimensional state
KW  - Learning systems
PB  - Association for Computing Machinery, Inc
SN  - 978-145038473-5 (ISBN)
LA  - English
J2  - AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 4; Conference name: 4th AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, AIES 2021; Conference date: 19 May 2021 through 21 May 2021; Conference code: 170685
ER  -

TY  - JOUR
AU  - Amato, C.
AU  - Ammar, H.B.
AU  - Churchill, E.
AU  - Karpas, E.
AU  - Kido, T.
AU  - Kuniavsky, M.
AU  - Lawless, W.F.
AU  - Oliehoek, F.A.
AU  - Rossi, F.
AU  - Russell, S.
AU  - Srivastava, S.
AU  - Takadama, K.
AU  - Van Allen, P.
AU  - Brent Venable, K.
AU  - Tuyls, K.
AU  - Vrancx, P.
AU  - Zhang, S.
TI  - Reports on the 2018 AAAI spring symposium series
PY  - 2018
T2  - AI Magazine
VL  - 39
IS  - 4
SP  - 29
EP  - 35
DO  - 10.1609/aimag.v39i4.2824
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059785329&doi=10.1609%2faimag.v39i4.2824&partnerID=40&md5=194561657fe55f0546eadf63cc74c0dd
AD  - Northeastern University, United States
AD  - PROWLER.io, Cambridge, United Kingdom
AD  - Google, United States
AD  - Israel Institute of Technology, Israel
AD  - Stanford University, United States
AD  - Parc, United States
AD  - Paine College, United States
AD  - TU Delft, University of Liverpool, United Kingdom
AD  - IBM T. J. Watson Research Center, University of Padova, Italy
AD  - US Army Research Laboratory, United States
AD  - Arizona State University, United States
AD  - University of ElectroCommunications in Japan, Japan
AD  - Google DeepMind, United States
AD  - Art Center College of Design, United States
AD  - Tulane University, Florida Institute for Human and Machine Cognition (IHMC), United States
AD  - Cleveland State University, United States
AB  - The Association for the Advancement of Artifcial Intelligence, in cooperation with Stanford University's Department of Computer Science, presented the 2018 Spring Symposium Series, held March 26-28, 2018, on the campus of Stanford University. The seven symposia held were AI and Society: Ethics, Safety, and Trustworthiness in Intelligent Agents; Artifcial Intelligence for the Internet of Everything; Beyond Machine Intelligence: Understanding Cognitive Bias and Humanity for WellBeing AI; Data-Effcient Reinforcement Learning; The Design of the User Experience for Artifcial Intelligence (the UX of AI); Integrated Representation, Reasoning, Learning, and Execution for Goal-Directed Autonomy; Learning, Inference, and Control of Multiagent Systems. This report, compiled from organizers of the symposia, summarizes the research of the symposia that took place. © 2018, Association for the Advancement of Artifcial Intelligence. All rights reserved.
KW  - Intelligent agents
KW  - Multi agent systems
KW  - Reinforcement learning
KW  - Cognitive bias
KW  - Goal directed
KW  - Integrated representations
KW  - Machine intelligence
KW  - Stanford University
KW  - User experience
KW  - Wellbeing
KW  - Cognitive systems
PB  - AI Access Foundation
SN  - 07384602 (ISSN)
LA  - English
J2  - AI Mag
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; CODEN: AIMAE
ER  -

TY  - CONF
AU  - Jayant, A.K.
AU  - Bhatnagar, S.
TI  - Model-based Safe Deep Reinforcement Learning via a Constrained Proximal Policy Optimization Algorithm
PY  - 2022
T2  - Advances in Neural Information Processing Systems
VL  - 35
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162125176&partnerID=40&md5=5f9928f370a684f5ebfccb1d9e7f8b9d
AD  - Department of Computer Science Automation, Indian Institute of Science, Bangalore, India
AB  - During initial iterations of training in most Reinforcement Learning (RL) algorithms, agents perform a significant number of random exploratory steps. In the real world, this can limit the practicality of these algorithms as it can lead to potentially dangerous behavior. Hence safe exploration is a critical issue in applying RL algorithms in the real world. This problem has been recently well studied under the Constrained Markov Decision Process (CMDP) Framework, where in addition to single-stage rewards, an agent receives single-stage costs or penalties as well depending on the state transitions. The prescribed cost functions are responsible for mapping undesirable behavior at any given time-step to a scalar value. The goal then is to find a feasible policy that maximizes reward returns while constraining the cost returns to be below a prescribed threshold during training as well as deployment. We propose an On-policy Model-based Safe Deep RL algorithm in which we learn the transition dynamics of the environment in an online manner as well as find a feasible optimal policy using the Lagrangian Relaxation-based Proximal Policy Optimization. We use an ensemble of neural networks with different initializations to tackle epistemic and aleatoric uncertainty issues faced during environment model learning. We compare our approach with relevant model-free and model-based approaches in Constrained RL using the challenging Safe Reinforcement Learning benchmark - the Open AI Safety Gym. We demonstrate that our algorithm is more sample efficient and results in lower cumulative hazard violations as compared to constrained model-free approaches. Further, our approach shows better reward performance than other constrained model-based approaches in the literature. © 2022 Neural information processing systems foundation. All rights reserved.
KW  - Constrained optimization
KW  - Cost functions
KW  - Deep learning
KW  - Learning algorithms
KW  - Learning systems
KW  - Markov processes
KW  - Constrained models
KW  - Model based approach
KW  - Model free
KW  - Model-based OPC
KW  - Optimization algorithms
KW  - Policy optimization
KW  - Real-world
KW  - Reinforcement learning algorithms
KW  - Reinforcement learnings
KW  - Single stage
KW  - Reinforcement learning
A2  - Koyejo S.
A2  - Mohamed S.
A2  - Agarwal A.
A2  - Belgrave D.
A2  - Cho K.
A2  - Oh A.
PB  - Neural information processing systems foundation
SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)
LA  - English
J2  - Adv. neural inf. proces. syst.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 6; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185
ER  -

TY  - CONF
AU  - Kasim, S.
AU  - Valliani, N.
AU  - Ki Wong, N.K.
AU  - Samadi, S.
AU  - Watkins, L.
AU  - Rubin, A.
TI  - Cybersecurity as a Tic-Tac-Toe Game Using Autonomous Forwards (Attacking) And Backwards (Defending) Penetration Testing in a Cyber Adversarial Artificial Intelligence System
PY  - 2022
T2  - ICOSNIKOM 2022 - 2022 IEEE International Conference of Computer Science and Information Technology: Boundary Free: Preparing Indonesia for Metaverse Society
DO  - 10.1109/ICOSNIKOM56551.2022.10034922
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149172350&doi=10.1109%2fICOSNIKOM56551.2022.10034922&partnerID=40&md5=ea3cd280cb481407f67f2062bc104c42
AD  - Johns Hopkins University, Department of Computer Science, Baltimore, United States
AB  - In this paper, we investigate the potential behavior of Artificial Intelligence (AI) black hat hackers and the ability of ethical human hackers to defend against them. To this end, we develop an adversarial AI testbed, which marries game board play to the cyber realm by employing a statistical AI algorithm that maps every move made on the tic-tac-toe game board to an attack or defensive move made in the cyber realm. Our work demonstrates that this approach is an effective means of constraining the possible cybersecurity state space for autonomous forwards (i.e., attacking) and backwards (i.e., defending) penetration testing. Our results suggest that welltrained AI hackers may be nearly impossible to beat by human defenders unless prior knowledge of their possible attack strategies are known; however, for humans, this approach quickly becomes intractable. Also, we have observed that different AI algorithms used to search (play) the game state space (and thus the cyber state space) behave as different AI hacker personalities, which illustrates how forwards and backwards penetration testers with varying skills and knowledge would exploit or defend network vulnerabilities.  © 2022 IEEE.
KW  - adversarial
KW  - artificial intelligence
KW  - cybersecurity
KW  - hacking
KW  - machine learning
KW  - reinforcement learning
KW  - Cybersecurity
KW  - Personal computing
KW  - Adversarial
KW  - Artificial intelligence algorithms
KW  - Artificial intelligence systems
KW  - Cyber security
KW  - Hacking
KW  - Machine-learning
KW  - Penetration testing
KW  - Reinforcement learnings
KW  - State-space
KW  - Tic-Tac-Toe game
KW  - Reinforcement learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 979-835039907-3 (ISBN)
LA  - English
J2  - ICOSNIKOM - IEEE Int. Conf. Comput. Sci. Inf. Technol.: Bound. Free: Prep. Indonesia Metaverse Soc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 4th IEEE International Conference of Computer Science and Information Technology, ICOSNIKOM 2022; Conference date: 19 October 2022 through 21 October 2022; Conference code: 186730
ER  -

TY  - CONF
AU  - Lutze, R.
AU  - Waldhör, K.
TI  - Improving dialogue design and control for smartwatches by reinforcement learning based behavioral acceptance patterns
PY  - 2020
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 12183 LNCS
SP  - 75
EP  - 85
DO  - 10.1007/978-3-030-49065-2_6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088742228&doi=10.1007%2f978-3-030-49065-2_6&partnerID=40&md5=780c44355535ef5d6425279c764639bb
AD  - Dr.-Ing. Rainer Lutze Consulting, Wachtlerhof, Langenzenn, Germany
AD  - FOM University of Applied Sciences, Nuremberg, Germany
AB  - Dialogue control for health-oriented smartwatch apps is a multi-dimensional task. In our application scenario, the intended purpose of the smartwatch app is the prevention and detection of health hazards jeopardizing the smartwatch wearer (e.g. exsiccosis because of insufficient drinking); the designated target group of the app are elderly people. The dimension of a potential simultaneity of health hazards and ethical considerations how to position the wearer always in control of the app have been presented before. In this paper we focus on the third dimension of the mandatory acceptance conditions of the app. The intended assistance functionality of the app can be only realized, if the interventions of the app occur only in daily life situations, when the wearer will accept such interventions. We present a machine learning approach, by which the app will learn from the wearer over time, when such interventions are appropriate and accepted - and when the app will be expected to remain silent. Of course, this decision has to take into account also the urgency of the intervention with respect to the severity of the threating health hazard. © Springer Nature Switzerland AG 2020.
KW  - Acceptance patterns for smartwatches
KW  - Ambient assisted living
KW  - Assistance for the elderly
KW  - Health hazard handling
KW  - Machine learning
KW  - Biohazards
KW  - E-learning
KW  - Health
KW  - Human computer interaction
KW  - Reinforcement learning
KW  - Wearable computers
KW  - Acceptance conditions
KW  - Application scenario
KW  - Daily-life situations
KW  - Dialogue control
KW  - Dialogue design
KW  - Ethical considerations
KW  - Machine learning approaches
KW  - Multi dimensional
KW  - Health hazards
A2  - Kurosu M.
PB  - Springer
SN  - 03029743 (ISSN); 978-303049064-5 (ISBN)
LA  - English
J2  - Lect. Notes Comput. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2; Correspondence Address: R. Lutze; Dr.-Ing. Rainer Lutze Consulting, Langenzenn, Wachtlerhof, Germany; email: rainerlutze@lustcon.eu; Conference name: Thematic Area on Human Computer Interaction, HCI 2020, held as part of the 22nd International Conference on Human-Computer Interaction, HCII 2020; Conference date: 19 July 2020 through 24 July 2020; Conference code: 242229
ER  -

TY  - CONF
AU  - Wu, Y.-H.
AU  - Lin, S.-D.
TI  - A low-cost ethics shaping approach for designing reinforcement learning agents
PY  - 2018
T2  - 32nd AAAI Conference on Artificial Intelligence, AAAI 2018
SP  - 1687
EP  - 1694
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055693013&partnerID=40&md5=45575c21ad9ccd3a16f9669140327273
AD  - Department of Computer Science and Information Engineering, National Taiwan University, Taipei, 10617, Taiwan
AB  - This paper proposes a low-cost, easily realizable strategy to equip a reinforcement learning (RL) agent the capability of behaving ethically. Our model allows the designers of RL agents to solely focus on the task to achieve, without having to worry about the implementation of multiple trivial ethical patterns to follow. Based on the assumption that the majority of human behavior, regardless which goals they are achieving, is ethical, our design integrates human policy with the RL policy to achieve the target objective with less chance of violating the ethical code that human beings normally obey. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
KW  - Artificial intelligence
KW  - Behavioral research
KW  - Costs
KW  - Intelligent agents
KW  - Philosophical aspects
KW  - Human behaviors
KW  - Human being
KW  - Low costs
KW  - Reinforcement learning agent
KW  - Reinforcement learning
PB  - AAAI press
SN  - 978-157735800-8 (ISBN)
LA  - English
J2  - AAAI Conf. Artif. Intell., AAAI
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 36; Conference name: 32nd AAAI Conference on Artificial Intelligence, AAAI 2018; Conference date: 2 February 2018 through 7 February 2018; Conference code: 143510
ER  -

TY  - CONF
AU  - Sweetser, P.
AU  - Aitchison, M.
TI  - Do Game Bots Dream of Electric Rewards?: The universality of intrinsic motivation
PY  - 2020
T2  - ACM International Conference Proceeding Series
C7  - 30
DO  - 10.1145/3402942.3402965
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092307187&doi=10.1145%2f3402942.3402965&partnerID=40&md5=bcba7a09febada8e50a14c282b76433d
AD  - Australian National University, Australia
AB  - The purpose of this paper is to draw together theories, ideas, and observations related to rewards, motivation, and play to develop and question our understanding and practice of designing reward-based systems and technology. Our exploration includes reinforcement, rewards, motivational theory, flow, play, games, gamification, and machine learning. We examine the design and psychology of reward-based systems in society and technology, using gamification and machine learning as case studies. We propose that the problems that exist with reward-based systems in our society are also present and pertinent when designing technology. We suggest that motivation, exploration, and play are not just fundamental to human learning and behaviour, but that they could transcend nature into machine learning. Finally, we question the value and potential harm of the reward-based systems that permeate every aspect of our lives and assert the importance of ethics in the design of all systems and technology. © 2020 ACM.
KW  - flow
KW  - games
KW  - gamification
KW  - intrinsic motivation
KW  - learning
KW  - machine learning
KW  - play
KW  - reinforcement learning
KW  - reward-based systems
KW  - rewards
KW  - self-determination theory
KW  - Behavioral research
KW  - Botnet
KW  - Computer games
KW  - Gamification
KW  - Machine learning
KW  - Case-studies
KW  - Game bots
KW  - Human learning
KW  - Intrinsic motivation
KW  - Potential harm
KW  - Society and technologies
KW  - Motivation
A2  - Yannakakis G.N.
A2  - Liapis A.
A2  - Penny K.
A2  - Volz V.
A2  - Khosmood F.
A2  - Lopes P.
PB  - Association for Computing Machinery
SN  - 978-145038807-8 (ISBN)
LA  - English
J2  - ACM Int. Conf. Proc. Ser.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 15th International Conference on the Foundations of Digital Games, FDG 2020; Conference date: 15 September 2020 through 18 September 2020; Conference code: 163063
ER  -

TY  - CONF
AU  - Behzadan, V.
AU  - Minton, J.
AU  - Munir, A.
TI  - TrolleyMod v1.0: An open-source simulation and data collection platform for ethical decision-making in autonomous vehicles
PY  - 2019
T2  - AIES 2019 - Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society
SP  - 391
EP  - 395
DO  - 10.1145/3306618.3314239
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070584891&doi=10.1145%2f3306618.3314239&partnerID=40&md5=dba7cf557f85ed0a99241fd776b0d2ae
AD  - Kansas State University, Manhattan, KS, United States
AB  - This paper presents TrolleyMod v1.0, an open-source platform based on the CARLA simulator for the collection of ethical decision-making data for autonomous vehicles. This platform is designed to facilitate experiments aiming to observe and record human decisions and actions in high-fidelity simulations of ethical dilemmas that occur in the context of driving. Targeting experiments in the class of trolley problems, TrolleyMod provides a seamless approach to creating new experimental settings and environments with the realistic physics-engine and the high-quality graphical capabilities of CARLA and the Unreal Engine. Also, TrolleyMod provides a straightforward interface between the CARLA environment and Python to enable the implementation of custom controllers, such as deep reinforcement learning agents. The results of such experiments can be used for sociological analyses, as well as the training and tuning of value-aligned autonomous vehicles based on social values that are inferred from observations. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
KW  - Artificial Intelligence
KW  - Autonomous Vehicles
KW  - Ethical Decision-Making
KW  - Simulation
KW  - Social Choice
KW  - Artificial intelligence
KW  - Decision making
KW  - Deep learning
KW  - Engines
KW  - Intelligent agents
KW  - Open Data
KW  - Philosophical aspects
KW  - Reinforcement learning
KW  - Simulation platform
KW  - Ethical decision making
KW  - High-fidelity simulations
KW  - Open source platforms
KW  - Realistic physics
KW  - Reinforcement learning agent
KW  - Simulation
KW  - Social choice
KW  - Sociological analysis
KW  - Autonomous vehicles
PB  - Association for Computing Machinery, Inc
SN  - 978-145036324-2 (ISBN)
LA  - English
J2  - AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Conference name: 2nd AAAI/ACM Conference on AI, Ethics, and Society, AIES 2019; Conference date: 27 January 2019 through 28 January 2019; Conference code: 149526
ER  -

TY  - JOUR
AU  - Wieczorek, K.
AU  - Jędrzejko, P.
TI  - The conscience of a machine? Artificial intelligence and the problem of moral responsibility
PY  - 2021
T2  - ER(R)GO
IS  - 42
SP  - 15
EP  - 34
DO  - 10.31261/ERRGO.10418
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100250856&doi=10.31261%2fERRGO.10418&partnerID=40&md5=52a63b0f16b4c21033240fdcf7b5ad20
AD  - University of Silesia, Faculty of Humanities, Katowice, Poland
AB  - The ever-accelerating progress in the area of smart technologies gives rise to new ethical challenges, which humankind will sooner or later have to face. An inevitable component of this progress is the increase in the autonomy of the decision-making processes carried out by machines and systems functioning without direct human control. At least some of these decisions will generate conflicts and moral dilemmas. It is therefore worth the while to reflect today upon the measures that need to be taken in order to endow the autonomous, self-learning and self-replicating entities - products equipped with artificial intelligence and capable of independent operation in a wide variety of external conditions and circumstances - with a unique kind of ethical intelligence. At the core of the problem, which both the designers and the users of entities bestowed with artificial intelligence must eventually face, lies the question of how to attain the optimal balance between the goals, needs and interests of both sides of the human-non-human interaction. It is so, because in the context of the expansion of the autonomy of the machines, the anthropocentric model of ethics does no longer suffice. It is therefore necessary to develop a new, extended and modified, model of ethics: a model which would encompass the whole, thus far non-existent, area of equal relations between the human and the machine, and which would allow one to predict its dynamics. The present article addresses some of the aspects of this claim. © ER(R)GO 2021.
KW  - Artificial intelligence
KW  - Decision-making autonomy
KW  - Ethics
KW  - Reinforcement learning
PB  - Wydawnictwo Uniwersytetu Slaskiego
SN  - 15086305 (ISSN)
LA  - English
J2  - ER(R)GO
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1
ER  -

TY  - JOUR
AU  - Gauld, C.
AU  - Dumas, G.
AU  - Fakra, É.
AU  - Mattout, J.
AU  - Micoulaud-Franchi, J.-A.
TI  - The three cultures of computational psychiatry
ST  - Les trois cultures de la psychiatrie computationnelle
PY  - 2021
T2  - Annales Medico-Psychologiques
VL  - 179
IS  - 1
SP  - 63
EP  - 71
DO  - 10.1016/j.amp.2020.11.011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099430273&doi=10.1016%2fj.amp.2020.11.011&partnerID=40&md5=acf5a58bccb9d73b02a7cadfb33c9aff
AD  - Institut d'histoire et de philosophie des sciences et des techniques, 13, rue du Four, Paris, 75006, France
AD  - Département de psychiatrie, centre hospitalier universitaire Grenoble-Alpes, avenue du Maquis-de-Grésivaudan, Grenoble, 38000, France
AD  - Precision psychiatry and social physiology laboratory, CHU de Sainte-Justine Research Center, department of psychiatry, university of Montreal, Quebec, Canada
AD  - Mila – Quebec artificial intelligence institute, university of Montreal, Quebec, Canada
AD  - Department of psychiatry, university hospital of Saint-Étienne, Saint-Étienne, France
AD  - INSERM, U1028; CNRS, UMR5292, Lyon neuroscience research center, psychiatric disorders: from resistance to response, PSYR2 Team, Lyon, France
AD  - University Lyon 1, Lyon, 69000, France
AD  - Lyon neuroscience research center, CRNL; INSERM, U1028; CNRS, UMR5292; brain dynamics and cognition team, Lyon, 69000, France
AD  - Service universitaire de médecine du sommeil (SUMS), CHU de Bordeaux, place Amélie-Raba-Léon, Bordeaux, 33076, France
AD  - USR CNRS 3413 SANPSY, CHU de Pellegrin, université de Bordeaux, Bordeaux, France
AB  - Introduction: Whether on the social, economic or scientific level, the digital sciences tend to change the conception of health. Computational Psychiatry, in the sense of a psychiatry based on “numbers” and information flow, has evolved rapidly. Methods: In this article, we propose the distinction between three fields of Computational Psychiatry. A first field corresponds to “Digital Psychiatry”, i.e. a field using digital, connected, tools in the main goal to collect digital data (especially important in this period of COVID-19). A second field corresponds to “Big Psychiatry”, or Big Data Psychiatry, which deals with large amounts of data, e.g. through recent methodologies in Machine learning or artificial intelligence. A third field corresponds to “Psychiatry Modeling”, which corresponds to the utilization of formal hypothesis (i.e. mathematical models) about brain and behavior (and their dysfunctions) in line with computational neurosciences. Results: The collection of digital data fits into methodologies of assessments and interventions in daily life, named Ecological Momentary Assessment. Of course, these digital data, which differ quantitatively and qualitatively from what psychiatry has been able to collect in its history, raise numerous epistemological and ethical questions. In the field of Big Psychiatry, most Machine learning techniques provide predictions rather than pathophysiological mechanisms, and these Machine learning techniques makes it possible to propose new delineations of disorders in a logic of stratified medicine. Lastly, resulting from studies in computational neurosciences, explanatory modeling of the brain (often called “Generative modeling”) proposes a number of theories to understand the functioning of the brain in psychiatric disorder (e.g. predictive coding, reinforcement learning, decision making theories, but also dynamical systems theories and graph and network theory). Discussion and conclusion: This field could offer a framework to characterize the origin of the psychiatric symptoms. Obviously, these three fields are highly mutually dependent, with for instance a data access provided by Digital Psychiatry (with Digital Tools), a data processing operated by Big Psychiatry (with Machine learning) and a formalization of hypotheses offered by Generative modeling of the brain from Psychiatry Modeling. This triple organization of Computational Psychiatry offers a robust framework for personalized and precision psychiatry, articulated around statistical and mathematical methodologies, focused on prediction and explanation and using qualitatively and quantitatively varied data. However, such a framework is necessarily geared to a common subject: the patient of the psychiatric clinic. © 2020 Elsevier Masson SAS
KW  - Artificial intelligence
KW  - Big data
KW  - Computational psychiatry
KW  - Digital tools
KW  - Machine learning
KW  - Neuroscience
KW  - Pathophysiology
KW  - Article
KW  - artificial intelligence
KW  - big data
KW  - brain function
KW  - clinical decision making
KW  - computational psychiatry
KW  - data processing
KW  - digital technology
KW  - ecological momentary assessment
KW  - epistemology
KW  - machine learning
KW  - mental disease
KW  - psychiatry
PB  - Elsevier Masson s.r.l.
SN  - 00034487 (ISSN)
LA  - English
J2  - Ann. Med.-Psychol.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 7; Correspondence Address: C. Gauld; Institut d'histoire et de philosophie des sciences et des techniques, Paris, 13, rue du Four, 75006, France; email: gauldchristophe@gmail.com; CODEN: AMPYA
ER  -

TY  - CONF
AU  - Hongvanthong, S.
TI  - Novel four-layered software defined 5g architecture for ai-based load balancing and QoS provisioning
PY  - 2020
T2  - 2020 5th International Conference on Computer and Communication Systems, ICCCS 2020
C7  - 9118463
SP  - 859
EP  - 863
DO  - 10.1109/ICCCS49078.2020.9118463
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087478961&doi=10.1109%2fICCCS49078.2020.9118463&partnerID=40&md5=c91d80e0df806f9a15133fe8080bbe37
AD  - Wuhan University of Technology, School of Computer Science, Wuhan, China
AB  - Software defined 5G network (SD-5G) is an evolving networking technology. The integration of SDN and 5G brings scalability, and efficiency. However, Quality of Service (QoS) provision is still challenging in SD-5G due to improper load balancing, traffic unawareness and so on. To overwhelm these issues this paper designs a novel load balancing scheme using Artificial Intelligence (AI) techniques. Firstly, novel four-layered SD-5G network is designed with user plane, smart data plane, load balancing plane, and distributed control plane. In the context to 5G, the data transmission rate must satisfy the QoS constraints based on the traffic type such as text, audio, video etc. Thus, the data from the user plane is classified by Smart Traffic Analyzer in the data plane. For traffic analysis, Enriched Neuro-Fuzzy (ENF) classifier is proposed. In the load balancing plane, Primary Load balancer and Secondary Load Balancer are deployed. This plane is responsible for balancing the load among controllers. For controller load balancing, switch migration is presented. Overloaded controller is predicted by Entropy function. Then decision for migration is made by Fitness-based Reinforcement Learning (F-RL) algorithm. Finally, the four-layered SD-5G network is modeled in the NS-3.26. The observations shows that the proposed work improves the SD-5G network in terms of Loss Rate, Packet Delivery Rate, Delay, and round trip time. © 2020 IEEE.
KW  - Artificial intelligence
KW  - Distributed control plane
KW  - QoS
KW  - Software defined 5G network
KW  - 5G mobile communication systems
KW  - Balancing
KW  - Closed loop control systems
KW  - Controllers
KW  - Distributed parameter control systems
KW  - Queueing networks
KW  - Reinforcement learning
KW  - Data transmission rates
KW  - Distributed control planes
KW  - Entropy function
KW  - Layered softwares
KW  - Load-balancing schemes
KW  - Networking technology
KW  - QOS provisioning
KW  - Traffic analysis
KW  - Quality of service
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-172816136-5 (ISBN)
LA  - English
J2  - Int. Conf. Comput. Commun. Syst., ICCCS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 3; Correspondence Address: S. Hongvanthong; Wuhan University of Technology, School of Computer Science, Wuhan, China; email: viet-24utd@windowslive.com; Conference name: 5th International Conference on Computer and Communication Systems, ICCCS 2020; Conference date: 15 May 2020 through 18 May 2020; Conference code: 161227
ER  -

TY  - JOUR
AU  - Abdi, A.H.
AU  - Sagl, B.
AU  - Srungarapu, V.P.
AU  - Stavness, I.
AU  - Prisman, E.
AU  - Abolmaesumi, P.
AU  - Fels, S.
TI  - Characterizing Motor Control of Mastication With Soft Actor-Critic
PY  - 2020
T2  - Frontiers in Human Neuroscience
VL  - 14
C7  - 188
DO  - 10.3389/fnhum.2020.00188
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086151237&doi=10.3389%2ffnhum.2020.00188&partnerID=40&md5=e9068d9421dad24e9c1feed79256812c
AD  - Electrical and Computer Engineering Department, University of British Columbia, Vancouver, BC, Canada
AD  - Department of Prosthodontics, University Clinic of Dentistry, Medical University of Vienna, Vienna, Austria
AD  - Department of Computer Science, University of Saskatchewan, Saskatoon, SK, Canada
AD  - Department of Surgery, University of British Columbia, Vancouver, BC, Canada
AB  - The human masticatory system is a complex functional unit characterized by a multitude of skeletal components, muscles, soft tissues, and teeth. Muscle activation dynamics cannot be directly measured on live human subjects due to ethical, safety, and accessibility limitations. Therefore, estimation of muscle activations and their resultant forces is a longstanding and active area of research. Reinforcement learning (RL) is an adaptive learning strategy which is inspired by the behavioral psychology and enables an agent to learn the dynamics of an unknown system via policy-driven explorations. The RL framework is a well-formulated closed-loop system where high capacity neural networks are trained with the feedback mechanism of rewards to learn relatively complex actuation patterns. In this work, we are building on a deep RL algorithm, known as the Soft Actor-Critic, to learn the inverse dynamics of a simulated masticatory system, i.e., learn the activation patterns that drive the jaw to its desired location. The outcome of the proposed training procedure is a parametric neural model which acts as the brain of the biomechanical system. We demonstrate the model's ability to navigate the feasible three-dimensional (3D) envelope of motion with sub-millimeter accuracies. We also introduce a performance analysis platform consisting of a set of quantitative metrics to assess the functionalities of a given simulated masticatory system. This platform assesses the range of motion, metabolic efficiency, the agility of motion, the symmetry of activations, and the accuracy of reaching the desired target positions. We demonstrate how the model learns more metabolically efficient policies by integrating a force regularization term in the RL reward. We also demonstrate the inverse correlation between the metabolic efficiency of the models and their agility and range of motion. The presented masticatory model and the proposed RL training mechanism are valuable tools for the analysis of mastication and other biomechanical systems. We see this framework's potential in facilitating the functional analyses aspects of surgical treatment planning and predicting the rehabilitation performance in post-operative subjects. © Copyright © 2020 Abdi, Sagl, Srungarapu, Stavness, Prisman, Abolmaesumi and Fels.
KW  - computational biomechanics
KW  - inverse dynamics
KW  - jaw
KW  - mastication modeling
KW  - motor control
KW  - musculoskeletal modeling
KW  - reinforcement learning
KW  - soft actor-critic
KW  - adult
KW  - agility
KW  - article
KW  - biomechanics
KW  - brain
KW  - controlled study
KW  - feedback system
KW  - human
KW  - jaw
KW  - mastication
KW  - motor control
KW  - quantitative analysis
KW  - range of motion
KW  - rehabilitation
KW  - reinforcement learning (machine learning)
KW  - reward
KW  - simulation
KW  - treatment planning
PB  - Frontiers Media S.A.
SN  - 16625161 (ISSN)
LA  - English
J2  - Front. Human Neurosci.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 4; Correspondence Address: A.H. Abdi; Electrical and Computer Engineering Department, University of British Columbia, Vancouver, Canada; email: amirabdi@ece.ubc.ca
ER  -

TY  - CONF
AU  - Chen, J.
AU  - Zuo, Y.
AU  - Zhang, D.
AU  - Qu, Z.
AU  - Wang, C.
TI  - Promoting constructive interaction and moral behaviors using adaptive empathetic learning
PY  - 2019
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 11740 LNAI
SP  - 3
EP  - 14
DO  - 10.1007/978-3-030-27526-6_1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070592259&doi=10.1007%2f978-3-030-27526-6_1&partnerID=40&md5=cc3dc915fe5f0598ca10b8095ba76984
AD  - Space Science and Inertial Technology Research Center, Harbin Institute of Technology, Harbin, 150001, China
AD  - Department of Biological Chemistry and Department of Neurobiology, University of California, Los Angeles, 90095, CA, United States
AB  - Moral system assists people with constructive interaction by maximizing the inner stimulus transfered from outer feelings. For this reason, building an intrinsic sense of morality is one potential way of regulating agents’ behaviors. Incorporating ideas found in social neuroscience, we hardwired a theoretical model of empathy in rational reinforcement learning-based agents to enable affective state sharing between agents. Our learning algorithm accounts for the impact of social comparison and companion impression, which play an important role on the update of empathy and make it possible for agents to change between cooperation and competition adaptively. Empathetic learners’ behavioral dynamics were tested and analyzed in multiple game settings. In iterated prisoner dilemma, empathetic agents showed increased cooperation in most cases except exhibiting self-protection awareness vigilantly when their partners were in the antagonistic state. Empathetic agents also showed a strong sense of fairness in the ultimatum game which resulted in an evenhanded allocation scheme on resources. © 2019, Springer Nature Switzerland AG.
KW  - Constructive interaction
KW  - Empathy
KW  - Multi-agent system
KW  - Reinforcement learning
KW  - Learning algorithms
KW  - Machine learning
KW  - Multi agent systems
KW  - Reinforcement learning
KW  - Robotics
KW  - Affective state
KW  - Behavioral dynamics
KW  - Constructive interaction
KW  - Cooperation and competitions
KW  - Empathy
KW  - Iterated prisoner dilemmas
KW  - Self protection
KW  - Theoretical modeling
KW  - Intelligent agents
A2  - Yu H.
A2  - Liu J.
A2  - Liu L.
A2  - Liu Y.
A2  - Ju Z.
A2  - Zhou D.
PB  - Springer Verlag
SN  - 03029743 (ISSN); 978-303027525-9 (ISBN)
LA  - English
J2  - Lect. Notes Comput. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: C. Wang; Space Science and Inertial Technology Research Center, Harbin Institute of Technology, Harbin, 150001, China; email: cwang@hit.edu.cn; Conference name: 12th International Conference on Intelligent Robotics and Applications, ICIRA 2019; Conference date: 8 August 2019 through 11 August 2019; Conference code: 229369
ER  -

TY  - JOUR
AU  - Petousis, P.
AU  - Winter, A.
AU  - Speier, W.
AU  - Aberle, D.R.
AU  - Hsu, W.
AU  - Bui, A.A.T.
TI  - Using sequential decision making to improve lung cancer screening performance
PY  - 2019
T2  - IEEE Access
VL  - 7
C7  - 2935763
SP  - 119403
EP  - 119419
DO  - 10.1109/ACCESS.2019.2935763
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087808861&doi=10.1109%2fACCESS.2019.2935763&partnerID=40&md5=89f1d50c6ba3e8b3a8d1357d82e68ad6
AD  - Ucla Bioengineering Department, Los Angeles, 90095, CA, United States
AD  - Department of Radiological Sciences, Ucla Medical and Imaging Informatics, Los Angeles, 90095, CA, United States
AB  - Globally, lung cancer is responsible for nearly one in five cancer deaths. The National Lung Screening Trial (NLST) demonstrated the efficacy of low-dose computed tomography (LDCT) to identify early-stage disease, setting the basis for widespread implementation of lung cancer screening programs. However, the specificity of LDCT lung cancer screening is suboptimal, with a significant false positive rate. Representing this imaging-based screening process as a sequential decision making problem, we combined multiple machine learning-based methods to learn a partially-observable Markov decision process that simultaneously optimizes lung cancer detection while enhancing test specificity. Using NLST data, we trained a dynamic Bayesian network as an observational model and used inverse reinforcement learning to discover a rewards function based on experts' decisions. Our resultant predictive model decreased the false positive rate while maintaining a high true positive rate at a level comparable to human experts. Our model also detected a number of lung cancers earlier. © 2020 Association for Computing Machinery. All rights reserved.
KW  - Dynamic Bayesian networks
KW  - Early disease prediction
KW  - Lung cancer screening
KW  - Partially observable Markov decision processes
KW  - QMDP algorithm
KW  - Bayesian networks
KW  - Behavioral research
KW  - Biological organs
KW  - Computerized tomography
KW  - Decision making
KW  - Diagnosis
KW  - Diseases
KW  - Learning systems
KW  - Markov processes
KW  - Predictive analytics
KW  - Dynamic Bayesian networks
KW  - False positive rates
KW  - Inverse reinforcement learning
KW  - Lung cancer detections
KW  - Lung cancer screening
KW  - Observational models
KW  - Partially observable Markov decision process
KW  - Sequential decision making
KW  - Reinforcement learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 21693536 (ISSN)
LA  - English
J2  - IEEE Access
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 33; Correspondence Address: P. PETOUSIS; Ucla Bioengineering Department, Los Angeles, 90095, United States; email: pp89@ucla.edu
ER  -

TY  - CONF
AU  - Noothigattu, R.
AU  - Bouneffouf, D.
AU  - Mattei, N.
AU  - Chandra, R.
AU  - Madan, P.
AU  - Varshney, K.R.
AU  - Campbell, M.
AU  - Singh, M.
AU  - Rossi, F.
TI  - Teaching AI agents ethical values using reinforcement learning and policy orchestration (extended abstract)
PY  - 2019
T2  - IJCAI International Joint Conference on Artificial Intelligence
VL  - 2019-August
SP  - 6377
EP  - 6381
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074909469&partnerID=40&md5=36ed448567d247212ba774da12950e6f
AD  - IBM Research, Yorktown Heights, NY, United States
AD  - IBM Research, Cambridge, MA, United States
AD  - Carnegie Mellon University, Pittsburgh, PA, United States
AD  - Tulane University, New Orleans, LA, United States
AB  - Autonomous cyber-physical agents play an increasingly large role in our lives. To ensure that they behave in ways aligned with the values of society, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspecified constraints from demonstrations and reinforcement learning to learn to maximize environmental rewards. A contextual bandit-based orchestrator then picks between the two policies: constraint-based and environment reward-based. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward-maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using Pac-Man and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways. © 2019 International Joint Conferences on Artificial Intelligence. All rights reserved.
A2  - Kraus S.
PB  - International Joint Conferences on Artificial Intelligence
SN  - 10450823 (ISSN); 978-099924114-1 (ISBN)
LA  - English
J2  - IJCAI Int. Joint Conf. Artif. Intell.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 12; Conference name: 28th International Joint Conference on Artificial Intelligence, IJCAI 2019; Conference date: 10 August 2019 through 16 August 2019; Conference code: 153611
ER  -

TY  - JOUR
AU  - Gutierrez-Franco, E.
AU  - Mejia-Argueta, C.
AU  - Rabelo, L.
TI  - Data-driven methodology to support long-lasting logistics and decision making for urban last-mile operations
PY  - 2021
T2  - Sustainability (Switzerland)
VL  - 13
IS  - 11
C7  - 6230
DO  - 10.3390/su13116230
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107990882&doi=10.3390%2fsu13116230&partnerID=40&md5=aa12b51bd93637850be01162551d001c
AD  - Center for Latin-American Logistics Innovation, Massachusetts Institute of Technology, Global SCALE Network, Cambridge, 02139, MA, United States
AD  - Department of Industrial Engineering and Management Systems, University of Central Florida, Orlando, 162993, FL, United States
AD  - Food and Retail Operations Lab, Center for Transportation and Logistics, Massachusetts Institute of Technology, Cambridge, 02139, MA, United States
AB  - Last-mile operations in forward and reverse logistics are responsible for a large part of the costs, emissions, and times in supply chains. These operations have increased due to the growth of electronic commerce and direct-to-consumer strategies. We propose a novel data-and model-driven framework to support decision making for urban distribution. The methodology is composed of diverse, hybrid, and complementary techniques integrated by a decision support system. This approach focuses on key elements of megacities such as socio-demographic diversity, portfolio mix, logistics fragmentation, high congestion factors, and dense commercial areas. The methodological framework will allow decision makers to create early warning systems and, with the implementation of optimization, machine learning, and simulation models together, make the best utilization of resources. The advantages of the system include flexibility in decision making, social welfare, increased productivity, and reductions in cost and environmental impacts. A real-world illustrative example is presented under conditions in one of the most congested cities: the megacity of Bogota, Colombia. Data come from a retail organization operating in the city. A network of stakeholders is analyzed to understand the complex urban distribution. The execution of the methodology was capable of solving a complex problem reducing the number of vehicles utilized, increasing the resource capacity utilization, and reducing the cost of operations of the fleet, meeting all constraints. These constraints included the window of operations and accomplishing the total number of deliveries. Furthermore, the methodology could accomplish the learning function using deep reinforcement learning in reasonable computational times. This preliminary analysis shows the potential benefits, especially in understudied metropolitan areas from emerging markets, supporting a more effective delivery process, and encouraging proactive, dynamic decision making during the execution stage. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - Customer-centric supply chains
KW  - Digital twin
KW  - Emerging markets
KW  - Framework
KW  - Hybrid methods
KW  - Nanostores
KW  - Prescriptive analytics
KW  - Urban logistics
PB  - MDPI AG
SN  - 20711050 (ISSN)
LA  - English
J2  - Sustainability
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 39; Correspondence Address: E. Gutierrez-Franco; Center for Latin-American Logistics Innovation, Massachusetts Institute of Technology, Global SCALE Network, Cambridge, 02139, United States; email: edfranco@mit.edu
ER  -

TY  - CONF
AU  - Naderializadeh, N.
AU  - Soleyman, S.
AU  - Hung, F.
AU  - Khosla, D.
AU  - Chen, Y.
AU  - Fadaie, J.G.
TI  - Distributed hierarchical reinforcement learning in multi-agent adversarial environments
PY  - 2022
T2  - Proceedings of SPIE - The International Society for Optical Engineering
VL  - 12113
C7  - 121130L
DO  - 10.1117/12.2616582
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146548036&doi=10.1117%2f12.2616582&partnerID=40&md5=084cbd8fe22aca91d313956cce353367
AD  - HRL Laboratories, LLC, 3011 Malibu Canyon Road, Malibu, 90265, CA, United States
AD  - University of Pennsylvania, 200 South 33rd Street, Philadelphia, PA, United States
AB  - We develop a hierarchical approach for controlling a team of aircraft in multi-agent adversarial environments. Each individual aircraft is equipped with a high-level agent that is solely responsible for target assignment decisions, and a low-level agent that generates actions based only on the selected target. We use distributed deep reinforcement learning to train the high-level agents, and neuroevolution to train the low-level agents. This approach leverages centralized training for decentralized execution to enable individual autonomy when communication is limited. Simulation results confirm the superiority of our proposed approach as compared to non-hierarchical multi-agent reinforcement learning methods. © 2022 SPIE.
KW  - adversarial
KW  - artificial intelligence
KW  - decentralized
KW  - distributed
KW  - machine learning
KW  - multi-agent
KW  - neuroevolution
KW  - reinforcement learning
KW  - Aircraft
KW  - Deep learning
KW  - Learning systems
KW  - Multi agent systems
KW  - Adversarial
KW  - Adversarial environments
KW  - Decentralised
KW  - Distributed
KW  - Hierarchical approach
KW  - Hierarchical reinforcement learning
KW  - Machine-learning
KW  - Multi agent
KW  - Neuro evolutions
KW  - Reinforcement learnings
KW  - Reinforcement learning
A2  - Pham T.
A2  - Solomon L.
PB  - SPIE
SN  - 0277786X (ISSN); 978-151065102-9 (ISBN)
LA  - English
J2  - Proc SPIE Int Soc Opt Eng
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications IV 2022; Conference date: 6 June 2022 through 12 June 2022; Conference code: 185857; CODEN: PSISD
ER  -

TY  - CONF
AU  - Biswas, D.
TI  - Reinforcement Learning based HVAC Optimization in Factories
PY  - 2020
T2  - e-Energy 2020 - Proceedings of the 11th ACM International Conference on Future Energy Systems
SP  - 428
EP  - 433
DO  - 10.1145/3396851.3402363
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088510483&doi=10.1145%2f3396851.3402363&partnerID=40&md5=47009493315d4d5e9331daf54bb629e7
AD  - Philip Morris Products S.A., Lausanne, Switzerland
AB  - Heating, Ventilation and Air Conditioning (HVAC) units are responsible for maintaining the temperature and humidity settings in a building. Studies have shown that HVAC accounts for almost 50% energy consumption in a building and 10% of global electricity usage. HVAC optimization thus has the potential to contribute significantly towards our sustainability goals, reducing energy consumption and CO2 emissions. In this work, we explore ways to optimize the HVAC controls in factories. Unfortunately, this is a complex problem as it requires computing an optimal state considering multiple variable factors, e.g. the occupancy, manufacturing schedule, temperature requirements of operating machines, air flow dynamics within the building, external weather conditions, energy savings, etc. We present a Reinforcement Learning (RL) based energy optimization model that has been applied in our factories. We show that RL is a good fit as it is able to learn and adapt to multi-parameterized system dynamics in real-time. It provides around 25% energy savings on top of the previously used Proportional-Integral-Derivative (PID) controllers. © 2020 ACM.
KW  - Energy Optimization
KW  - HVAC
KW  - Machine Learning
KW  - Reinforcement Learning
KW  - Sustainability
KW  - Air conditioning
KW  - Energy conservation
KW  - Energy utilization
KW  - HVAC
KW  - Proportional control systems
KW  - Reinforcement learning
KW  - Two term control systems
KW  - Complex problems
KW  - Electricity usage
KW  - Energy optimization
KW  - Parameterized system
KW  - Proportional integral derivative controllers
KW  - Reducing energy consumption
KW  - Temperature and humidities
KW  - Variable factors
KW  - Smart power grids
PB  - Association for Computing Machinery, Inc
SN  - 978-145038009-6 (ISBN)
LA  - English
J2  - e-Energy - Proc. ACM Int. Conf. Future Energy Syst.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 4; Correspondence Address: D. Biswas; Philip Morris Products S.A., Lausanne, Switzerland; email: debmalya.biswas@pmi.com; Conference name: 11th ACM International Conference on Future Energy Systems, e-Energy 2020; Conference date: 22 June 2020 through 26 June 2020; Conference code: 161242
ER  -

TY  - JOUR
AU  - Noothigattu, R.
AU  - Bouneffouf, D.
AU  - Mattei, N.
AU  - Chandra, R.
AU  - Madan, P.
AU  - Varshney, K.R.
AU  - Campbell, M.
AU  - Singh, M.
AU  - Rossi, F.
TI  - Teaching AI agents ethical values using reinforcement learning and policy orchestration
PY  - 2019
T2  - IBM Journal of Research and Development
VL  - 63
IS  - 4-5
C7  - 8827920
DO  - 10.1147/JRD.2019.2940428
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074918441&doi=10.1147%2fJRD.2019.2940428&partnerID=40&md5=5d489dd9ae188985211978aa0b8dff6f
AD  - IBM Research, Cambridge, 02142, MA, United States
AB  - Autonomous cyber-physical agents play an increasingly large role in our lives. To ensure that they behave in ways aligned with the values of society, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspecified constraints from demonstrations and reinforcement learning to learn to maximize environmental rewards. A contextual-bandit-based orchestrator then picks between the two policies: constraint-based and environment reward-based. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward-maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using Pac-Man and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways. © 1957-2012 IBM.
KW  - Machine learning
KW  - Reinforcement learning
KW  - Constraint-based
KW  - Contextual bandits
KW  - Cyber physicals
KW  - Ethical values
KW  - Implicit constraints
KW  - Inverse reinforcement learning
KW  - Pac-man
KW  - Time step
KW  - Autonomous agents
PB  - IBM Corporation
SN  - 00188646 (ISSN)
LA  - English
J2  - IBM J. Res. Dev.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 30; CODEN: IBMJA
ER  -

TY  - JOUR
AU  - Eyni, A.
AU  - Skardi, M.J.E.
AU  - Kerachian, R.
TI  - A regret-based behavioral model for shared water resources management: Application of the correlated equilibrium concept
PY  - 2021
T2  - Science of the Total Environment
VL  - 759
C7  - 143892
DO  - 10.1016/j.scitotenv.2020.143892
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097795233&doi=10.1016%2fj.scitotenv.2020.143892&partnerID=40&md5=dc52a6179f9ae7d87b183624c839bfd6
AD  - School of Civil Engineering, College of Engineering, University of Tehran, Tehran, Iran
AB  - The competition over water use in shared water resources systems may lead to conflict. Conflict can lead to strategic behaviors with the consequence of “Tragedy of Common” in water resources. In this paper, a novel approach is proposed for the quantity and quality management of shared water resources using the Correlated Equilibrium (CE) concept. For the first time in water resources management studies, a Reinforcement Learning (RL)-based method, namely Regret Matching (RM), is proposed to simulate agents' behaviors. In the proposed methodology, an agent, which is responsible for water allocations, tries to reduce illegal water withdrawal from resources, using some non-mandatory and mandatory suggestions. This agent's objectives are leading the system towards social optimality (SO) and reaching the environmental sustainability goal. A modified RM algorithm is also developed for behavioral simulation in urban areas. The proposed methodology's applicability and efficiency are evaluated considering some criteria such as the concentration of the nitrate pollutant in groundwater, the groundwater table fluctuations, the rate of illegal water extraction from the groundwater, and the stakeholders' general satisfaction. The results of applying the methodology to the western part of the Tehran metropolitan area show its ability to deal with the water and treated wastewater allocation problems in urban areas and increase in the learning and cooperation among agents. According to the results, a meaningful decrease in nitrate concentration in the aquifer and an increase in groundwater table levels are observed. The results also indicate that the model could teach the stakeholders to act more responsibly towards protecting the environment and conserving shared water resources. © 2020 Elsevier B.V.
KW  - Correlated Equilibrium (CE)
KW  - Game Theory (GT)
KW  - Regret Matching (RM)
KW  - Reinforcement Learning (RL)
KW  - Social Optimality (SO)
KW  - Water resources management
KW  - Aquifers
KW  - Crime
KW  - Environmental protection
KW  - Groundwater pollution
KW  - Groundwater resources
KW  - Learning systems
KW  - Nitrates
KW  - Quality management
KW  - Reinforcement learning
KW  - Sustainable development
KW  - Wastewater treatment
KW  - Water supply
KW  - ground water
KW  - Allocation problems
KW  - Behavioral simulation
KW  - Correlated equilibria
KW  - Environmental sustainability
KW  - Nitrate concentration
KW  - Strategic Behavior
KW  - Water resources management
KW  - Water resources systems
KW  - algorithm
KW  - conceptual framework
KW  - game theory
KW  - machine learning
KW  - numerical model
KW  - qualitative analysis
KW  - quantitative analysis
KW  - water management
KW  - water resource
KW  - Article
KW  - artificial neural network
KW  - concentration (parameter)
KW  - correlated equilibrium concept
KW  - environmental monitoring
KW  - environmental policy
KW  - environmental sustainability
KW  - geography
KW  - Iran
KW  - mathematical computing
KW  - mathematical model
KW  - mathematical phenomena
KW  - performance
KW  - priority journal
KW  - regret matching
KW  - reinforcement learning (machine learning)
KW  - urban area
KW  - waste water management
KW  - water management
KW  - water pollution
KW  - water supply
KW  - Water treatment
PB  - Elsevier B.V.
SN  - 00489697 (ISSN)
C2  - 33340861
LA  - English
J2  - Sci. Total Environ.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 14; Correspondence Address: R. Kerachian; School of Civil Engineering, College of Engineering, University of Tehran, Tehran, Iran; email: kerachian@ut.ac.ir; CODEN: STEVA
ER  -

TY  - CONF
AU  - Aoudia, F.A.
AU  - Gautier, M.
AU  - Berder, O.
TI  - RLMan: An Energy Manager Based on Reinforcement Learning for Energy Harvesting Wireless Sensor Networks
PY  - 2018
T2  - IEEE Transactions on Green Communications and Networking
VL  - 2
IS  - 2
SP  - 408
EP  - 417
DO  - 10.1109/TGCN.2018.2801725
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061389718&doi=10.1109%2fTGCN.2018.2801725&partnerID=40&md5=5eb18954b52a8ce514a16a01721e87bd
AD  - CNRS, IRISA, University of Rennes, Rennes, 35042, France
AB  - A promising solution for achieving autonomous wireless sensor networks is to enable each node to harvest energy in its environment. To address the time-varying behavior of energy sources, each node embeds an energy manager responsible for dynamically adapting the power consumption of the node in order to maximize the quality of service while avoiding power failures. A novel energy management algorithm based on reinforcement learning (RLMan) is proposed in this paper. By continuously exploring the environment, RLMan adapts its energy management policy to time-varying environment, regarding both the harvested energy and the energy consumption of the node. Linear function approximations are used to achieve very low computational and memory footprint, making RLMan suitable for resource-constrained systems, such as wireless sensor nodes. Moreover, RLMan only requires the state of charge of the energy storage device to operate, which makes it practical to implement. Exhaustive simulations using real measurements of indoor light and outdoor wind show that RLMan outperforms current state-of-the-art approaches, by enabling almost 70% gain regarding the average packet rate. Moreover, RLMan is more robust to variability of the node energy consumption. © 2018 IEEE.
KW  - autonomous systems
KW  - energy harvesting
KW  - energy management
KW  - learning systems
KW  - Wireless sensor networks
KW  - Energy harvesting
KW  - Energy management
KW  - Energy policy
KW  - Energy storage
KW  - Energy utilization
KW  - Managers
KW  - Quality of service
KW  - Reinforcement learning
KW  - Wireless sensor networks
KW  - Energy management algorithms
KW  - Exhaustive simulation
KW  - Learning (artificial intelligence)
KW  - State of charge
KW  - State-of-the-art approach
KW  - Time varying behavior
KW  - Time-varying environments
KW  - Wireless sensor node
KW  - Sensor nodes
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 24732400 (ISSN)
LA  - English
J2  - IEEE Trans. Green Commun. Networking
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 89; Correspondence Address: F.A. Aoudia; CNRS, IRISA, University of Rennes, Rennes, 35042, France; email: faycal.ait-aoudia@irisa.fr
ER  -

TY  - CONF
AU  - Sun, F.-Y.
AU  - Chang, Y.-Y.
AU  - Wu, Y.-H.
AU  - Lin, S.-D.
TI  - Designing Non-greedy Reinforcement Learning Agents with Diminishing Reward Shaping
PY  - 2018
T2  - AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society
SP  - 297
EP  - 302
DO  - 10.1145/3278721.3278759
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061024133&doi=10.1145%2f3278721.3278759&partnerID=40&md5=ccdd4bb2f9ac1501dbb921f911cddd8a
AD  - National Taiwan University, Taiwan
AB  - This paper intends to address an issue in RL that when agents possessing varying capabilities, most resources may be acquired by stronger agents, leaving the weaker ones "starving". We introduce a simple method to train non-greedy agents in multi-agent reinforcement learning scenarios with nearly no extra cost. Our model can achieve the following goals in designing the non-greedy agent:non-homogeneous equality, only need local information, cost-effective, generalizable and configurable. We propose the idea of diminishing reward that makes the agent feel less satisfied for consecutive rewards obtained. This idea allows the agents to behave less greedy with-out the need to explicitly coding any ethical pattern nor monitor other agents' status. Given our framework, resources distributed more equally without running the risk of reaching homogeneous equality. We designed two games, Gathering Game and Hunter Prey to evaluate the quality of the model. © 2018 ACM.
KW  - multi-agent reinforcement learning
KW  - non-greedy
KW  - reward shaping
KW  - Cost effectiveness
KW  - Intelligent agents
KW  - Machine learning
KW  - Multi agent systems
KW  - Philosophical aspects
KW  - Cost effective
KW  - Local information
KW  - Multi-agent reinforcement learning
KW  - non-greedy
KW  - Non-homogeneous
KW  - Reinforcement learning agent
KW  - reward shaping
KW  - SIMPLE method
KW  - Reinforcement learning
PB  - Association for Computing Machinery, Inc
SN  - 978-145036012-8 (ISBN)
LA  - English
J2  - AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 9; Conference name: 1st AAAI/ACM Conference on AI, Ethics, and Society, AIES 2018; Conference date: 2 February 2018 through 3 February 2018; Conference code: 144126
ER  -

TY  - JOUR
AU  - De Paola, A.
AU  - Gaglio, S.
AU  - Giammanco, A.
AU  - Lo Re, G.
AU  - Morana, M.
TI  - A multi-agent system for itinerary suggestion in smart environments
PY  - 2021
T2  - CAAI Transactions on Intelligence Technology
VL  - 6
IS  - 4
SP  - 377
EP  - 393
DO  - 10.1049/cit2.12056
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114316745&doi=10.1049%2fcit2.12056&partnerID=40&md5=38a96cfefe937631bd8c6b5deef1a721
AD  - Università degli studi di Palermo, Palermo, Italy
AD  - Smart Cities and Communities National Lab CINI - Consorzio Interuniversitario Nazionale per l’Informatica, Palermo, Italy
AB  - Modern smart environments pose several challenges, among which the design of intelligent algorithms aimed to assist the users. When a variety of points of interest are available, for instance, trajectory recommendations are needed to suggest users the most suitable itineraries based on their interests and contextual constraints. Unfortunately, in many cases, these interests must be explicitly requested and their lack causes the so-called cold-start problem. Moreover, lengthy travelling distances and excessive crowdedness of specific points of interest make itinerary planning more difficult. To address these aspects, a multi-agent itinerary suggestion system that aims at assisting the users in an online and collaborative way is proposed. A profiling agent is responsible for the detection of groups of users whose movements are characterised by similar semantic, spatial and temporal features; then, a recommendation agent leverages contextual information and dynamically associates the current user with the trajectory clusters according to a Multi-Armed Bandit policy. Framing the trajectory recommendation as a reinforcement learning problem permits to provide high-quality suggestions while avoiding both cold-start and preference elicitation issues. The effectiveness of the approach is demonstrated by some deployments in real-life scenarios, such as smart campuses and theme parks. © 2021 The Authors. CAAI Transactions on Intelligence Technology published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology and Chongqing University of Technology.
KW  - artificial intelligence
KW  - pattern recognition
KW  - Multi agent systems
KW  - Reinforcement learning
KW  - Semantics
KW  - Trajectories
KW  - Cold start problems
KW  - Contextual constraints
KW  - Contextual information
KW  - Intelligent Algorithms
KW  - Itinerary planning
KW  - Multi armed bandit
KW  - Preference elicitation
KW  - Recommendation agents
KW  - Intelligent agents
PB  - John Wiley and Sons Inc
SN  - 24686557 (ISSN)
LA  - English
J2  - CAAI Trans. Intell. Technol.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2; Correspondence Address: M. Morana; Università degli studi di Palermo, Palermo, Italy; email: marco.morana@unipa.it
ER  -

TY  - CONF
AU  - Myhre, J.N.
AU  - Launonen, I.K.
AU  - Wei, S.
AU  - Godtliebsen, F.
TI  - Controlling blood glucose levels in patients with type 1 diabetes using fitted q-iterations and functional features
PY  - 2018
T2  - IEEE International Workshop on Machine Learning for Signal Processing, MLSP
VL  - 2018-September
C7  - 8516946
DO  - 10.1109/MLSP.2018.8516946
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056995574&doi=10.1109%2fMLSP.2018.8516946&partnerID=40&md5=cb2fd85f91c6a8f8eb6e18dc4452b172
AD  - Department of Mathematics and Statistics, Arctic University of Norway, Norway
AD  - School of Mathematics and Statistics, University of Melbourne, Australia
AB  - Type 1 Diabetes is characterized by the lack of insulin-producing beta cells in the pancreas. The artificial pancreas promises to alleviate the burdens of self-management. While the physical components of the system - the continuous glucose monitor and insulin pump - have experienced rapid advances, a technological bottleneck remains in the control algorithm, which is responsible for translating data from the former into instructions for the latter. In this work, we propose to bring machine learning techniques to bear upon the challenges of blood glucose control. Specifically, we employ reinforcement learning to learn an optimal insulin policy. Learning is generalized using nonparametric regression with functional features, exploiting information contained in the shape of the glucose curve. Our algorithm is model-free, data-driven and personalized. In-silico simulations with T1D models demonstrate the potential of the proposed algorithm. © 2018 IEEE.
KW  - Biomedical engineering
KW  - Diabetes
KW  - Nonparametric regression
KW  - Reinforcement learning
KW  - Artificial intelligence
KW  - Artificial organs
KW  - Biomedical engineering
KW  - Blood
KW  - Glucose
KW  - Insulin
KW  - Medical problems
KW  - Signal processing
KW  - Artificial pancreas
KW  - Blood glucose controls
KW  - Blood glucose level
KW  - Functional features
KW  - Glucose monitors
KW  - Machine learning techniques
KW  - Non-parametric regression
KW  - Physical components
KW  - Reinforcement learning
A2  - Pustelnik N.
A2  - Tan Z.-H.
A2  - Ma Z.
A2  - Larsen J.
PB  - IEEE Computer Society
SN  - 21610363 (ISSN); 978-153865477-4 (ISBN)
LA  - English
J2  - IEEE Int. Workshop Mach. Learn. Signal Process., MLSP
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 10; Conference name: 28th IEEE International Workshop on Machine Learning for Signal Processing, MLSP 2018; Conference date: 17 September 2018 through 20 September 2018; Conference code: 141793
ER  -

TY  - JOUR
AU  - Livingston, S.
AU  - Risse, M.
TI  - The Future Impact of Artificial Intelligence on Humans and Human Rights
PY  - 2019
T2  - Ethics and International Affairs
VL  - 33
IS  - 2
SP  - 141
EP  - 158
DO  - 10.1017/S089267941900011X
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066972810&doi=10.1017%2fS089267941900011X&partnerID=40&md5=ebf44c4591e736704d75bcc2642ce82f
AD  - Department of Media and Public Affairs and of International Affairs, George Washington University, United States
AD  - Carr Center for Human Rights Policy, John F. Kennedy School of Government, Harvard University, United States
AB  - What are the implications of artificial intelligence (AI) on human rights in the next three decades? Precise answers to this question are made difficult by the rapid rate of innovation in AI research and by the effects of human practices on the adaption of new technologies. Precise answers are also challenged by imprecise usages of the term "AI." There are several types of research that all fall under this general term. We begin by clarifying what we mean by AI. Most of our attention is then focused on the implications of artificial general intelligence (AGI), which entail that an algorithm or group of algorithms will achieve something like superintelligence. While acknowledging that the feasibility of superintelligence is contested, we consider the moral and ethical implications of such a potential development. What do machines owe humans and what do humans owe superintelligent machines? © 2019 Carnegie Council for Ethics in International Affairs.
KW  - artificial general intelligence
KW  - artificial intelligence
KW  - categorical imperative
KW  - deep learning
KW  - ethical impact agents
KW  - human rights
KW  - implicit ethical agents
KW  - machine learning
KW  - reinforcement learning
KW  - superintelligence
PB  - Cambridge University Press
SN  - 08926794 (ISSN)
LA  - English
J2  - Ethics Int. Aff.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 35
ER  -

TY  - JOUR
AU  - Heuillet, A.
AU  - Couthouis, F.
AU  - Díaz-Rodríguez, N.
TI  - Explainability in deep reinforcement learning
PY  - 2021
T2  - Knowledge-Based Systems
VL  - 214
C7  - 106685
DO  - 10.1016/j.knosys.2020.106685
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100111156&doi=10.1016%2fj.knosys.2020.106685&partnerID=40&md5=f85ac4aae1f027113b0f4b5743c9e0b7
AD  - ENSEIRB-MATMECA, Bordeaux INP, 1 avenue du Docteur Albert Schweitzer, Talence, 33400, France
AD  - ENSC, Bordeaux INP, 109 avenue Roul, Talence, 33400, France
AD  - ENSTA Paris, Institut Polytechnique Paris, Inria Flowers Team, 828 boulevard des Maréchaux, Palaiseau, 91762, France
AB  - A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent's behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainability. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems. © 2020 Elsevier B.V.
KW  - Deep Learning
KW  - Explainable artificial intelligence
KW  - Machine Learning
KW  - Reinforcement Learning
KW  - Representation learning
KW  - Responsible artificial intelligence
KW  - Deep neural networks
KW  - Reinforcement learning
KW  - AND splits
KW  - Black boxes
KW  - Classification tasks
KW  - Feature relevance
KW  - General publics
KW  - Image source
KW  - Interpretability
KW  - R-l models
KW  - Deep learning
PB  - Elsevier B.V.
SN  - 09507051 (ISSN)
LA  - English
J2  - Knowl Based Syst
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 159; Correspondence Address: N. Díaz-Rodríguez; ENSTA Paris, Institut Polytechnique Paris, Inria Flowers Team, Palaiseau, 828 boulevard des Maréchaux, 91762, France; email: natalia.diaz@ensta-paris.fr; CODEN: KNSYE
ER  -

TY  - CONF
AU  - Al, W.A.
AU  - Yun, I.D.
TI  - Actor-Critic Reinforcement Learning for Automatic Left Atrial Appendage Segmentation
PY  - 2019
T2  - Proceedings - 2018 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2018
C7  - 8621575
SP  - 609
EP  - 612
DO  - 10.1109/BIBM.2018.8621575
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062487073&doi=10.1109%2fBIBM.2018.8621575&partnerID=40&md5=939a50e2ced0eba9f72ec8c2396bc813
AD  - Dept. of Computer and Electronic Systems Engineering, Hankuk University of Foreign Studies, Yongin, South Korea
AB  - Left atrial appendage (LAA) is a major thrombus formation site, potentially responsible for atrial fibrillation (AF)associated stroke. In analyzing the risk factor of the AF-patients, diagnosing the LAA anatomy plays a significant role. Therefore, an automatic segmentation of the LAA can facilitate an accelerated AF diagnosis. It can also help physicians in preprocedural planning of LAA closure, which is an implant-based strategy to prevent thromboembolism in LAA. However, the high anatomic variation of the LAA, and leaking through the adjacent left superior pulmonary vein yield major challenges in LAA segmentation. With some prior works generally relying on a manual annotation of a bounding box, fully automated segmentation approach is rare to be found. In this paper, we propose a fully automatic LAA segmentation method powered by an actor-critic reinforcement learning agent where the agent proposes necessary segmentation seeds to perform a geodesic distance-based segmentation. The proposed method could resolve all the major challenges of LAA segmentation. To the best of our knowledge, this is the first automated method for LAA segmentation. Compared to the previous methods, it performs the segmentation with a significantly greater efficiency, taking only 7.6 seconds. © 2018 IEEE.
KW  - Actor-critic
KW  - automatic
KW  - left atrial appendage
KW  - reinforcement learning
KW  - segmentation
KW  - Bioinformatics
KW  - Diagnosis
KW  - Image segmentation
KW  - Machine learning
KW  - Actor critic
KW  - Actor-critic reinforcement learning
KW  - Atrial fibrillation
KW  - automatic
KW  - Automatic segmentations
KW  - left atrial appendage
KW  - Preprocedural planning
KW  - Segmentation methods
KW  - Reinforcement learning
A2  - Schmidt H.
A2  - Griol D.
A2  - Wang H.
A2  - Baumbach J.
A2  - Zheng H.
A2  - Callejas Z.
A2  - Hu X.
A2  - Dickerson J.
A2  - Zhang L.
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-153865488-0 (ISBN)
LA  - English
J2  - Proc. - IEEE Int. Conf. Bioinform. Biomed., BIBM
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 4; Conference name: 2018 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2018; Conference date: 3 December 2018 through 6 December 2018; Conference code: 144524
ER  -

TY  - CONF
AU  - Qiao, R.
AU  - Yan, S.
AU  - Shen, B.
TI  - A Reinforcement Learning Solution to Cold-Start Problem in Software Crowdsourcing Recommendations
PY  - 2018
T2  - Proceedings of the 2018 IEEE International Conference on Progress in Informatics and Computing, PIC 2018
C7  - 8706279
SP  - 8
EP  - 14
DO  - 10.1109/PIC.2018.8706279
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065901447&doi=10.1109%2fPIC.2018.8706279&partnerID=40&md5=ab6556f7b67c80ee618b0874c224f381
AD  - School of Software, Shanghai Jiao Tong University, China
AB  - Recommendation is one key functionality of software crowdsourcing platforms, which is responsible for recommending developers appropriate software projects, or vice versa. Meanwhile, software crowdsourcing recommendation in practice usually faces a cold-start problem: a platform has not yet gathered sufficient information, and thus its recommendations can be imprecise or unbalanced.To tackle this problem, this paper introduces reinforcement learning into crowdsourcing recommendations, and presents ClusterUCBscRec, a novel project recommending approach to learn user feedbacks actively. ClusterUCBscRec adopts the "explore exploit" strategy to improve the recommending performance continuously, and therefore goes quickly through the cold-start stage. Besides the project models, developer models built from multiple aspects, including developer profile, preferences and skills are introduced into recommendation. Developers and projects are clustered to speed up training and recommending processes to further improve the performance. We have evaluated ClusterUCBscRec on Jointforce. Experimental results show that the novel approach significantly improves the performance of crowdsourcing recommendations and can solve the cold-start problem effectively, compared with COFIBA and BiUCB. © 2018 IEEE.
KW  - cold start problem
KW  - Interactive learning
KW  - reinforcement learning
KW  - software crowdsourcing recommendation
KW  - Crowdsourcing
KW  - Machine learning
KW  - Cold start
KW  - Cold start problems
KW  - Crowdsourcing platforms
KW  - Interactive learning
KW  - Project model
KW  - Reinforcement learning solution
KW  - Software project
KW  - User feedback
KW  - Reinforcement learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-153867670-7 (ISBN)
LA  - English
J2  - Proc. IEEE Int. Conf. Prog. Inf. Comput., PIC
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 7; Correspondence Address: B. Shen; School of Software, Shanghai Jiao Tong University, China; email: bjshen@sjtu.edu.cn; Conference name: 6th IEEE International Conference on Progress in Informatics and Computing, PIC 2018; Conference date: 14 December 2018 through 16 December 2018; Conference code: 147823
ER  -

TY  - CONF
AU  - Böther, M.
AU  - Kißig, O.
AU  - Taraz, M.
AU  - Cohen, S.
AU  - Seidel, K.
AU  - Friedrich, T.
TI  - WHAT'S WRONG WITH DEEP LEARNING IN TREE SEARCH FOR COMBINATORIAL OPTIMIZATION
PY  - 2022
T2  - ICLR 2022 - 10th International Conference on Learning Representations
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146300109&partnerID=40&md5=cec0379e6db128431d1f37b94c3b5e23
AD  - Hasso Plattner Institute, University of Potsdam, Germany
AD  - The Academic College of Tel Aviv-Yaffo, Israel
AD  - Department of Mathematics, University of Potsdam, Germany
AB  - Combinatorial optimization lies at the core of many real-world problems. Especially since the rise of graph neural networks (GNNs), the deep learning community has been developing solvers that derive solutions to NP-hard problems by learning the problem-specific solution structure. However, reproducing the results of these publications proves to be difficult. We make three contributions. First, we present an open-source benchmark suite for the NP-hard MAXIMUM INDEPENDENT SET problem, in both its weighted and unweighted variants. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. Second, using our benchmark suite, we conduct an in-depth analysis of the popular guided tree search algorithm by Li et al. [NeurIPS 2018], testing various configurations on small and large synthetic and real-world graphs. By re-implementing their algorithm with a focus on code quality and extensibility, we show that the graph convolution network used in the tree search does not learn a meaningful representation of the solution structure, and can in fact be replaced by random values. Instead, the tree search relies on algorithmic techniques like graph kernelization to find good solutions. Thus, the results from the original publication are not reproducible. Third, we extend the analysis to compare the tree search implementations to other solvers, showing that the classical algorithmic solvers often are faster, while providing solutions of similar quality. Additionally, we analyze a recent solver based on reinforcement learning and observe that for this solver, the GNN is responsible for the competitive solution quality. © 2022 ICLR 2022 - 10th International Conference on Learning Representationss. All rights reserved.
KW  - Computational complexity
KW  - Deep neural networks
KW  - Graph neural networks
KW  - Interface states
KW  - Quality control
KW  - Reinforcement learning
KW  - Trees (mathematics)
KW  - Benchmark suites
KW  - Graph neural networks
KW  - Learning community
KW  - NP-hard
KW  - Open-source
KW  - Real-world problem
KW  - Solution structures
KW  - State of the art
KW  - Traditional learning
KW  - Tree-search
KW  - Combinatorial optimization
PB  - International Conference on Learning Representations, ICLR
LA  - English
J2  - ICLR - Int. Conf. Learn. Represent.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 10; Correspondence Address: M. Böther; Hasso Plattner Institute, University of Potsdam, Germany; email: maxi@boether.de; Conference name: 10th International Conference on Learning Representations, ICLR 2022; Conference date: 25 April 2022 through 29 April 2022; Conference code: 186704
ER  -

TY  - CONF
AU  - Zhang, L.
AU  - Lv, X.
AU  - Dhakal, S.
TI  - A Reinforcement Learning-Based Stakeholder Value Aggregation Model for Collaborative Decision Making on Disaster Resilience
PY  - 2019
T2  - Computing in Civil Engineering 2019: Smart Cities, Sustainability, and Resilience - Selected Papers from the ASCE International Conference on Computing in Civil Engineering 2019
SP  - 490
EP  - 497
DO  - 10.1061/9780784482445.063
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068795822&doi=10.1061%2f9780784482445.063&partnerID=40&md5=c63d4e81ef873c8e4df1ccaa370ec70a
AD  - Moss School of Construction, Infrastructure, and Sustainability, Florida International Univ., 10555 West Flagler St., EC 2935, Miami, 33174, FL, United States
AD  - Dept. of Civil and Environmental Engineering, Florida International Univ., 10555 West Flagler St., Miami, 33174, FL, United States
AB  - Building a disaster resilient community is a shared responsibility among multi-sector stakeholders, including government officials, private sectors, non-government organizations (NGOs), and community residents. Different stakeholders have different concerns and needs, and they make different decisions regarding the priorities of implementing disaster resilience strategies given limited resources. These differences are deeply rooted in their value systems, which are defined as ranked systems of things that are of importance, merit, and utility to the stakeholders. The differences in their value system cause conflicts and disputes for disaster-related decision making, which may result in longer decision-making time and huge financial losses. Therefore, there is a need to understand and systemically integrate the value systems of multi-sector stakeholders to support more effective and collaborative decision making in disaster resilience. To address such need, this paper proposes a stakeholder value aggregation (SVA) model to systematically and quantitatively aggregate divergent stakeholder value systems by leveraging a reinforcement learning-based method. The proposed SVA model was illustrated using a case study that focuses on aggregating the value systems of responsible and impacted stakeholders. The paper also discusses our proposed framework for a cloud-based decision support system that allows for automatic aggregation of divergent stakeholder value systems. The model, together with its potential application of cloud-based decision support system will facilitate more collaborative and efficient decision making by identifying the collective value priorities on disaster resilience practices. © 2019 American Society of Civil Engineers.
KW  - Decision support systems
KW  - Disasters
KW  - Losses
KW  - Machine learning
KW  - Reinforcement learning
KW  - Smart city
KW  - Sustainable development
KW  - Automatic aggregations
KW  - Collaborative decision making
KW  - Disaster resiliences
KW  - Government officials
KW  - Non government organizations
KW  - Private sectors
KW  - Shared responsibility
KW  - Stakeholder values
KW  - Decision making
A2  - Cho Y.K.
A2  - Leite F.
A2  - Behzadan A.
A2  - Wang C.
PB  - American Society of Civil Engineers (ASCE)
SN  - 978-078448244-5 (ISBN)
LA  - English
J2  - Comput. Civ. Eng.: Smart Cities, Sustain., Resil. - Sel. Pap. ASCE Int. Conf. Comput. Civ. Eng.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 10; Conference name: ASCE International Conference on Computing in Civil Engineering 2019: Smart Cities, Sustainability, and Resilience, i3CE 2019; Conference date: 17 June 2019 through 19 June 2019; Conference code: 148902
ER  -

TY  - CONF
AU  - Hendrycks, D.
AU  - Burns, C.
AU  - Basart, S.
AU  - Critch, A.
AU  - Li, J.
AU  - Song, D.
AU  - Steinhardt, J.
TI  - ALIGNING AI WITH SHARED HUMAN VALUES
PY  - 2021
T2  - ICLR 2021 - 9th International Conference on Learning Representations
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127117185&partnerID=40&md5=2b58c82401e52cac4f0949354d21a44f
AD  - UC Berkeley, United States
AD  - Columbia University, United States
AD  - UChicago, United States
AD  - Microsoft
AB  - We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values. © 2021 ICLR 2021 - 9th International Conference on Learning Representations. All rights reserved.
KW  - Computational linguistics
KW  - Intelligent agents
KW  - Philosophical aspects
KW  - Basic concepts
KW  - Chatbots
KW  - Human values
KW  - Language model
KW  - Model knowledge
KW  - Moral judgment
KW  - Reinforcement learning agent
KW  - Value judgement
KW  - Well being
KW  - World knowledge
KW  - Reinforcement learning
PB  - International Conference on Learning Representations, ICLR
LA  - English
J2  - ICLR - Int. Conf. Learn. Represent.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 58; Conference name: 9th International Conference on Learning Representations, ICLR 2021; Conference date: 3 May 2021 through 7 May 2021; Conference code: 186703
ER  -

TY  - CONF
AU  - Kasenberg, D.
AU  - Arnold, T.
AU  - Scheutz, M.
TI  - Norms, Rewards, and the Intentional Stance: Comparing Machine Learning Approaches to Ethical Training
PY  - 2018
T2  - AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society
SP  - 184
EP  - 190
DO  - 10.1145/3278721.3278774
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045238240&doi=10.1145%2f3278721.3278774&partnerID=40&md5=03f064b4f58eab282156155a9d5dbca0
AD  - Tufts University, Medford, MA, United States
AB  - The challenge of training AI systems to perform responsibly and beneficially has inspired different approaches for teaching a system what people want and how it is acceptable to attain that in the world. In this paper we compare work in reinforcement learning, in particular inverse reinforcement learning, with our norm inference approach. We test those two systems and present results. Using the idea of the "intentional stance", we explain how a norm inference approach can work even when another agent is acting strictly according to reward functions. In this way norm inference presents itself as a promising, more explicitly accountable approach with which to design AI systems from the start. © 2018 ACM.
KW  - intentional stance
KW  - norm inference
KW  - value alignment
KW  - Behavioral research
KW  - Machine learning
KW  - Reinforcement learning
KW  - AI systems
KW  - Ethical training
KW  - intentional stance
KW  - Inverse reinforcement learning
KW  - Machine learning approaches
KW  - norm inference
KW  - Reward function
KW  - Philosophical aspects
PB  - Association for Computing Machinery, Inc
SN  - 978-145036012-8 (ISBN)
LA  - English
J2  - AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 13; Conference name: 1st AAAI/ACM Conference on AI, Ethics, and Society, AIES 2018; Conference date: 2 February 2018 through 3 February 2018; Conference code: 144126
ER  -

TY  - CONF
AU  - Holler, J.
AU  - Vuorio, R.
AU  - Qin, Z.
AU  - Tang, X.
AU  - Jiao, Y.
AU  - Jin, T.
AU  - Singh, S.
AU  - Wang, C.
AU  - Ye, J.
TI  - Deep reinforcement learning for multi-driver vehicle dispatching and repositioning problem
PY  - 2019
T2  - Proceedings - IEEE International Conference on Data Mining, ICDM
VL  - 2019-November
C7  - 8970873
SP  - 1090
EP  - 1095
DO  - 10.1109/ICDM.2019.00129
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078937790&doi=10.1109%2fICDM.2019.00129&partnerID=40&md5=99d21491cad414335724ddfe7a578fb1
AD  - University of Michigan, United States
AD  - Didi Chuxing
AB  - Order dispatching and driver repositioning (also known as fleet management) in the face of spatially and temporally varying supply and demand are central to a ride-sharing platform marketplace. Hand-crafting heuristic solutions that account for the dynamics in these resource allocation problems is difficult, and may be better handled by an end-to-end machine learning method. Previous works have explored machine learning methods to the problem from a high-level perspective, where the learning method is responsible for either repositioning the drivers or dispatching orders, and as a further simplification, the drivers are considered independent agents maximizing their own reward functions. In this paper we present a deep reinforcement learning approach for tackling the full fleet management and dispatching problems. In addition to treating the drivers as individual agents, we consider the problem from a system-centric perspective, where a central fleet management agent is responsible for decision-making for all drivers. © 2019 IEEE.
KW  - Fleet management
KW  - Order dispatching
KW  - Reinforcement learning
KW  - Ride-sharing
KW  - Data mining
KW  - Decision making
KW  - Economics
KW  - Fleet operations
KW  - Heuristic methods
KW  - Machine learning
KW  - Reinforcement learning
KW  - Fleet management
KW  - Heuristic solutions
KW  - Machine learning methods
KW  - Order dispatching
KW  - Reinforcement learning approach
KW  - Resource allocation problem
KW  - Ride-sharing
KW  - Vehicle dispatching
KW  - Deep learning
A2  - Wang J.
A2  - Shim K.
A2  - Wu X.
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 15504786 (ISSN); 978-172814603-4 (ISBN)
LA  - English
J2  - Proc. IEEE Int. Conf. Data Min. ICDM
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 70; Conference name: 19th IEEE International Conference on Data Mining, ICDM 2019; Conference date: 8 November 2019 through 11 November 2019; Conference code: 157223
ER  -

TY  - CONF
AU  - Zhang, H.
AU  - Li, W.
AU  - Gao, S.
AU  - Wang, X.
AU  - Ye, B.
TI  - ReLeS: A Neural Adaptive Multipath Scheduler based on Deep Reinforcement Learning
PY  - 2019
T2  - Proceedings - IEEE INFOCOM
VL  - 2019-April
C7  - 8737649
SP  - 1648
EP  - 1656
DO  - 10.1109/INFOCOM.2019.8737649
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068231600&doi=10.1109%2fINFOCOM.2019.8737649&partnerID=40&md5=f96b93bc1d79d848c1ca6dbfcbc6efc0
AD  - State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China
AB  - The Multipath TCP (MPTCP) protocol, featured by its ability of capacity aggregation across multiple links and connectivity maintenance against single-path failure, has been attracting increasing attention from the industry and academy. Multipath packet scheduling is a unique and fundamental mechanism for the design and implementation of MPTCP, which is responsible for distributing the traffic over multiple subflows. The existing multipath schedulers are facing the challenges of network heterogeneities, comprehensive QoS goals, and dynamic environments, etc. To address these challenges, we propose ReLeS, a Reinforcement Learning based Scheduler for MPTCP. ReLeS uses modern deep reinforcement learning (DRL) techniques to learn a neural network to generate the control policy for packet scheduling. It adopts a comprehensive reward function that takes diverse QoS characteristics into consideration to optimize packet scheduling. To support real-time scheduling, we propose an asynchronous training algorithm that enables parallel execution of packet scheduling, data collecting, and neural network training. We implement ReLeS in the Linux kernel and evaluate it over both emulated and real network conditions. Extensive experiments show that ReLeS significantly outperforms the state-of-the-art schedulers. © 2019 IEEE.
KW  - Computer operating systems
KW  - Machine learning
KW  - Neural networks
KW  - Reinforcement learning
KW  - Scheduling
KW  - Scheduling algorithms
KW  - Connectivity maintenance
KW  - Design and implementations
KW  - Dynamic environments
KW  - Fundamental mechanisms
KW  - Network heterogeneity
KW  - Neural network training
KW  - Parallel executions
KW  - Real - time scheduling
KW  - Deep learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 0743166X (ISSN); 978-172810515-4 (ISBN)
LA  - English
J2  - Proc IEEE INFOCOM
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 74; Correspondence Address: W. Li; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; email: lwz@nju.edu.cn; Conference name: 2019 IEEE Conference on Computer Communications, INFOCOM 2019; Conference date: 29 April 2019 through 2 May 2019; Conference code: 148846; CODEN: PINFE
ER  -

TY  - JOUR
AU  - Koch, W.
AU  - Mancuso, R.
AU  - West, R.
AU  - Bestavros, A.
TI  - Reinforcement learning for UAV attitude control
PY  - 2019
T2  - ACM Transactions on Cyber-Physical Systems
VL  - 3
IS  - 2
C7  - 22
DO  - 10.1145/3301273
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071868213&doi=10.1145%2f3301273&partnerID=40&md5=115594caa9b236a9f7bc7da58c24a616
AD  - Department of Computer Science, Boston University, 111 Cummington Mall, Boston, 02215, MA, United States
AB  - Autopilot systems are typically composed of an “inner loop” providing stability and control, whereas an “outer loop” is responsible for mission-level objectives, such as way-point navigation. Autopilot systems for unmanned aerial vehicles are predominately implemented using Proportional-Integral-Derivative (PID) control systems, which have demonstrated exceptional performance in stable environments. However, more sophisticated control is required to operate in unpredictable and harsh environments. Intelligent flight control systems is an active area of research addressing limitations of PID control most recently through the use of reinforcement learning (RL), which has had success in other applications, such as robotics. Yet previous work has focused primarily on using RL at the mission-level controller. In this work, we investigate the performance and accuracy of the inner control loop providing attitude control when using intelligent flight control systems trained with state-of-the-art RL algorithms-Deep Deterministic Policy Gradient, Trust Region Policy Optimization, and Proximal Policy Optimization. To investigate these unknowns, we first developed an open source high-fidelity simulation environment to train a flight controller attitude control of a quadrotor through RL. We then used our environment to compare their performance to that of a PID controller to identify if using RL is appropriate in high-precision, time-critical flight control. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
KW  - Adaptive control
KW  - Attitude control
KW  - Autopilot
KW  - Intelligent control
KW  - Machine learning
KW  - PID
KW  - Quadcopter
KW  - Reinforcement learning
KW  - UAV
KW  - Air navigation
KW  - Aircraft control
KW  - Antennas
KW  - Attitude control
KW  - Controllers
KW  - Flight simulators
KW  - Intelligent control
KW  - Learning algorithms
KW  - Learning systems
KW  - Machine learning
KW  - Proportional control systems
KW  - Reinforcement learning
KW  - Robots
KW  - Three term control systems
KW  - Two term control systems
KW  - Unmanned aerial vehicles (UAV)
KW  - Adaptive Control
KW  - Autopilot
KW  - High-fidelity simulations
KW  - Intelligent flight control systems
KW  - Policy optimization
KW  - Proportional-integral-derivative control systems
KW  - Quadcopter
KW  - Stability and control
KW  - Flight control systems
PB  - Association for Computing Machinery
SN  - 2378962X (ISSN)
LA  - English
J2  - ACM Trans.  Cyber-Phys. Syst.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 247
ER  -

TY  - JOUR
AU  - Greene, J.D.
TI  - The rat-a-gorical imperative: Moral intuition and the limits of affective learning
PY  - 2017
T2  - Cognition
VL  - 167
SP  - 66
EP  - 77
DO  - 10.1016/j.cognition.2017.03.004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016422280&doi=10.1016%2fj.cognition.2017.03.004&partnerID=40&md5=2903e1fb3af1df364c4b5fd2abb3cf16
AD  - Department of Psychology, Center for Brain Science, Harvard University, United States
AB  - Decades of psychological research have demonstrated that intuitive judgments are often unreliable, thanks to their inflexible reliance on limited information (Kahneman, 2003, 2011). Research on the computational underpinnings of learning, however, indicates that intuitions may be acquired by sophisticated learning mechanisms that are highly sensitive and integrative. With this in mind, Railton (2014) urges a more optimistic view of moral intuition. Is such optimism warranted? Elsewhere (Greene, 2013) I've argued that moral intuitions offer reasonably good advice concerning the give-and-take of everyday social life, addressing the basic problem of cooperation within a “tribe” (“Me vs. Us”), but that moral intuitions offer unreliable advice concerning disagreements between tribes with competing interests and values (“Us vs. Them”). Here I argue that a computational perspective on moral learning underscores these conclusions. The acquisition of good moral intuitions requires both good (representative) data and good (value-aligned) training. In the case of inter-tribal disagreement (public moral controversy), the problem of bad training looms large, as training processes may simply reinforce tribal differences. With respect to moral philosophy and the paradoxical problems it addresses, the problem of bad data looms large, as theorists seek principles that minimize counter-intuitive implications, not only in typical real-world cases, but in unusual, often hypothetical, cases such as some trolley dilemmas. In such cases the prevailing real-world relationships between actions and consequences are severed or reversed, yielding intuitions that give the right answers to the wrong questions. Such intuitions—which we may experience as the voice of duty or virtue—may simply reflect the computational limitations inherent in affective learning. I conclude, in optimistic agreement with Railton, that progress in moral philosophy depends on our having a better understanding of the mechanisms behind our moral intuitions. © 2017 Elsevier B.V.
KW  - Consequentialism
KW  - Deontology
KW  - Ethics
KW  - Machine learning
KW  - Model-free learning
KW  - Moral judgment
KW  - Normative ethics
KW  - Reinforcement learning
KW  - Utilitarianism
KW  - Affect
KW  - Animals
KW  - Humans
KW  - Intuition
KW  - Learning
KW  - Models, Psychological
KW  - Morals
KW  - Rats
KW  - Thinking
KW  - affective learning
KW  - Article
KW  - ethics
KW  - heuristics
KW  - human
KW  - intuition
KW  - learning
KW  - morality
KW  - nonhuman
KW  - optimism
KW  - pessimism
KW  - philosophy
KW  - priority journal
KW  - rat
KW  - thinking
KW  - trust
KW  - affect
KW  - animal
KW  - psychological model
PB  - Elsevier B.V.
SN  - 00100277 (ISSN)
C2  - 28343626
LA  - English
J2  - Cognition
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 59; Correspondence Address: J.D. Greene; Department of Psychology, Cambridge, 33 Kirkland St., 02138, United States; email: jgreene@wjh.harvard.edu; CODEN: CGTNA
ER  -

TY  - JOUR
AU  - Kim, M.-Y.
AU  - Atakishiyev, S.
AU  - Babiker, H.K.B.
AU  - Farruque, N.
AU  - Goebel, R.
AU  - Zaïane, O.R.
AU  - Motallebi, M.-H.
AU  - Rabelo, J.
AU  - Syed, T.
AU  - Yao, H.
AU  - Chun, P.
TI  - A Multi-Component Framework for the Analysis and Design of Explainable Artificial Intelligence
PY  - 2021
T2  - Machine Learning and Knowledge Extraction
VL  - 3
IS  - 4
SP  - 900
EP  - 921
DO  - 10.3390/make3040045
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123312605&doi=10.3390%2fmake3040045&partnerID=40&md5=2de3fe1f34821767999dc65e09e9cc1f
AD  - XAI Lab, Department of Computing Science, Alberta Machine Intelligence Institute, University of Alberta, Edmonton, T6G 2E8, AB, Canada
AD  - Department of Science, Augustana Faculty, University of Alberta, Camrose, T4V 2R3, AB, Canada
AD  - Huawei Technologies, Edmonton, T6G 2E8, AB, Canada
AD  - Department of Electrical and Computer Engineering, University of Alberta, Edmonton, T6G 2E8, AB, Canada
AD  - Huawei Technologies Canada, Markham, L3R 5A4, ON, Canada
AB  - The rapid growth of research in explainable artificial intelligence (XAI) follows on two substantial developments. First, the enormous application success of modern machine learning methods, especially deep and reinforcement learning, have created high expectations for industrial, commercial, and social value. Second, the emerging and growing concern for creating ethical and trusted AI systems, including compliance with regulatory principles to ensure transparency and trust. These two threads have created a kind of “perfect storm” of research activity, all motivated to create and deliver any set of tools and techniques to address the XAI demand. As some surveys of current XAI suggest, there is yet to appear a principled framework that respects the literature of explainability in the history of science and which provides a basis for the development of a framework for transparent XAI. We identify four foundational components, including the requirements for (1) explicit explanation knowledge representation, (2) delivery of alternative explanations, (3) adjusting explanations based on knowledge of the explainee, and (4) exploiting the advantage of interactive explanation. With those four components in mind, we intend to provide a strategic inventory of XAI requirements, demonstrate their connection to a basic history of XAI ideas, and then synthesize those ideas into a simple framework that can guide the design of AI systems that require XAI. © 2021 by the authors.
KW  - causal explanation
KW  - evaluation of explainable AI
KW  - explainable artificial intelligence
KW  - explainee-specific explanation
KW  - explanation
KW  - interpretation
PB  - MDPI
SN  - 25044990 (ISSN)
LA  - English
J2  - Mach. Learn. Knowl. Extr.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 20; Correspondence Address: M.-Y. Kim; XAI Lab, Department of Computing Science, Alberta Machine Intelligence Institute, University of Alberta, Edmonton, T6G 2E8, Canada; email: miyoung2@ualberta.ca
ER  -

TY  - CONF
AU  - Henderson, P.
AU  - Sinha, K.
AU  - Angelard-Gontier, N.
AU  - Ke, N.R.
AU  - Fried, G.
AU  - Lowe, R.
AU  - Pineau, J.
TI  - Ethical Challenges in Data-Driven Dialogue Systems
PY  - 2018
T2  - AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society
SP  - 123
EP  - 129
DO  - 10.1145/3278721.3278777
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061026082&doi=10.1145%2f3278721.3278777&partnerID=40&md5=6e98606d7304446ccb48341cd5036ae4
AD  - McGill University and Mila, Montréal, QC, Canada
AD  - Polytechnique Montréal and Mila, Montréal, QC, Canada
AB  - The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems. © 2018 ACM.
KW  - Adversarial Examples
KW  - Bias
KW  - Computers and Society
KW  - Dialogue Systems
KW  - Ethics and Safety
KW  - Machine Learning
KW  - Natural Language Processing
KW  - Privacy
KW  - Reinforcement Learning
KW  - Reproducibility
KW  - Security
KW  - Data privacy
KW  - Large dataset
KW  - Learning algorithms
KW  - Learning systems
KW  - Machine learning
KW  - Philosophical aspects
KW  - Reinforcement learning
KW  - Speech processing
KW  - Adversarial Examples
KW  - Bias
KW  - Computers and societies
KW  - Dialogue systems
KW  - NAtural language processing
KW  - Reproducibilities
KW  - Security
KW  - Natural language processing systems
PB  - Association for Computing Machinery, Inc
SN  - 978-145036012-8 (ISBN)
LA  - English
J2  - AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 81; Conference name: 1st AAAI/ACM Conference on AI, Ethics, and Society, AIES 2018; Conference date: 2 February 2018 through 3 February 2018; Conference code: 144126
ER  -

TY  - CONF
AU  - Orłowski, M.
AU  - Wrona, T.
AU  - Pankiewicz, N.
AU  - Turlej, W.
TI  - Safe and Goal-Based Highway Maneuver Planning with Reinforcement Learning
PY  - 2020
T2  - Advances in Intelligent Systems and Computing
VL  - 1196 AISC
SP  - 1261
EP  - 1274
DO  - 10.1007/978-3-030-50936-1_105
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088207585&doi=10.1007%2f978-3-030-50936-1_105&partnerID=40&md5=fd66701b671524956ab50f826a9a357c
AD  - AGH University of Science and Technology, Krakow, Poland
AD  - Aptiv, Krakow, Poland
AB  - As autonomous driving moves closer to a real-world application, more and more attention is being paid to the motion planning part of the system. To handle vastness of possible road scenarios, negotiate with other road users and generate an intelligent control strategy in a constantly changing environment, data-driven techniques and artificial intelligence methods seem to be the approach of choice. In this paper, we present reinforcement learning (RL) agent which is embedded in a deterministic, safety envelope. The agent is responsible for generating high-level maneuvers, such as a lane following or a lane change. The primary goal of the agent is to reach a given lane in a given distance, while traveling on a highway. The selected maneuver is then executed with use of deterministic methods utilizing concept of Responsible-Sensitive Safety (RSS) framework, which formalizes safety constrains in a form of mathematical model. The proposed solution has been evaluated in two environments: one in which the agent receives a predefined reward for getting to a correct lane and second, in which it is rewarded for doing this in a time-optimal manner. We have evaluated the proposed solution against an another RL-based agent, which is steering vehicle by low-level control signals, such as acceleration and steering angle. © 2020, Springer Nature Switzerland AG.
KW  - Autonomous driving
KW  - Behavior planning
KW  - Deep reinforcement learning
KW  - Maneuver planning
KW  - Automobile steering equipment
KW  - Highway planning
KW  - Motion planning
KW  - Motor transportation
KW  - Roads and streets
KW  - Artificial intelligence methods
KW  - Autonomous driving
KW  - Changing environment
KW  - Data driven technique
KW  - Deterministic methods
KW  - Intelligent control strategies
KW  - Low level control
KW  - Reinforcement learning agent
KW  - Reinforcement learning
A2  - Bartoszewicz A.
A2  - Kabzinski J.
A2  - Kacprzyk J.
PB  - Springer
SN  - 21945357 (ISSN); 978-303050935-4 (ISBN)
LA  - English
J2  - Adv. Intell. Sys. Comput.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 3; Correspondence Address: N. Pankiewicz; AGH University of Science and Technology, Krakow, Poland; email: nikodem.pankiewicz@aptiv.com; Conference name: 20th Polish Control Conference, PCC 2020; Conference date: 22 June 2020 through 24 June 2020; Conference code: 241519
ER  -

TY  - JOUR
AU  - Hausknecht, M.
AU  - Li, W.-K.
AU  - Mauk, M.
AU  - Stone, P.
TI  - Machine Learning Capabilities of a Simulated Cerebellum
PY  - 2017
T2  - IEEE Transactions on Neural Networks and Learning Systems
VL  - 28
IS  - 3
C7  - 7393590
SP  - 510
EP  - 522
DO  - 10.1109/TNNLS.2015.2512838
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961366242&doi=10.1109%2fTNNLS.2015.2512838&partnerID=40&md5=7c996669fa9f94750dd7d0eb3d302e5b
AD  - Department of Computer Science, University of Texas at Austin, Austin, 78712, TX, United States
AD  - Center for Learning and Memory, University of Texas at Austin, Austin, 78712, TX, United States
AB  - This paper describes the learning and control capabilities of a biologically constrained bottom-up model of the mammalian cerebellum. Results are presented from six tasks: 1) eyelid conditioning; 2) pendulum balancing; 3) proportional-integral-derivative control; 4) robot balancing; 5) pattern recognition; and 6) MNIST handwritten digit recognition. These tasks span several paradigms of machine learning, including supervised learning, reinforcement learning, control, and pattern recognition. Results over these six domains indicate that the cerebellar simulation is capable of robustly identifying static input patterns even when randomized across the sensory apparatus. This capability allows the simulated cerebellum to perform several different supervised learning and control tasks. On the other hand, both reinforcement learning and temporal pattern recognition prove problematic due to the delayed nature of error signals and the simulator's inability to solve the credit assignment problem. These results are consistent with previous findings which hypothesize that in the human brain, the basal ganglia is responsible for reinforcement learning, while the cerebellum handles supervised learning. © 2016 IEEE.
KW  - Cerebellar pattern recognition
KW  - cerebellum
KW  - inverted pendulum balancing (cart-pole)
KW  - MNIST handwritten digit recognition
KW  - proportional-integral-derivative (PID) control
KW  - Robot balance
KW  - Animals
KW  - Cerebellum
KW  - Computer Simulation
KW  - Conditioning, Eyelid
KW  - Humans
KW  - Machine Learning
KW  - Models, Neurological
KW  - Neural Networks (Computer)
KW  - Neurons
KW  - Pattern Recognition, Physiological
KW  - Postural Balance
KW  - Reinforcement (Psychology)
KW  - Artificial intelligence
KW  - Brain
KW  - Character recognition
KW  - Combinatorial optimization
KW  - Learning systems
KW  - Mammals
KW  - Pattern recognition
KW  - Proportional control systems
KW  - Supervised learning
KW  - Two term control systems
KW  - Basal ganglia
KW  - Bottom up models
KW  - Control capabilities
KW  - Credit assignment problems
KW  - Handwritten digit recognition
KW  - Input patterns
KW  - Proportional integral derivative control
KW  - Temporal pattern recognition
KW  - animal
KW  - artificial neural network
KW  - biological model
KW  - body equilibrium
KW  - cerebellum
KW  - computer simulation
KW  - conditioning
KW  - human
KW  - machine learning
KW  - nerve cell
KW  - pattern recognition
KW  - physiology
KW  - reinforcement
KW  - ultrastructure
KW  - Reinforcement learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 2162237X (ISSN)
C2  - 26829807
LA  - English
J2  - IEEE Trans. Neural Networks Learn. Sys.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 24
ER  -

TY  - JOUR
AU  - Vo, N.N.Y.
AU  - He, X.
AU  - Liu, S.
AU  - Xu, G.
TI  - Deep learning for decision making and the optimization of socially responsible investments and portfolio
PY  - 2019
T2  - Decision Support Systems
VL  - 124
C7  - 113097
DO  - 10.1016/j.dss.2019.113097
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069803699&doi=10.1016%2fj.dss.2019.113097&partnerID=40&md5=84486f00e58eea319ebf1004673a5c6e
AD  - Advanced Analytics Institute, University of Technology Sydney, 2-12 Blackfriars Street, Chippendale, NSW 2008, Australia
AD  - Business School, University of Technology Sydney, 14-28 Ultimo Rd, Ultimo, NSW 2007, Australia
AB  - A socially responsible investment portfolio takes into consideration the environmental, social and governance aspects of companies. It has become an emerging topic for both financial investors and researchers recently. Traditional investment and portfolio theories, which are used for the optimization of financial investment portfolios, are inadequate for decision-making and the construction of an optimized socially responsible investment portfolio. In response to this problem, we introduced a Deep Responsible Investment Portfolio (DRIP) model that contains a Multivariate Bidirectional Long Short-Term Memory neural network, to predict stock returns for the construction of a socially responsible investment portfolio. The deep reinforcement learning technique was adapted to retrain neural networks and rebalance the portfolio periodically. Our empirical data revealed that the DRIP framework could achieve competitive financial performance and better social impact compared to traditional portfolio models, sustainable indexes and funds. © 2019 Elsevier B.V.
KW  - Decision support systems
KW  - Deep reinforcement learning
KW  - Multivariate analytics
KW  - Portfolio optimization
KW  - Socially responsible investment
KW  - Decision making
KW  - Decision support systems
KW  - Electronic trading
KW  - Investments
KW  - Machine learning
KW  - Reinforcement learning
KW  - Financial investments
KW  - Financial investors
KW  - Financial performance
KW  - Investment portfolio
KW  - Multivariate analytics
KW  - Portfolio optimization
KW  - Reinforcement learning techniques
KW  - Socially responsible investments
KW  - Deep learning
PB  - Elsevier B.V.
SN  - 01679236 (ISSN)
LA  - English
J2  - Decis Support Syst
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 86; Correspondence Address: G. Xu; Advanced Analytics Institute, University of Technology Sydney, Chippendale, 2-12 Blackfriars Street, NSW 2008, Australia; email: guandong.xu@uts.edu.au; CODEN: DSSYD
ER  -

TY  - CONF
AU  - Ho, J.
AU  - Wang, C.-M.
TI  - Human-Centered AI using Ethical Causality and Learning Representation for Multi-Agent Deep Reinforcement Learning
PY  - 2021
T2  - Proceedings of the 2021 IEEE International Conference on Human-Machine Systems, ICHMS 2021
DO  - 10.1109/ICHMS53169.2021.9582667
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118919258&doi=10.1109%2fICHMS53169.2021.9582667&partnerID=40&md5=645bce754f6391d69de06806823bc0f9
AD  - Taiwan International Graduate Program, Social Networks and Human-Centered Computing Program, Taiwan
AD  - Institute of Information Science, Academia Sinica, Taipei, 115, Taiwan
AB  - Human-Centered Computing and AI are two fields devoted to several cross-intersecting interests in the modern AI design. They consider human factors and the machine learning algorithms to enhance compatibility and reliability for human-robot interaction and cooperation. In this work, we propose a novel design concept for the challenging issues that have raised ethical dilemmas; an augmented ethical causality with successor representation for policy gradient models Human-Centered AI with environments. The proposed system leverages Human-Centered AI for using explainable knowledge to construct the ethical causality, and shows it significantly outperformed the statistical approach and baselines alone by further considering meta parametric Human-Centered ethical priorities, when compared to other approaches in the simulated game theory Deep Reinforcement Learning environments. The experimental results aim to efficiently and effectively access the cause, effect and impact of causal inference and multi-agent heterogeneity in the DRL environments for natural, general and significant causal learning representations. © 2021 IEEE.
KW  - Ethical Causality
KW  - Human-CenteredAI
KW  - Multi-Agent Deep Reinforcement Learning
KW  - Successor Representation
KW  - Computation theory
KW  - Computer aided instruction
KW  - Game theory
KW  - Human robot interaction
KW  - Learning algorithms
KW  - Multi agent systems
KW  - Philosophical aspects
KW  - Reinforcement learning
KW  - Ethical causality
KW  - Human-centered computing
KW  - Human-centeredai
KW  - Human-robot-cooperation
KW  - Humans-robot interactions
KW  - Machine learning algorithms
KW  - Multi agent
KW  - Multi-agent deep reinforcement learning
KW  - Novel design
KW  - Successor representation
KW  - Deep learning
A2  - Nurnberger A.
A2  - Fortino G.
A2  - Guerrieri A.
A2  - Kaber D.
A2  - Mendonca D.
A2  - Schilling M.
A2  - Yu Z.
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-166540170-8 (ISBN)
LA  - English
J2  - Proc. IEEE Int. Conf. Human-Mach. Syst., ICHMS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2; Conference name: 2021 IEEE International Conference on Human-Machine Systems, ICHMS 2021; Conference date: 8 September 2021 through 10 September 2021; Conference code: 173215
ER  -

TY  - JOUR
AU  - McGuire, S.
AU  - Furlong, P.M.
AU  - Heckman, C.
AU  - Julier, S.
AU  - Ahmed, N.
TI  - Human-aware reinforcement learning for fault recovery using contextual gaussian processes
PY  - 2021
T2  - Journal of Aerospace Information Systems
VL  - 18
IS  - 7
SP  - 429
EP  - 441
DO  - 10.2514/1.I010921
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119201178&doi=10.2514%2f1.I010921&partnerID=40&md5=02acf46a2a502c8b4c2234ffd4ca168f
AD  - University of California Santa Cruz, Santa Cruz, 95064, CA, United States
AD  - University of Waterloo, Waterloo, N2L 3G1, ON, Canada
AD  - University of Colorado at Boulder, Boulder, 80309, CO, United States
AD  - University College London, London, WC1E 6BT, United Kingdom
AD  - University of Colorado at Boulder, Boulder, 80303, CO, United States
AB  - This work addresses the iterated nonstationary assistant selection problem, in which over the course of repeated interactions on a mission, an autonomous robot experiencing a fault must select a single human from among a group of assistants to restore it to operation. The assistants in our problem have a level of performance that changes as a function of their experience solving the problem. Our approach uses reinforcement learning via a multi-arm bandit formulation to learn about the capabilities of each potential human assistant and decide which human to task. This study, which is built on our past work, evaluates the potential for a Gaussian-process-based machine learning method to effectively model the complex dynamics associated with human learning and forgetting. Application of our method in simulation shows that our method is capable of tracking performance of human-like dynamics for learning and forgetting. Using a novel selection policy called the proficiency window, it is shown that our technique can outperform baseline selection strategies while providing guarantees on human use. Our work offers an effective potential alternative to dedicated human supervisors, with application to any human–robot system where a set of humans is responsible for overseeing autonomous robot operations. © 2021 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved.
KW  - Aviation
KW  - Gaussian distribution
KW  - Human robot interaction
KW  - Reinforcement learning
KW  - Fault recovery
KW  - Gaussian Processes
KW  - Human-aware
KW  - Learn+
KW  - Learning and forgetting
KW  - Machine learning methods
KW  - Nonstationary
KW  - Performance
KW  - Process-based
KW  - Selection problems
KW  - Gaussian noise (electronic)
PB  - AIAA International
SN  - 23273097 (ISSN)
LA  - English
J2  - J. Aerosp. Inf. Sys.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2
ER  -

TY  - CONF
AU  - Ecoffet, A.
AU  - Lehman, J.
TI  - Reinforcement Learning Under Moral Uncertainty
PY  - 2021
T2  - Proceedings of Machine Learning Research
VL  - 139
SP  - 2926
EP  - 2936
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115673718&partnerID=40&md5=7e539f5f37c066cca7a652dd308cdb81
AD  - Uber AI Labs, San Francisco, CA, United States
AD  - OpenAI, San Francisco, CA, United States
AB  - An ambitious goal for machine learning is to create agents that behave ethically: The capacity to abide by human moral norms would greatly expand the context in which autonomous agents could be practically and safely deployed, e.g. fully autonomous vehicles will encounter charged moral decisions that complicate their deployment. While ethical agents could be trained by rewarding correct behavior under a specific moral theory (e.g. utilitarianism), there remains widespread disagreement about the nature of morality. Acknowledging such disagreement, recent work in moral philosophy proposes that ethical behavior requires acting under moral uncertainty, i.e. to take into account when acting that one's credence is split across several plausible ethical theories. This paper translates such insights to the field of reinforcement learning, proposes two training methods that realize different points among competing desiderata, and trains agents in simple environments to act under moral uncertainty. The results illustrate (1) how such uncertainty can help curb extreme behavior from commitment to single theories and (2) several technical complications arising from attempting to ground moral philosophy in RL (e.g. how can a principled trade-off between two competing but incomparable reward functions be reached). The aim is to catalyze progress towards morally-competent agents and highlight the potential of RL to contribute towards the computational grounding of moral philosophy. Copyright © 2021 by the author(s)
KW  - Autonomous agents
KW  - Computation theory
KW  - Economic and social effects
KW  - Ethical technology
KW  - Learning systems
KW  - Ethical behavior
KW  - Ethical theories
KW  - Fully-autonomous vehicles
KW  - Machine-learning
KW  - Moral philosophy
KW  - Moral theory
KW  - Reinforcement learnings
KW  - Simple environments
KW  - Training methods
KW  - Uncertainty
KW  - Reinforcement learning
PB  - ML Research Press
SN  - 26403498 (ISSN); 978-171384506-5 (ISBN)
LA  - English
J2  - Proc. Mach. Learn. Res.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 13; Correspondence Address: A. Ecoffet; Uber AI Labs, San Francisco, United States; email: adrienecoffet@gmail.com; Conference name: 38th International Conference on Machine Learning, ICML 2021; Conference date: 18 July 2021 through 24 July 2021; Conference code: 178975
ER  -

TY  - CONF
AU  - Rodriguez-Soto, M.
AU  - Lopez-Sanchez, M.
AU  - Rodriguez-Aguilar, J.A.
TI  - Multi-Objective Reinforcement Learning for Designing Ethical Environments
PY  - 2021
T2  - IJCAI International Joint Conference on Artificial Intelligence
SP  - 545
EP  - 551
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125445496&partnerID=40&md5=ec1eeba2f9452e689a732ce5703c88a9
AD  - Artificial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Spain
AD  - Universitat de Barcelona (UB), Barcelona, Spain
AB  - AI research is being challenged with ensuring that autonomous agents learn to behave ethically, namely in alignment with moral values. A common approach, founded on the exploitation of Reinforcement Learning techniques, is to design environments that incentivise agents to behave ethically. However, to the best of our knowledge, current approaches do not theoretically guarantee that an agent will learn to behave ethically. Here, we make headway along this direction by proposing a novel way of designing environments wherein it is formally guaranteed that an agent learns to behave ethically while pursuing its individual objective. Our theoretical results develop within the formal framework of Multi-Objective Reinforcement Learning to ease the handling of an agent's individual and ethical objectives. As a further contribution, we leverage on our theoretical results to introduce an algorithm that automates the design of ethical environments. © 2021 International Joint Conferences on Artificial Intelligence. All rights reserved.
KW  - Ethical technology
KW  - Reinforcement learning
KW  - 'current
KW  - Design environment
KW  - Formal framework
KW  - Learn+
KW  - Multi objective
KW  - Reinforcement learning techniques
KW  - Reinforcement learnings
KW  - Autonomous agents
A2  - Zhou Z.-H.
PB  - International Joint Conferences on Artificial Intelligence
SN  - 10450823 (ISSN); 978-099924119-6 (ISBN)
LA  - English
J2  - IJCAI Int. Joint Conf. Artif. Intell.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 12; Conference name: 30th International Joint Conference on Artificial Intelligence, IJCAI 2021; Conference date: 19 August 2021 through 27 August 2021; Conference code: 177242
ER  -

TY  - JOUR
AU  - Rabe, M.
AU  - Ammouriova, M.
AU  - Schmitt, D.
AU  - Dross, F.
TI  - Simheuristics approaches for efficient decision-making support in materials trading networks
PY  - 2021
T2  - Algorithms
VL  - 14
IS  - 1
C7  - 23
DO  - 10.3390/a14010023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099777899&doi=10.3390%2fa14010023&partnerID=40&md5=d3767eade1ea93120a5bd33d21257cf7
AD  - IT in Production and Logistics, Faculty of Mechanical Engineering, TU Dortmund, Dortmund, 44227, Germany
AB  - The distribution process in business-to-business materials trading is among the most complex and in transparent ones within logistics. The highly volatile environment requires continuous adaptations by the responsible decision-makers, who face a substantial number of potential improvement actions with conflicting goals, such as simultaneously maintaining a high service level and low costs. Simulation-optimisation approaches have been proposed in this context, for example based on evolutionary algorithms. But, on real-world system dimensions, they face impractically long computation times. This paper addresses this challenge in two principal streams. On the one hand, reinforcement learning is investigated to reduce the response time of the system in a concrete decision situation. On the other hand, domain-specific information and defining equivalent solutions are exploited to support a metaheuristic algorithm. For these approaches, we have developed suitable implementations and evaluated them with subsets of real-world data. The results demonstrate that reinforcement learning exploits the idle time between decision situations to learn which decisions might be most promising, thus adding computation time but significantly reducing the response time. Using domain-specific information reduces the number of required simulation runs and guides the search for promising actions. In our experimentation, defining equivalent solutions decreased the number of required simulation runs up to 15%. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - Distribution networks
KW  - Logistics
KW  - Machine learning
KW  - Optimization
KW  - Simulation
KW  - Commerce
KW  - Evolutionary algorithms
KW  - Reinforcement learning
KW  - Response time (computer systems)
KW  - Business to business
KW  - Decision making support
KW  - Decision situation
KW  - Distribution process
KW  - Domain-specific information
KW  - Meta heuristic algorithm
KW  - Simulation optimisation
KW  - Volatile environments
KW  - Decision making
PB  - MDPI AG
SN  - 19994893 (ISSN)
LA  - English
J2  - Algorithms
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2; Correspondence Address: M. Rabe; IT in Production and Logistics, Faculty of Mechanical Engineering, TU Dortmund, Dortmund, 44227, Germany; email: markus.rabe@tu-dortmund.de
ER  -

TY  - CONF
AU  - Bussmann, B.
AU  - Heinerman, J.
AU  - Lehman, J.
TI  - Towards empathic deep q-learning
PY  - 2019
T2  - CEUR Workshop Proceedings
VL  - 2419
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071013649&partnerID=40&md5=bbed63babea47b622b436078cf880e60
AD  - University of Amsterdam, Netherlands
AD  - VU University Amsterdam, Netherlands
AD  - Uber AI Labs, San Francisco, United States
AB  - As reinforcement learning (RL) scales to solve increasingly complex tasks, interest continues to grow in the fields of AI safety and machine ethics. As a contribution to these fields, this paper introduces an extension to Deep Q-Networks (DQNs), called Empathic DQN, that is loosely inspired both by empathy and the golden rule ("Do unto others as you would have them do unto you"). Empathic DQN aims to help mitigate negative side effects to other agents resulting from myopic goal-directed behavior. We assume a setting where a learning agent coexists with other independent agents (who receive unknown rewards), where some types of reward (e.g. negative rewards from physical harm) may generalize across agents. Empathic DQN combines the typical (self-centered) value with the estimated value of other agents, by imagining (by its own standards) the value of it being in the other's situation (by considering constructed states where both agents are swapped). Proof-of-concept results in two gridworld environments highlight the approach's potential to decrease collateral harms. While extending Empathic DQN to complex environments is non-trivial, we believe that this first step highlights the potential of bridge-work between machine ethics and RL to contribute useful priors for norm-abiding RL agents. © 2019 CEUR-WS. All rights reserved.
KW  - Artificial intelligence
KW  - Complex networks
KW  - Philosophical aspects
KW  - Reinforcement learning
KW  - Bridge works
KW  - Complex environments
KW  - Complex task
KW  - Goal-directed behavior
KW  - Independent agents
KW  - Learning agents
KW  - Negative side effects
KW  - Proof of concept
KW  - Deep learning
A2  - Espinoza H.
A2  - Yu H.
A2  - Huang X.
A2  - Lecue F.
A2  - Chen C.
A2  - Hernandez-Orallo J.
A2  - hEigeartaigh S.O.
A2  - Mallah R.
PB  - CEUR-WS
SN  - 16130073 (ISSN)
LA  - English
J2  - CEUR Workshop Proc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5; Correspondence Address: B. Bussmann; University of Amsterdam, Netherlands; email: bart.bussmann@student.uva.nl; Conference name: 2019 Workshop on Artificial Intelligence Safety, AISafety 2019; Conference date: 11 August 2019 through 12 August 2019; Conference code: 149956
ER  -

TY  - JOUR
AU  - Wei, Y.
AU  - Pan, L.
AU  - Liu, S.
AU  - Wu, L.
AU  - Meng, X.
TI  - DRL-Scheduling: An intelligent QoS-Aware job scheduling framework for applications in clouds
PY  - 2018
T2  - IEEE Access
VL  - 6
C7  - 8476582
SP  - 55112
EP  - 55125
DO  - 10.1109/ACCESS.2018.2872674
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054368042&doi=10.1109%2fACCESS.2018.2872674&partnerID=40&md5=837f4ad028e9374a99b427c19e3a8dc7
AD  - School of Software, Shandong University, Jinan, 250101, China
AB  - As an increasing number of traditional applications migrated to the cloud, achieving resource management and performance optimization in such a dynamic and uncertain environment becomes a big challenge for cloud-based application providers. In particular, job scheduling is a non-trivial task, which is responsible for allocating massive job requests submitted by users to the most suitable resources and satisfying user QoS requirements as much as possible. Inspired by recent success of using deep reinforcement learning techniques to solve AI control problems, in this paper, we propose an intelligent QoS-aware job scheduling framework for application providers. A deep reinforcement learning-based job scheduler is the key component of the framework. It is able to learn to make appropriate online job-to-VM decisions for continuous job requests directly from its experiences without any prior knowledge. Experimental results using synthetic workloads and real-world NASA workload traces show that compared with other baseline solutions, our proposed job scheduling approach can efficiently reduce average job response time (e.g., reduced by 40.4% compared with the best baseline for NASA traces), guarantee the QoS at a high level (e.g., job success rate is higher than 93% for all simulated changing workload scenarios), and adapt to different workload conditions. © 2013 IEEE.
KW  - Cloud computing
KW  - deep Q-Learning
KW  - job scheduling
KW  - QoS
KW  - reinforcement learning
KW  - Cloud computing
KW  - Deep learning
KW  - Job analysis
KW  - Job shop scheduling
KW  - Learning algorithms
KW  - Learning systems
KW  - NASA
KW  - Optimization
KW  - Quality control
KW  - Reinforcement learning
KW  - Scheduling
KW  - Cloud-based applications
KW  - Job scheduling
KW  - Performance optimizations
KW  - Processor scheduling
KW  - Q-learning
KW  - Reinforcement learning techniques
KW  - Task analysis
KW  - Uncertain environments
KW  - Quality of service
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 21693536 (ISSN)
LA  - English
J2  - IEEE Access
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 67; Correspondence Address: L. Pan; School of Software, Shandong University, Jinan, 250101, China; email: panli@sdu.edu.cn
ER  -

TY  - JOUR
AU  - Pinka, R.
TI  - Synthetic Deliberation: Can Emulated Imagination Enhance Machine Ethics?
PY  - 2021
T2  - Minds and Machines
VL  - 31
IS  - 1
SP  - 121
EP  - 136
DO  - 10.1007/s11023-020-09531-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087942371&doi=10.1007%2fs11023-020-09531-w&partnerID=40&md5=2ccd2bf08daeb9afaad910d87d439e5e
AD  - Department of Philosophy, University of North Carolina at Charlotte, Charlotte, NC, United States
AB  - Artificial intelligence is becoming increasingly entwined with our daily lives: AIs work as assistants through our phones, control our vehicles, and navigate our vacuums. As these objects become more complex and work within our societies in ways that affect our well-being, there is a growing demand for machine ethics—we want a guarantee that the various automata in our lives will behave in a way that minimizes the amount of harm they create. Though many technologies exist as moral artifacts (and perhaps moral agents), the development of a truly ethical AI system is highly contentious; theorists have proposed and critiqued countless possibilities for programming these agents to become ethical. Many of these arguments, however, presuppose the possibility that an artificially intelligent system can actually be ethical. In this essay, I will explore a potential path to AI ethics by considering the role of imagination in the deliberative process via the work of John Dewey and his interpreters, showcasing one form of reinforcement learning that mimics imaginative deliberation. With these components in place, I contend that such an artificial agent is capable of something very near ethical behavior—close enough that we may consider it so. © 2020, Springer Nature B.V.
KW  - Artificial intelligence
KW  - Machine ethics
KW  - Machine learning
KW  - Philosophy of technology
KW  - Pragmatism
KW  - STS
KW  - Intelligent systems
KW  - Reinforcement learning
KW  - AI systems
KW  - Artificial agents
KW  - Daily lives
KW  - Deliberative process
KW  - Ethical behavior
KW  - Growing demand
KW  - Moral agents
KW  - Well being
KW  - Philosophical aspects
PB  - Springer Science and Business Media B.V.
SN  - 09246495 (ISSN)
LA  - English
J2  - Minds Mach
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: R. Pinka; Department of Philosophy, University of North Carolina at Charlotte, Charlotte, United States; email: rpinka@uncc.edu; CODEN: MMACE
ER  -

TY  - JOUR
AU  - Rampini, L.
AU  - Re Cecconi, F.
TI  - ARTIFICIAL INTELLIGENCE IN CONSTRUCTION ASSET MANAGEMENT: A REVIEW OF PRESENT STATUS, CHALLENGES AND FUTURE OPPORTUNITIES
PY  - 2022
T2  - Journal of Information Technology in Construction
VL  - 27
SP  - 884
EP  - 913
DO  - 10.36680/j.itcon.2022.043
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144309918&doi=10.36680%2fj.itcon.2022.043&partnerID=40&md5=491c5bb6f648df0a223141bcb7f80467
AD  - Politecnico di Milano, Italy
AB  - The built environment is responsible for roughly 40% of global greenhouse emissions, making the sector a crucial factor for climate change and sustainability. Meanwhile, other sectors (like manufacturing) adopted Artificial Intelligence (AI) to solve complex, non-linear problems to reduce waste, inefficiency, and pollution. Therefore, many research efforts in the Architecture, Engineering, and Construction community have recently tried introducing AI into building asset management (AM) processes. Since AM encompasses a broad set of disciplines, an overview of several AI applications, current research gaps, and trends is needed. In this context, this study conducted the first state-of-the-art research on AI for building asset management. A total of 578 papers were analyzed with bibliometric tools to identify prominent institutions, topics, and journals. The quantitative analysis helped determine the most researched areas of AM and which AI techniques are applied. The areas were furtherly investigated by reading in-depth the 83 most relevant studies selected by screening the articles' abstracts identified in the bibliometric analysis. The results reveal many applications for Energy Management, Condition assessment, Risk management, and Project management areas. Finally, the literature review identified three main trends that can be a reference point for future studies made by practitioners or researchers: Digital Twin, Generative Adversarial Networks (with synthetic images) for data augmentation, and Deep Reinforcement Learning. © 2022 International Council for Research and Innovation in Building and Construction. All rights reserved.
KW  - Artificial Intelligence
KW  - Asset Management
KW  - Computer Vision
KW  - Machine Learning
KW  - Neural Network
KW  - Climate change
KW  - Computer vision
KW  - Deep learning
KW  - Project management
KW  - Risk assessment
KW  - Risk management
KW  - Sustainable development
KW  - Architecture community
KW  - Architecture engineering
KW  - Assets management
KW  - Built environment
KW  - Greenhouse emissions
KW  - Machine-learning
KW  - Neural-networks
KW  - Nonlinear problems
KW  - Present status
KW  - Research efforts
KW  - Asset management
PB  - International Council for Research and Innovation in Building and Construction
SN  - 18744753 (ISSN)
LA  - English
J2  - J. Inf. Technol. Constr.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 8
ER  -

TY  - CONF
AU  - Gorodishcheva, A.N.
AU  - Paskhalskaya, Y.V.
AU  - Gorodishchev, A.V.
AU  - Vinogradova, A.I.
AU  - Kovalev, G.P.
TI  - IoT Ethic in Scientific Communications
PY  - 2021
T2  - Proceedings of the 2021 Communication Strategies in Digital Society Seminar, ComSDS 2021
C7  - 9422860
SP  - 136
EP  - 140
DO  - 10.1109/ComSDS52473.2021.9422860
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106053528&doi=10.1109%2fComSDS52473.2021.9422860&partnerID=40&md5=eaeac6757a00f0b6bf8f00936c0a9ae4
AD  - Reshetnev Siberian State University of Science and Technology, Krasnoyarsk, Russian Federation
AD  - Krasnogvardeyskiy District Centralized Library System, Saint Petersburg, Russian Federation
AB  - IoT is now essential for the resource allocation of organized things and people. Science is one of the social institutions that now need in the Internet of Things technology to develop the interoperability of heterogeneous devices and scientific research services. The system of interactions with IoT technology for professional scientific communities, which have special ethical rules for responding to political, economic, social and cultural challenges of society, deserves special rules. Poor contextual awareness of the scientific community is closely related to the problem of protecting scientific data in IoT networks from bad compatibility and inability to connect to the specified communication channels. The search and analysis of scientific communications by existing IoT services is limited by the design limits of the methodological use of connecting IoT devices. The ethical norms of the scientific community can be seen in IoT technologies as constraints, indicators, characteristics and reinforcements for machine learning and IoT human-machine communications © 2021 IEEE.
KW  - Internet of things
KW  - IoT
KW  - law of priority
KW  - scientific communications
KW  - scientific ethics
KW  - Interoperability
KW  - Philosophical aspects
KW  - Reinforcement learning
KW  - Design limits
KW  - Heterogeneous devices
KW  - Human-machine communication
KW  - Internet of things technologies
KW  - Scientific communication
KW  - Scientific community
KW  - Scientific data
KW  - Scientific researches
KW  - Internet of things
A2  - Shaposhnikov S.
A2  - Saint Petersburg Electrotechnical University "LETI", Prof. Popov Str. 5, Saint Petersburg
A2  - Sharakhina L.
A2  - Saint Petersburg Electrotechnical University "LETI", Prof. Popov Str. 5,, Saint Petersburg
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-073814529-7 (ISBN)
LA  - English
J2  - Proc. Commun. Strateg. Digit. Soc. Semin., ComSDS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 2021 Communication Strategies in Digital Society Seminar, ComSDS 2021; Conference code: 168784
ER  -

TY  - JOUR
AU  - Da Silva, F.L.
AU  - Warnell, G.
AU  - Costa, A.H.R.
AU  - Stone, P.
TI  - Agents teaching agents: a survey on inter-agent transfer learning
PY  - 2020
T2  - Autonomous Agents and Multi-Agent Systems
VL  - 34
IS  - 1
C7  - 9
DO  - 10.1007/s10458-019-09430-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076345237&doi=10.1007%2fs10458-019-09430-0&partnerID=40&md5=ee9ed720e5c11e2d6944163ea64cc550
AD  - University of São Paulo, São Paulo, Brazil
AD  - Army Research Laboratory, Austin, TX, United States
AD  - The University of Texas, Austin, TX, United States
AB  - While recent work in reinforcement learning (RL) has led to agents capable of solving increasingly complex tasks, the issue of high sample complexity is still a major concern. This issue has motivated the development of additional techniques that augment RL methods in an attempt to increase task learning speed. In particular, inter-agent teaching—endowing agents with the ability to respond to instructions from others—has been responsible for many of these developments. RL agents that can leverage instruction from a more competent teacher have been shown to be able to learn tasks significantly faster than agents that cannot take advantage of such instruction. That said, the inter-agent teaching paradigm presents many new challenges due to, among other factors, differences between the agents involved in the teaching interaction. As a result, many inter-agent teaching methods work only in restricted settings and have proven difficult to generalize to new domains or scenarios. In this article, we propose two frameworks that provide a comprehensive view of the challenges associated with inter-agent teaching. We highlight state-of-the-art solutions, open problems, prospective applications, and argue that new research in this area should be developed in the context of the proposed frameworks. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.
KW  - Multiagent learning
KW  - Reinforcement learning
KW  - Transfer learning
KW  - Machine learning
KW  - Multi agent systems
KW  - Multi-agent learning
KW  - Prospective applications
KW  - Sample complexity
KW  - State of the art
KW  - Teaching agents
KW  - Teaching methods
KW  - Teaching paradigm
KW  - Transfer learning
KW  - Reinforcement learning
PB  - Springer
SN  - 13872532 (ISSN)
LA  - English
J2  - Auton. Agents Multi-Agent Syst.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 50; Correspondence Address: F.L. Da Silva; University of São Paulo, São Paulo, Brazil; email: f.leno@usp.br
ER  -

TY  - JOUR
AU  - Yang, C.
AU  - Chin, K.-W.
AU  - He, T.
AU  - Liu, Y.
TI  - On Sampling Time Maximization in Wireless Powered Internet of Things
PY  - 2019
T2  - IEEE Transactions on Green Communications and Networking
VL  - 3
IS  - 3
C7  - 8675548
SP  - 641
EP  - 650
DO  - 10.1109/TGCN.2019.2907913
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071342502&doi=10.1109%2fTGCN.2019.2907913&partnerID=40&md5=bab56136ce4e215723c498e5754dc4dd
AD  - School of Computer Science, Zhongyuan University of Technology, Zhengzhou, 451191, China
AD  - School of Electrical Computer and Telecommunications Engineering, University of Wollongong, Wollongong, 2522, NSW, Australia
AD  - College of Information Science and Technology, Jinan University, Guangzhou, 510000, China
AD  - Department of Electrical Engineering, Columbia University, New York, 10027, NY, United States
AB  - Sensing devices operating in the upcoming Internet of Things (IoT) are likely to rely on the radio frequency (RF) transmissions of a hybrid access point (HAP) for energy. The HAP is also responsible for setting the sampling or monitoring time of these devices according to their harvested energy. This task, however, is made challenging when the HAP has imprecise knowledge of the channel gains to each device. Consequently, the HAP does not know exactly the amount of energy harvested by each device. As a result, the HAP may program a sensing device with an incorrect sampling time. To address this problem, we employ stochastic programming, and use it to determine the time used for charging, and also the sampling time of each device. Its objective is to maximize the minimum sampling time of devices. The formulated stochastic program, however, requires a model or the probability distribution of channel gains. To this end, we propose a reinforcement learning (RL) approach to solve the same problem. In addition, as the state-space contains continuous quantities, we use linear function approximation and a set of novel features to represent the large state-space. Our experiment results show that the RL approach is able to achieve 93% of the minimum sampling time computed by the stochastic program. © 2017 IEEE.
KW  - actor-critic
KW  - reinforcement learning
KW  - sampling
KW  - Wireless charging
KW  - Internet of things
KW  - Machine learning
KW  - Probability distributions
KW  - Radio transmission
KW  - Sampling
KW  - Stochastic models
KW  - Stochastic programming
KW  - Stochastic systems
KW  - Actor critic
KW  - Continuous quantities
KW  - Imprecise knowledge
KW  - Internet of Things (IOT)
KW  - Linear functions
KW  - Radio frequency transmission
KW  - Sensing devices
KW  - Wireless charging
KW  - Reinforcement learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 24732400 (ISSN)
LA  - English
J2  - IEEE Trans. Green Commun. Networking
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 9; Correspondence Address: C. Yang; School of Computer Science, Zhongyuan University of Technology, Zhengzhou, 451191, China; email: changlin@zut.edu.cn
ER  -

TY  - CONF
AU  - Eliott, F.M.
AU  - Ribeiro, C.H.C.
TI  - A computational model for simulation of moral behavior
PY  - 2014
T2  - NCTA 2014 - Proceedings of the International Conference on Neural Computation Theory and Applications
SP  - 282
EP  - 287
DO  - 10.5220/0005139002820287
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908882609&doi=10.5220%2f0005139002820287&partnerID=40&md5=14e7bc16bb484af2dd4b32c5e900225f
AD  - Informatics, Technological Institute of Aeronautics, Praça Marechal Eduardo Gomes, São José dos Campos, São Paulo, Brazil
AB  - The extension of our integration to technologies brings about the possibility of inserting moral prototypes into artificial agents, no matter if they are going to interact with other artificial agents or biological creatures. We describe here MultiA, a computational model for simulating moral behavior derived from changes over a biologically inspired architecture. MultiA uses reinforcement learning techniques and is intended to produce selective cooperative behavior as a consequence of a biologically plausible model of morality inspired from a perusal of empathy. MultiA has its sensorial information translated into emotions and homeostatic variable values, which feed cognitive and learning systems. The moral behavior is expected to emerge from the artificial social emotion of sympathy and its associated feeling of empathy, based on an ability to internally emulate other agents internal states.
KW  - Artificial moral machine
KW  - Biologically inspired architecture
KW  - Reinforcement learning
KW  - Behavioral research
KW  - Biomimetics
KW  - Cognitive systems
KW  - Computational methods
KW  - Machine learning
KW  - Reinforcement learning
KW  - Artificial agents
KW  - Biologically inspired
KW  - Co-operative behaviors
KW  - Computational model
KW  - Plausible model
KW  - Reinforcement learning techniques
KW  - Sensorial information
KW  - Social emotions
KW  - Computation theory
A2  - Madani K.
A2  - Filipe J.
A2  - Filipe J.
PB  - INSTICC Press
SN  - 978-989758054-3 (ISBN)
LA  - English
J2  - NCTA - Proc. Int. Conf. Neural Comput. Theory Appl.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Conference name: 6th International Conference on Neural Computation Theory and Applications, NCTA 2014, Part of the 6th International Joint Conference on Computational Intelligence, IJCCI 2014; Conference date: 22 October 2014 through 24 October 2014; Conference code: 114694
ER  -

TY  - CONF
AU  - Gadanho, S.C.
AU  - Custódio, L.
TI  - Learning behavior-selection in a multi-goal robot task
PY  - 2003
T2  - Informatica (Ljubljana)
VL  - 27
IS  - 2
SP  - 175
EP  - 183
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041672178&partnerID=40&md5=2adf11247d2e6fea807e75dbe5d043cf
AD  - Institute of Systems and Robotics, IST, Lisbon, Portugal
AB  - The purpose of the work reported here is the development of an autonomous robot controller which learns to perform a multi-goal and multi-step task when faced with real world problems such as continuous time and space, noisy sensors and unreliable actuators. In order to make the learning task feasible, the agent does not have to learn its action abilities from scratch, but relies on a small set of simple hand-designed behaviors. Experience has shown that these low-level behaviors can be either easily designed or learned but that the coordination of these behaviors is not trivial. To solve the problem at hand, a dual-system architecture is proposed in which a traditional reinforcement learning adaptive system is complemented with a goal system responsible for both reinforcement and behavior switching. This goal system is inspired by emotions, which take a functional role on this work, and are evaluated in terms of their engineering benefits, i.e. in terms of their competitiveness when compared with alternative approaches. Experiments reported carefully evaluate the goal system and determine its requirements.
KW  - Autonomous robots
KW  - Emotions
KW  - Learning
KW  - Actuators
KW  - Artificial intelligence
KW  - Intelligent agents
KW  - Sensors
KW  - Robot controllers
KW  - Robots
SN  - 03505596 (ISSN)
LA  - English
J2  - Inf
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: S.C. Gadanho; Institute of Systems and Robotics, IST, Lisbon, Portugal; email: sandra@isr.ist.utl.pt; CODEN: INFOF
ER  -

TY  - JOUR
AU  - Shao, J.
AU  - Yang, J.
TI  - Multi-robot reinforcement learning based on learning classifier system with gradient descent methods
PY  - 2010
T2  - Journal of Computational Information Systems
VL  - 6
IS  - 8
SP  - 2449
EP  - 2455
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957805409&partnerID=40&md5=06d19fd1da32477b3347bb86d9683e5d
AD  - School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094, China
AB  - This paper proposed a robot reinforcement learning method based on learning classifier system. A Learning Classifier System is a accuracy-based machine learning system with gradient descent that combines reinforcement learning and rule discovery system. The genetic algorithm and the covering operator act as innovation discovery components which are responsible for discovering new better reinforcement learning rules. The reinforcement learning component is responsible for adjusting the fitness of rules in the system according to some reward obtained from the environment. The advantage of this approach is its accuracy-based representation, which can easily reduce learning space, improve online learning ability and robot robustness. Copyright © 2010 Binary Information Press.
KW  - Covering operator
KW  - Genetic algorithm
KW  - Gradient descent
KW  - Learning classifier system
KW  - Multi-robot
KW  - Reinforcement learning
KW  - Classifiers
KW  - Genetic algorithms
KW  - Industrial robots
KW  - Mathematical operators
KW  - Multipurpose robots
KW  - Reinforcement learning
KW  - Covering operator
KW  - Gradient descent
KW  - Gradient Descent method
KW  - Learning classifier system
KW  - Machine learning systems
KW  - Multirobots
KW  - Online learning abilities
KW  - Reinforcement learning method
KW  - Rule discovery
KW  - Learning algorithms
SN  - 15539105 (ISSN)
LA  - English
J2  - J. Comput. Inf. Syst.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 4; Correspondence Address: J. Shao; School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094, China; email: sj012328@163.com
ER  -

TY  - JOUR
AU  - Eliassi-Rad, T.
AU  - Shavlik, J.
TI  - A system for building intelligent agents that learn to retrieve and extract information
PY  - 2003
T2  - User Modelling and User-Adapted Interaction
VL  - 13
IS  - 1-2
SP  - 35
EP  - 88
DO  - 10.1023/A:1024009718142
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038548616&doi=10.1023%2fA%3a1024009718142&partnerID=40&md5=9258505c62c04d0db03ea9ca9f4bfa29
AD  - Ctr. for Appl. Scientific Computing, Lawrence Livermore Natl. Laboratory, Box 808, L-560, Livermore, CA 94551, United States
AD  - Computer Sciences Department, University of Wisconsin-Madison, Madison, WI 53706, 1210 West Dayton Street, United States
AB  - We present a system for rapidly and easily building instructable and self-adaptive software agents that retrieve and extract information. Our Wisconsin Adaptive Web Assistant (WAWA) constructs intelligent agents by accepting user preferences in the form of instructions. These user-provided instructions are compiled into neural networks that are responsible for the adaptive capabilities of an intelligent agent. The agent's neural networks are modified via user-provided and system-constructed training examples. Users can create training examples by rating Web pages (or documents), but more importantly WAWA'S agents uses techniques from reinforcement learning to internally create their own examples. Users can also provide additional instruction throughout the life of an agent. Our experimental evaluations on a 'home-page finder' agent and a 'seminar-announcement extractor' agent illustrate the value of using instructable and adaptive agents for retrieving and extracting information.
KW  - Information extraction
KW  - Information retrieval
KW  - Instructable and adaptive software agents
KW  - Machine learning
KW  - Neural networks
KW  - Web mining
KW  - Adaptive systems
KW  - Data mining
KW  - Information retrieval
KW  - Learning systems
KW  - Neural networks
KW  - Software agents
KW  - World Wide Web
KW  - Information extraction (IE)
KW  - Intelligent agents
SN  - 09241868 (ISSN)
LA  - English
J2  - User Modell User Adapt Interact
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 21; Correspondence Address: T. Eliassi-Rad; Ctr. for Appl. Scientific Computing, Lawrence Livermore Natl. Laboratory, Box 808, L-560, Livermore, CA 94551, United States; email: eliassi@llnl.gov; CODEN: UMUIE
ER  -

TY  - JOUR
AU  - Wauters, T.
AU  - Verbeeck, K.
AU  - Berghe, G.V.
AU  - De Causmaecker, P.
TI  - Learning agents for the multi-mode project scheduling problem
PY  - 2011
T2  - Journal of the Operational Research Society
VL  - 62
IS  - 2
SP  - 281
EP  - 290
DO  - 10.1057/jors.2010.101
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650408718&doi=10.1057%2fjors.2010.101&partnerID=40&md5=c0541e5eb853b31cfdac3b84f34ff77f
AD  - CODeS, Vakgroep IT, KaHo Sint-Lieven, 9000 Gent, Gebroeders Desmetstraat 1, Belgium
AD  - CODeS, K.U.Leuven, Belgium
AB  - Intelligent optimization refers to the promising technique of integrating learning mechanisms into (meta-)heuristic search. In this paper, we use multi-agent reinforcement learning for building high-quality solutions for the multi-mode resource-constrained project scheduling problem (MRCPSP). We use a network of distributed reinforcement learning agents that cooperate to jointly learn a well-performing constructive heuristic. Each agent, being responsible for one activity, uses two simple learning devices, called learning automata, that learn to select a successor activity order and a mode, respectively. By coupling the reward signals for both learning tasks, we can clearly show the advantage of using reinforcement learning in search. We present some comparative results, to show that our method can compete with the best performing algorithms for the MRCPSP, yet using only simple learning schemes without the burden of complex fine-tuning. © 2011 Operational Research Society Ltd.
KW  - learning automata
KW  - multi-agent reinforcement learning
KW  - project scheduling
KW  - Automata theory
KW  - Heuristic algorithms
KW  - Intelligent agents
KW  - Machine learning
KW  - Multi agent systems
KW  - Scheduling
KW  - Distributed reinforcement learning
KW  - High-quality solutions
KW  - Intelligent optimization
KW  - Learning Automata
KW  - Multi-agent reinforcement learning
KW  - Multi-mode resource-constrained project scheduling problem
KW  - Project scheduling
KW  - Project scheduling problem
KW  - Reinforcement learning
PB  - Palgrave Macmillan Ltd.
SN  - 01605682 (ISSN)
LA  - English
J2  - J.Oper.Res.Soc.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 33; Correspondence Address: T. Wauters; Vakgroep IT, KaHo Sint-Lieven, 9000 Gent, Gebroeders Desmetstraat 1, Belgium; email: tony.wauters@kahosl.be; CODEN: JORSD
ER  -

TY  - CONF
AU  - Shao, J.
AU  - Zhang, J.
AU  - Zhao, C.
TI  - Research on multi-robot path planning methods based on learning classifier system with gradient descent methods
PY  - 2012
T2  - Advances in Intelligent and Soft Computing
VL  - 169 AISC
IS  - VOL. 2
SP  - 229
EP  - 234
DO  - 10.1007/978-3-642-30223-7_37
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864312701&doi=10.1007%2f978-3-642-30223-7_37&partnerID=40&md5=9255cfc92e014860d50ef04f68757f07
AD  - School of Computer Science, Shangqiu Institute of Technology, Shangqiu 476000, China
AB  - This paper deals with the problem of multi-robot path planning based on learning classifier system in a dynamic narrow environment, where the workspace is cluttered with unpredictably moving objects. A Learning Classifier System is an accuracy-based machine learning system with gradient descent that combines reinforcement learning and rule discovery system. The genetic algorithm and the covering operator act as innovation discovery components which are responsible for discovering new better path planning rules. The reinforcement learning component is responsible for adjusting the fitness of rules in the system according to some reward obtained from the environment. The advantage of this approach is its accuracy-based representation, which can easily reduce learning space, improve online learning ability and robot robustness. © 2012 Springer-Verlag GmbH.
KW  - covering operator
KW  - Genetic algorithm
KW  - Gradient descent
KW  - Learning classifier system
KW  - Multi-robot
KW  - path planning
KW  - Computer science
KW  - Genetic algorithms
KW  - Reinforcement learning
KW  - covering operator
KW  - Gradient descent
KW  - Gradient Descent method
KW  - Learning classifier system
KW  - Moving objects
KW  - Multi-robot path planning
KW  - Multirobots
KW  - Narrow environment
KW  - Online learning
KW  - Rule discovery
KW  - Motion planning
SN  - 18675662 (ISSN); 978-364230222-0 (ISBN)
LA  - English
J2  - Adv. Intell. Soft Comput.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: J. Shao; School of Computer Science, Shangqiu Institute of Technology, Shangqiu 476000, China; email: sj012328@163.com; Conference name: Computer Science and Information Engineering, CSIE 2012; Conference date: 19 May 2012 through 20 May 2012; Conference code: 91392
ER  -

TY  - JOUR
AU  - Vale, Z.
AU  - Pinto, T.
AU  - Praça, I.
AU  - Morais, H.
TI  - MASCEM: Electricity markets simulation with strategic agents
PY  - 2011
T2  - IEEE Intelligent Systems
VL  - 26
IS  - 2
C7  - 5696716
SP  - 9
EP  - 17
DO  - 10.1109/MIS.2011.3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955396145&doi=10.1109%2fMIS.2011.3&partnerID=40&md5=b50387cdcbfb812ec143970e278155e4
AD  - Polytechnic Institute of Porto, Portugal
AB  - Electricity markets are complex environments, involving numerous entities trying to obtain the best advantages and profits while limited by power-network characteristics and constraints. This article proposes a new methodology integrated in MASCEM for bid definition in electricity markets. This methodology uses reinforcement learning algorithms to let players perceive changes in the environment, thus helping them react to the dynamic environment and adapt their bids accordingly. The system operator is usually responsible for managing the transmission grid and all the involved technical constraints. The market operator must assure that the economical dispatch accounts for the specified conditions, which might imply removing entities that have presented competitive bids but whose complex conditions were not satisfied. This result demonstrates that several algorithms can be combined with distinct characteristics.
KW  - electricity markets
KW  - intelligent agents
KW  - Intelligent systems
KW  - machine learning
KW  - modeling and prediction
KW  - multiagent systems
KW  - power systems
KW  - simulation support systems
KW  - Commerce
KW  - Electricity
KW  - Intelligent systems
KW  - Learning algorithms
KW  - Learning systems
KW  - Mathematical operators
KW  - Multi agent systems
KW  - Profitability
KW  - Electricity market
KW  - machine learning
KW  - modeling and prediction
KW  - power systems
KW  - simulation support systems
KW  - Intelligent agents
SN  - 15411672 (ISSN)
LA  - English
J2  - IEEE Intell. Syst.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 132; Correspondence Address: Z. Vale; Polytechnic Institute of Porto, Portugal; email: zav@isep.ipp.pt
ER  -

TY  - CONF
AU  - Paskaradevan, S.
AU  - Denzinger, J.
TI  - A hybrid cooperative behavior learning method for a rule-based shout-ahead architecture
PY  - 2012
T2  - Proceedings - 2012 IEEE/WIC/ACM International Conference on Intelligent Agent Technology, IAT 2012
VL  - 2
C7  - 6511580
SP  - 266
EP  - 273
DO  - 10.1109/WI-IAT.2012.33
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878448129&doi=10.1109%2fWI-IAT.2012.33&partnerID=40&md5=8abea3fcc0654345020e0de2f5dba9d6
AD  - Department of Computer Science, University of Calgary, Calgary, AB, Canada
AB  - We present an agent architecture and a hybrid behavior learning method for it that allows the use of communicated intentions of other agents to create agents that are able to cooperate with various configurations of other agents in fulfilling a task. Our shout-ahead architecture is based on two rule sets, one making decisions without communicated intentions and one with these intentions. Reinforcement learning is used to determine in a particular situation which set is responsible for the final decision. Evolutionary learning is used to learn these rules. Our application of this approach to learning behaviors for units in a computer game shows that the use of shout-ahead substantially improves the quality of the learned behavior compared to agents not using shout-ahead. © 2012 IEEE.
KW  - Artificial Intelligence
KW  - Cooperative Systems
KW  - Learning Systems
KW  - Artificial intelligence
KW  - Intelligent agents
KW  - Learning systems
KW  - Reinforcement learning
KW  - Agent architectures
KW  - Co-operative behaviors
KW  - Co-operative systems
KW  - Evolutionary Learning
KW  - Final decision
KW  - Hybrid behavior
KW  - Learning behavior
KW  - Making decision
KW  - Behavioral research
SN  - 978-076954880-7 (ISBN)
LA  - English
J2  - Proc. - IEEE/WIC/ACM Int. Conf. Intelligent Agent Technol., IAT
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 5; Conference name: 2012 IEEE/WIC/ACM International Conference on Intelligent Agent Technology, IAT 2012; Conference date: 4 December 2012 through 7 December 2012; Conference code: 97231
ER  -

TY  - CONF
AU  - Sotala, K.
TI  - Defining human values for value learners
PY  - 2016
T2  - AAAI Workshop - Technical Report
VL  - WS-16-01 - WS-16-15
SP  - 113
EP  - 123
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019229900&partnerID=40&md5=64dcd89a58e770409c34b10e439a9da4
AD  - Machine Intelligence Research Institute, Berkeley, 94704, CA, United States
AD  - Theiss Research, San Diego, 92037, CA, United States
AB  - Hypothetical "value learning" Als learn human values and then try to act according to those values. The design of such Als, however, is hampered by the fact that there exists no satisfactory definition of what exactly human values are. After arguing that the standard concept of preference is insufficient as a definition, I draw on reinforcement learning theory, emotion research, and moral psychology to offer an alternative definition. In this definition, human values are conceptualized as mental representations that encode the brain's value function (in the reinforcement learning sense) by being imbued with a context-sensitive affective gloss. I finish with a discussion of the implications that this hypothesis has on the design of value learners. Copyright © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
KW  - Artificial intelligence
KW  - Big data
KW  - Cognitive systems
KW  - Computer games
KW  - Computer programming
KW  - Computer systems programming
KW  - Data mining
KW  - Hybrid systems
KW  - Population statistics
KW  - Reinforcement learning
KW  - Context sensitive
KW  - Human values
KW  - Mental representations
KW  - Reinforcement Learning theories
KW  - Value functions
KW  - Education
PB  - AI Access Foundation
SN  - 978-157735759-9 (ISBN)
LA  - English
J2  - AAAI Workshop Tech. Rep.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 13; Correspondence Address: K. Sotala; Machine Intelligence Research Institute, Berkeley, 94704, United States; email: kaj.sotala@intelligence.org; Conference name: 30th AAAI Conference on Artificial Intelligence, AAAI 2016; Conference date: 12 February 2016 through 13 February 2016; Conference code: 128195
ER  -

TY  - CONF
AU  - Nagy, Z.
AU  - Park, J.Y.
AU  - Vazquez-Canteli, J.
TI  - Reinforcement learning for smart buildings and cities
PY  - 2017
T2  - Proceedings of 33rd PLEA International Conference: Design to Thrive, PLEA 2017
VL  - 2
SP  - 2618
EP  - 2624
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085912844&partnerID=40&md5=3419490f1b59e958180a440cae6d344b
AD  - Intelligent Environments Laboratory, Department of Civil, Architectural and Environmental Engineering, University of Texas at Austin, Austin, TX, United States
AB  - The built environment is responsible for about 30% of the anthropogenic greenhouse gas emissions, and, thus, offering a large reduction potential. Advances in information and communication technologies made widespread monitoring of environmental conditions and building operation possible, and have sparked the notion of smart buildings. The promise of smart buildings is that these large amounts of data can be used in advanced control algorithms to ensure optimal, energy efficient behavior. In this paper, we discuss a specific machine learning method called reinforcement learning (RL). RL is an agent based control method, in which the agent/controller performs actions in the environment for which it receives a reward. We argue that RL is particularly interesting for applications in the built environment. First, RL is a model-less approach, suited for large, complex models. Second, RL has been successful in applications where sequential decision making is important, e.g., in building and urban scale energy systems. Finally, due to its interactive nature, RL is the sole machine learning method capable of learning directly from humans, allowing the control system to adapt over time to occupants. We review key developments of RL in the built environment, and discuss the challenges and opportunities. Copyright © NCEUB 2017.
KW  - Artificial intelligence
KW  - Human - building interaction
KW  - Reinforcement learning
KW  - Smart buildings
KW  - Smart cities
KW  - Buildings
KW  - Decision making
KW  - Energy efficiency
KW  - Environmental technology
KW  - Gas emissions
KW  - Greenhouse gases
KW  - Reinforcement learning
KW  - Structural design
KW  - Advanced control algorithms
KW  - Anthropogenic greenhouse gas emissions
KW  - Environmental conditions
KW  - Information and Communication Technologies
KW  - Large amounts of data
KW  - Machine learning methods
KW  - Reduction potential
KW  - Sequential decision making
KW  - Learning systems
A2  - Brotas L.
A2  - Roaf S.
A2  - Nicol F.
PB  - NCEUB 2017 - Network for Comfort and Energy Use in Buildings
SN  - 978-099289575-4 (ISBN)
LA  - English
J2  - Proc. PLEA Int. Conf.: Des. Thrive, PLEA
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: Z. Nagy; Intelligent Environments Laboratory, Department of Civil, Architectural and Environmental Engineering, University of Texas at Austin, Austin, United States; email: nagy@utexas.edu; Conference name: 33rd International on Passive and Low Energy Architecture Conference: Design to Thrive, PLEA 2017; Conference date: 2 July 2017 through 5 July 2017; Conference code: 159932
ER  -

TY  - CONF
AU  - Ma, Y.B.
TI  - Robot reinforcement learning based on LCS-GDM
PY  - 2013
T2  - Applied Mechanics and Materials
VL  - 347-350
SP  - 416
EP  - 420
DO  - 10.4028/www.scientific.net/AMM.347-350.416
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883138007&doi=10.4028%2fwww.scientific.net%2fAMM.347-350.416&partnerID=40&md5=6a88afa2c5006daac4b10834e39e8988
AD  - School of computer Science, Shangqiu Institute of Technology, Shangqiu 476000, China
AB  - This paper proposed a robot reinforcement learning method based on learning classifier system. A learning Classifier System is a rule-based machine learning system that combines reinforcement learning and genetic algorithms. The reinforcement learning component is responsible for adjusting the strength of rules in the system according to some reward obtained from the environment. The genetic algorithm acts as an innovation discovery component which is responsible for discovering new better learning rules. The advantages of this approach are its rule-based representation, which can be easily reduce learning space, online learning ability, robustness. © 2013 Trans Tech Publications Ltd, Switzerland.
KW  - Genetic algorithm
KW  - Learning classifier system(LCS)
KW  - Reinforcement learning
KW  - Robot;gradient deescent methods(GDM)
KW  - Genetic algorithms
KW  - Reinforcement learning
KW  - Robots
KW  - Learning classifier system
KW  - Learning rules
KW  - Online learning
KW  - Reinforcement learning method
KW  - Rule based
KW  - Rule-based representations
KW  - E-learning
SN  - 16627482 (ISSN)
LA  - English
J2  - Appl. Mech. Mater.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: Y. Ma; School of computer Science, Shangqiu Institute of Technology, Shangqiu 476000, China; email: myb0513@163.com; Conference name: 2013 International Conference on Precision Mechanical Instruments and Measurement Technology, ICPMIMT 2013; Conference date: 25 May 2013 through 26 May 2013; Conference code: 99138
ER  -

TY  - JOUR
AU  - Mazumdar, J.
AU  - Harley, R.G.
TI  - Recurrent neural networks trained with backpropagation through time algorithm to estimate nonlinear load harmonic currents
PY  - 2008
T2  - IEEE Transactions on Industrial Electronics
VL  - 55
IS  - 9
SP  - 3484
EP  - 3491
DO  - 10.1109/TIE.2008.925315
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-51449114248&doi=10.1109%2fTIE.2008.925315&partnerID=40&md5=c4fbf7f77fa94b83226a2066464fbb11
AD  - Siemens Energy and Automation, Alpharetta, GA 30005, United States
AD  - School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, United States
AB  - Generation of harmonics and the existence of waveform pollution in power system networks are important problems facing the power utilities. The determination of harmonic currents injected into a power network by a nonlinear load is complicated when the supply voltage waveform to the load is distorted by other loads and not a pure sinusoid. This paper proposes a neural network solution to this problem. A recurrent neural network trained with the backpropagation through time training algorithm is used to find a way of distinguishing between the so-called load harmonics and supply harmonics, without disconnecting the load from the network. The advantage of this method is that only waveforms of voltages and currents have to be measured. This method is applicable for both single and three phase loads and could be fabricated into a commercial instrument that could be installed in substations of large customer loads, or used as a hand-held clip on instrument. This paper is particularly useful in determining whether the utility or the customer side has a higher contribution to harmonic pollution in a network. Hence, this method would be helpful in settling utility-customer disputes over who is responsible for harmonic pollution. © 2008 IEEE.
KW  - Harmonics
KW  - Metering
KW  - Power quality
KW  - Recurrent neural networks (RNNs)
KW  - Active filters
KW  - Artificial intelligence
KW  - Backpropagation
KW  - Backpropagation algorithms
KW  - Boolean functions
KW  - Computer networks
KW  - Customer satisfaction
KW  - Electric power systems
KW  - Electric power transmission networks
KW  - Harmonic analysis
KW  - Image classification
KW  - Pollution
KW  - Reinforcement learning
KW  - Sales
KW  - Systems analysis
KW  - Vegetation
KW  - Harmonics
KW  - Metering
KW  - Neural networks
KW  - Power quality
KW  - Recurrent neural networks (RNNs)
KW  - Recurrent neural networks
SN  - 02780046 (ISSN)
LA  - English
J2  - IEEE Trans Ind Electron
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 94; Correspondence Address: J. Mazumdar; Siemens Energy and Automation, Alpharetta, GA 30005, United States; email: joy.mazumdar@siemens.com; CODEN: ITIED
ER  -

TY  - CONF
AU  - Sutton, R.S.
AU  - Modayil, J.
AU  - Degris, M.D.T.
AU  - Pilarski, P.M.
AU  - White, A.
AU  - Precup, D.
TI  - Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction
PY  - 2011
T2  - 10th International Conference on Autonomous Agents and Multiagent Systems 2011, AAMAS 2011
VL  - 2
SP  - 713
EP  - 720
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899464022&partnerID=40&md5=ef4003d27a07a96e3868adb18f2ccf82
AD  - Reinforcement Learning and Artificial Intelligence Laboratory, Department of Computing Science, University of Alberta, Canada
AD  - School of Computer Science, McGill University, Montreal, Canada
AB  - Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other artificial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a single predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the system's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, reward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever actions are taken by the system as a whole. Gradient-based temporal-difference learning methods are used to learn efficiently and reliably with function approximation in this off-policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in realtime applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from off-policy experience. Horde is a significant incremental step towards a real-time architecture for efficient learning of general knowledge from unsupervised sensorimotor interaction. Copyright © 2011, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.
KW  - Artificial intelligence
KW  - Knowledge representation
KW  - Off-policy learning
KW  - Real-time
KW  - Reinforcement learning
KW  - Robotics
KW  - Temporal-difference learning
KW  - Value function approximation
KW  - Artificial intelligence
KW  - Autonomous agents
KW  - Knowledge representation
KW  - Multi agent systems
KW  - Robotics
KW  - Artificial intelligence systems
KW  - Function approximation
KW  - Goal-oriented behavior
KW  - Off-policy learning
KW  - Real-time
KW  - Real-time architecture
KW  - Temporal difference learning
KW  - Value function approximation
KW  - Reinforcement learning
PB  - International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)
LA  - English
J2  - Int. Conf. Auton. Agents Multiagent Syst., AAMAS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 238; Conference name: 10th International Conference on Autonomous Agents and Multiagent Systems 2011, AAMAS 2011; Conference date: 2 May 2011 through 6 May 2011; Conference code: 104775
ER  -

TY  - CONF
AU  - Kudenko, D.
AU  - Grzes, M.
TI  - Knowledge-based reinforcement learning for data mining
PY  - 2009
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 5680 LNAI
SP  - 21
EP  - 22
DO  - 10.1007/978-3-642-03603-3_2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-69949163700&doi=10.1007%2f978-3-642-03603-3_2&partnerID=40&md5=0aeff14970b91841a3373def5bc67308
AD  - Department of Computer Science, University of York, York YO105DD, United Kingdom
AB  - Data Mining is the process of extracting patterns from data. Two general avenues of research in the intersecting areas of agents and data mining can be distinguished. The first approach is concerned with mining an agent's observation data in order to extract patterns, categorize environment states, and/or make predictions of future states. In this setting, data is normally available as a batch, and the agent's actions and goals are often independent of the data mining task. The data collection is mainly considered as a side effect of the agent's activities. Machine learning techniques applied in such situations fall into the class of supervised learning. In contrast, the second scenario occurs where an agent is actively performing the data mining, and is responsible for the data collection itself. For example, a mobile network agent is acquiring and processing data (where the acquisition may incur a certain cost), or a mobile sensor agent is moving in a (perhaps hostile) environment, collecting and processing sensor readings. In these settings, the tasks of the agent and the data mining are highly intertwined and interdependent (or even identical). Supervised learning is not a suitable technique for these cases. Reinforcement Learning (RL) enables an agent to learn from experience (in form of reward and punishment for explorative actions) and adapt to new situations, without a teacher. RL is an ideal learning technique for these data mining scenarios, because it fits the agent paradigm of continuous sensing and acting, and the RL agent is able to learn to make decisions on the sampling of the environment which provides the data. Nevertheless, RL still suffers from scalability problems, which have prevented its successful use in many complex real-world domains. The more complex the tasks, the longer it takes a reinforcement learning algorithm to converge to a good solution. For many real-world tasks, human expert knowledge is available. For example, human experts have developed heuristics that help them in planning and scheduling resources in their work place. However, this domain knowledge is often rough and incomplete. When the domain knowledge is used directly by an automated expert system, the solutions are often sub-optimal, due to the incompleteness of the knowledge, the uncertainty of environments, and the possibility to encounter unexpected situations. RL, on the other hand, can overcome the weaknesses of the heuristic domain knowledge and produce optimal solutions. In the talk we propose two techniques, which represent first steps in the area of knowledge-based RL (KBRL). The first technique [1] uses high-level STRIPS operator knowledge in reward shaping to focus the search for the optimal policy. Empirical results show that the plan-based reward shaping approach outperforms other RL techniques, including alternative manual and MDP-based reward shaping when it is used in its basic form. We showed that MDP-based reward shaping may fail and successful experiments with STRIPS-based shaping suggest modifications which can overcome encountered problems. The STRIPSbased method we propose allows expressing the same domain knowledge in a different way and the domain expert can choose whether to define an MDP or STRIPS planning task. We also evaluated the robustness of the proposed STRIPS-based technique to errors in the plan knowledge. In case that STRIPS knowledge is not available, we propose a second technique [2] that shapes the reward with hierarchical tile coding. Where the Q-function is represented with low-level tile coding, a V-function with coarser tile coding can be learned in parallel and used to approximate the potential for ground states. In the context of data mining, our KBRL approaches can also be used for any data collection task where the acquisition of data may incur considerable cost. In addition, observing the data collection agent in specific scenarios may lead to new insights into optimal data collection behaviour in the respective domains. In future work, we intend to demonstrate and evaluate our techniques on concrete real-world data mining applications. © 2009 Springer Berlin Heidelberg.
KW  - Data acquisition
KW  - Education
KW  - Expert systems
KW  - Heuristic methods
KW  - Learning algorithms
KW  - Mathematical operators
KW  - Mergers and acquisitions
KW  - Motion planning
KW  - Optimization
KW  - Reinforcement
KW  - Reinforcement learning
KW  - Sensor networks
KW  - Sensors
KW  - Supervised learning
KW  - Agent paradigm
KW  - Automated expert systems
KW  - Continuous sensing
KW  - Data collection
KW  - Data mining tasks
KW  - Domain experts
KW  - Domain knowledge
KW  - Empirical results
KW  - Environment state
KW  - Human expert
KW  - Human expert knowledge
KW  - Learning techniques
KW  - Machine learning techniques
KW  - Mobile networks
KW  - Mobile sensor agents
KW  - Observation data
KW  - Optimal data
KW  - Optimal policies
KW  - Optimal solutions
KW  - Plan-based
KW  - Planning and scheduling
KW  - Q-functions
KW  - Real world data
KW  - Real world domain
KW  - Real-world task
KW  - Scalability problems
KW  - Sensor readings
KW  - Side effect
KW  - STRIPS planning
KW  - Tile coding
KW  - Work place
KW  - Data mining
SN  - 16113349 (ISSN); 3642036023 (ISBN); 978-364203602-6 (ISBN)
LA  - English
J2  - Lect. Notes Comput. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: D. Kudenko; Department of Computer Science, University of York, York YO105DD, United Kingdom; email: kudenko@cs.york.ac.uk; Conference name: 4th International Workshop on Agents and Data Mining Interaction, ADMI 2009; Conference date: 10 May 2009 through 15 May 2009; Conference code: 77101
ER  -

TY  - CONF
AU  - Beck, J.E.
AU  - Woolf, B.P.
AU  - Beal, C.R.
TI  - ADVISOR: A Machine Learning Architecture for Intelligent Tutor Construction
PY  - 2000
T2  - Proceedings of the 17th National Conference on Artificial Intelligence and 12th Conference on Innovative Applications of Artificial Intelligence, AAAI 2000
SP  - 552
EP  - 557
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077012858&partnerID=40&md5=1fc1824c97550ad6b6a56e8ee1e5bfe2
AD  - Computer Science Department, University of Massachusetts, Amherst, 01003, MA, United States
AD  - Psychology Department, University of Massachusetts, Amherst, 01003, MA, United States
AB  - We have constructed ADVISOR, a two-agent machine learning architecture for intelligent tutoring systems (ITS). The purpose of this architecture is to centralize the reasoning of an ITS into a single component to allow customization of teaching goals and to simplify improving the ITS. The first agent is responsible for learning a model of how students perform using the tutor in a variety of contexts. The second agent is provided this model of student behavior and a goal specifying the desired educational objective. Reinforcement learning is used by this agent to derive a teaching policy that meets the specified educational goal. Component evaluation studies show each agent performs adequately in isolation. We have also conducted an evaluation with actual students of the complete architecture. Results show ADVISOR was successful in learning a teaching policy that met the educational objective provided. Although this set of machine learning agents has been integrated with a specific intelligent tutor, the general technique could be applied to a broad class of ITS. Copyright © 2000, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.
KW  - Architecture
KW  - Computer aided instruction
KW  - Education computing
KW  - Intelligent agents
KW  - Intelligent vehicle highway systems
KW  - Reinforcement learning
KW  - Teaching
KW  - Customisation
KW  - Educational objectives
KW  - Intelligent tutoring
KW  - Intelligent tutors
KW  - Learning architectures
KW  - Machine-learning
KW  - Single components
KW  - Students' behaviors
KW  - Tutoring system
KW  - Two agents
KW  - Students
PB  - AAAI Press
SN  - 0262511126 (ISBN); 978-026251112-4 (ISBN)
LA  - English
J2  - Proc. Natl. Conf. Artif. Intell. Conf. Innov. Appl. Artif. Intell., AAAI
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 78; Conference name: 17th National Conference on Artificial Intelligence, AAA1 2000; Conference date: 30 July 2000 through 3 August 2000; Conference code: 187902
ER  -

TY  - JOUR
AU  - Pinto, T.
AU  - Vale, Z.
TI  - Adaptive learning in games: Defining profiles of competitor players
PY  - 2013
T2  - Advances in Intelligent Systems and Computing
VL  - 217
SP  - 351
EP  - 359
DO  - 10.1007/978-3-319-00551-5_43
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891068742&doi=10.1007%2f978-3-319-00551-5_43&partnerID=40&md5=dea1f6ce7ef538075781f1e2973c9c5c
AD  - GECAD - Knowledge Engineering and Decision-Support Research Center, Institute of Engineering - Politechnic of Porto (ISEP/IPP), Porto, Portugal
AB  - Artificial Intelligence has been applied to dynamic games for many years. The ultimate goal is creating responses in virtual entities that display human-like reasoning in the definition of their behaviors. However, virtual entities that can be mistaken for real persons are yet very far from being fully achieved. This paper presents an adaptive learning based methodology for the definition of players' profiles, with the purpose of supporting decisions of virtual entities. The proposed methodology is based on reinforcement learning algorithms, which are responsible for choosing, along the time, with the gathering of experience, the most appropriate from a set of different learning approaches. These learning approaches have very distinct natures, from mathematical to artificial intelligence and data analysis methodologies, so that the methodology is prepared for very distinct situations. This way it is equipped with a variety of tools that individually can be useful for each encountered situation. The proposed methodology is tested firstly on two simpler computer versus human player games: the rock-paperscissors game, and a penalty-shootout simulation. Finally, the methodology is applied to the definition of action profiles of electricity market players; players that compete in a dynamic game-wise environment, in which the main goal is the achievement of the highest possible profits in the market. © Springer International Publishing Switzerland 2013.
KW  - Adaptive learning
KW  - Artificial Intelligence
KW  - Player profiles
KW  - Artificial intelligence
KW  - Computer simulation
KW  - Reinforcement learning
KW  - Adaptive learning
KW  - Dynamic game
KW  - Electricity market
KW  - Human players
KW  - Learning approach
KW  - Player profiles
KW  - Virtual entities
KW  - Computer games
PB  - Springer Verlag
SN  - 21945357 (ISSN); 978-331900550-8 (ISBN)
LA  - English
J2  - Adv. Intell. Sys. Comput.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Conference name: 10th International Symposium on Distributed Computing and Artificial Intelligence 2013, DCAI 2013; Conference date: 22 May 2013 through 24 May 2013; Conference code: 101723
ER  -

TY  - CONF
AU  - Shao, J.
AU  - Yang, J.-Y.
TI  - Robot reinforcement learning based on learning classifier system
PY  - 2010
T2  - Communications in Computer and Information Science
VL  - 93 CCIS
SP  - 200
EP  - 207
DO  - 10.1007/978-3-642-14831-6_27
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956990484&doi=10.1007%2f978-3-642-14831-6_27&partnerID=40&md5=379a8f586837a3251637b038b321efba
AD  - School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094, China
AB  - This paper proposed a robot reinforcement learning method based on learning classifier system. A learning Classifier System is a rule-based machine learning system that combines reinforcement learning and genetic algorithms. The reinforcement learning component is responsible for adjusting the strength of rules in the system according to some reward obtained from the environment. The genetic algorithm acts as an innovation discovery component which is responsible for discovering new better learning rules. The advantages of this approach are its rule-based representation, which can easily reduce learning space, improve online learning ability and robustness. © 2010 Springer-Verlag Berlin Heidelberg.
KW  - Genetic algorithm
KW  - Learning classifier system (LCS)
KW  - Reinforcement learning
KW  - Robot
KW  - Classifiers
KW  - Computation theory
KW  - Genetic algorithms
KW  - Intelligent computing
KW  - Reinforcement learning
KW  - Robots
KW  - Learning classifier system
KW  - Learning rules
KW  - Machine learning systems
KW  - Online learning abilities
KW  - Reinforcement learning method
KW  - Rule based
KW  - Learning algorithms
SN  - 18650929 (ISSN); 3642148301 (ISBN); 978-364214830-9 (ISBN)
LA  - English
J2  - Commun. Comput. Info. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Correspondence Address: J. Shao; School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094, China; email: sj012328@163.com; Conference name: 6th International Conference on Intelligent Computing, ICIC 2010; Conference date: 18 August 2010 through 21 August 2010; Conference code: 81746
ER  -

TY  - JOUR
AU  - Ling, K.
AU  - Shalaby, A.
TI  - Automated transit headway control via adaptive signal priority
PY  - 2004
T2  - Journal of Advanced Transportation
VL  - 38
IS  - 1
SP  - 45
EP  - 67
DO  - 10.1002/atr.5670380105
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-2342650082&doi=10.1002%2fatr.5670380105&partnerID=40&md5=831a49c564142fae755fbec80d128bb0
AD  - Department of Civil Engineering, University of Toronto, Toronto, Ont., Canada
AB  - This paper reports on a study that developed a next-generation Transit Signal Priority (TSP) strategy, Adaptive TSP, that controls adaptively transit operations of high frequency routes using traffic signals, thus automating the operations control task and relieving transit agencies of this burden. The underlying algorithm is based on Reinforcement Learning (RL), an emerging Artificial Intelligence method. The developed RL agent is responsible for determining the best duration of each signal phase such that transit vehicles can recover to the scheduled headway taking into consideration practical phase length constraints. A case study was carried out by employing the microscopic traffic simulation software Paramics to simulate transit and traffic operations at one signalized intersection along the King Streetcar route in downtown Toronto. The results show that the control policy learned by the agent could effectively reduce the transit headway deviation and causes smaller disruption to cross street traffic compared with the existing unconditional transit signal priority algorithm.
KW  - Algorithms
KW  - Artificial intelligence
KW  - Highway traffic control
KW  - Operations research
KW  - Public policy
KW  - Scheduling
KW  - Traffic signals
KW  - Control strategies
KW  - Signal controllers
KW  - Toronto, Canada
KW  - Transit Signal Priority (TSP)
KW  - Intelligent vehicle highway systems
PB  - Institute for Transportation
SN  - 01976729 (ISSN)
LA  - English
J2  - J Adv Transp
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 44; CODEN: JATRD
ER  -

TY  - JOUR
AU  - de Lope, J.
AU  - Maravall, D.
AU  - Quiñonez, Y.
TI  - Self-organizing techniques to improve the decentralized multi-task distribution in multi-robot systems
PY  - 2015
T2  - Neurocomputing
VL  - 163
SP  - 47
EP  - 55
DO  - 10.1016/j.neucom.2014.08.094
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930277058&doi=10.1016%2fj.neucom.2014.08.094&partnerID=40&md5=cbb15b2bdb25e14a708a2fa60284e717
AD  - Centro de Automática y Robótica (UPM - CSIC), Universidad Politécnica de Madrid, Madrid, Spain
AD  - Facultad de Informática Mazatlán, Universidad Autónoma de Sinaloa, Sinaloa, Mexico
AB  - This paper focuses on the general problem of coordinating multiple robots, in particular, addresses the problem of the distribution of heterogeneous multi-task in a robust and efficient manner. The main interest in these systems is to understand how from simple rules inspired by the division of labor in social insects, a group of robots can perform tasks in an organized and coordinated way. We take into account a specifically distributed or decentralized approach as we are particularly interested in experimenting with truly autonomous and decentralized techniques in which the robots themselves are responsible for choosing a particular task in an autonomous and individual way. Under this approach we can speak of multi-task selection instead of multi-task assignment, which means, that the agents or robots select the tasks instead of being assigned a task by a central controller. In this regard, we have established an experimental scenario to solve the corresponding multi-task distribution problem and we propose a solution using different approaches by applying the response threshold models inspired by division of labor in social insects, the application of the reinforcement learning algorithm based on learning automata theory and ant colony optimization-based deterministic algorithms. We have evaluated the robustness of the algorithms, perturbing the number of pending loads to simulate the robot's error in estimating the real number of pending tasks and also the dynamic generation of loads through time. The paper ends with a critical discussion of experimental results. © 2015 Elsevier B.V.
KW  - Ant colony optimization
KW  - Bio-inspired threshold models
KW  - Multi-robot systems
KW  - Multi-task distribution
KW  - Self-coordination of multiple robots
KW  - Stochastic learning automata
KW  - Ant colony optimization
KW  - Automata theory
KW  - Biomimetics
KW  - Industrial robots
KW  - Multipurpose robots
KW  - Reinforcement learning
KW  - Robot learning
KW  - Stochastic models
KW  - Stochastic systems
KW  - Critical discussions
KW  - Decentralized approach
KW  - Deterministic algorithms
KW  - Learning automata theories
KW  - Multi-robot systems
KW  - Self-Coordination of Multiple Robots
KW  - Stochastic learning automata
KW  - Threshold model
KW  - ant
KW  - ant colony optimization
KW  - Article
KW  - artificial intelligence
KW  - classification algorithm
KW  - computer simulation
KW  - information processing
KW  - learning algorithm
KW  - mathematical analysis
KW  - multi robot system
KW  - multi task distribution
KW  - priority journal
KW  - probability
KW  - process optimization
KW  - reinforcement
KW  - self organizing technique
KW  - stochastic model
KW  - Learning algorithms
PB  - Elsevier B.V.
SN  - 09252312 (ISSN)
LA  - English
J2  - Neurocomputing
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 31; CODEN: NRCGE
ER  -

TY  - CONF
AU  - Arnold, T.
AU  - Kasenberg, D.
AU  - Scheutz, M.
TI  - Value alignment or misalignment - What will keep systems accountable?
PY  - 2017
T2  - AAAI Workshop - Technical Report
VL  - WS-17-01 - WS-17-15
SP  - 81
EP  - 88
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045276127&partnerID=40&md5=fa6c749c157a02113fb2891967a225da
AD  - Department of Computer Science, Tufts University, Medford, 02155, MA, United States
AB  - Machine learning's advances have led to new ideas about the feasibility and importance of machine ethics keeping pace, with increasing emphasis on safety, containment, and alignment. This paper addresses a recent suggestion that inverse reinforcement learning (IRL) could be a means to so-called "value alignment." We critically consider how such an approach can engage the social, norm-infused nature of ethical action and oudine several features of ethical appraisal that go beyond simple models of behavior, including unavoidably temporal dimensions of norms and counterfactuals. We propose that a hybrid approach for computational architectures still offers the most promising avenue for machines acting in an ethical fashion. © Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
KW  - Alignment
KW  - Computer games
KW  - Deep learning
KW  - Inverse problems
KW  - Knowledge based systems
KW  - Operations research
KW  - Philosophical aspects
KW  - Reinforcement learning
KW  - Computational architecture
KW  - Counterfactuals
KW  - Hybrid approach
KW  - Inverse reinforcement learning
KW  - Temporal dimensions
KW  - Problem solving
PB  - AI Access Foundation
SN  - 978-157735786-5 (ISBN)
LA  - English
J2  - AAAI Workshop Tech. Rep.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 63; Conference name: 31st AAAI Conference on Artificial Intelligence, AAAI 2017; Conference date: 4 February 2017 through 5 February 2017; Conference code: 135573
ER  -

TY  - CONF
AU  - Kofinas, P.
AU  - Vouros, G.
AU  - Dounis, A.I.
TI  - Energy management in solar microgrid via reinforcement learning
PY  - 2016
T2  - ACM International Conference Proceeding Series
VL  - 18-20-May-2016
C7  - a12
DO  - 10.1145/2903220.2903257
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986593794&doi=10.1145%2f2903220.2903257&partnerID=40&md5=71d503062ae54d3dc9a3fbcdb247dca9
AD  - University of Piraeus, Department of Digital Systems, 80, Karaoli and Dimitriou Str, Piraeus, 18534, Greece
AD  - Piraeus University of Applied Sciences (T.E.I. of Piraeus), Department of Automation Engineering, 250, Thivon and P. Ralli Str., Egaleo - Athens, 12244, Greece
AB  - This paper proposes a single agent system towards solving energy management issues in solar microgrids. The system considered consists of a Photovoltaic (PV) source, a battery bank, a desalination unit (responsible for providing the demanded water) and a local consumer. The trade-offs and complexities involved in the operation of the different units, and the quality of services' demanded from energy consumer units (e.g. the desalination unit), makes the energy management a challenging task. The goal of the agent is to satisfy the energy demand in the solar microgrid, optimizing the battery usage, in conjunction to satisfying the quality of services provided. It is assumed that the solar microgrid operates in island-mode. Thus, no connection to the electrical grid is considered. The agent collects data from the elements of the system and learns the suitable policy towards optimizing system performance. Simulation results provided, show the performance of the agent. © 2016 ACM.
KW  - Energy management
KW  - Microgrid
KW  - Q-learning
KW  - Reinforcement learning
KW  - Artificial intelligence
KW  - Desalination
KW  - Economic and social effects
KW  - Electric batteries
KW  - Energy management
KW  - Battery banks
KW  - Desalination units
KW  - Electrical grids
KW  - Energy consumer
KW  - Energy demands
KW  - Micro grid
KW  - Q-learning
KW  - Single-agent
KW  - Reinforcement learning
A2  - Bikakis A.
A2  - Vrakas D.
A2  - Bassiliades N.
A2  - Vlahavas I.
A2  - Vouros G.
PB  - Association for Computing Machinery
SN  - 978-145033734-2 (ISBN)
LA  - English
J2  - ACM Int. Conf. Proc. Ser.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 4; Conference name: 9th Hellenic Conference on Artificial Intelligence, SETN 2016; Conference date: 18 May 2016 through 20 May 2016; Conference code: 121811
ER  -

TY  - CONF
AU  - Shao, J.
AU  - Lin, H.
AU  - Zhang, K.
TI  - Swarm robots reinforcement learning convergence Accuracy-based learning classifier systems with Gradient descent (XCS-GD)
PY  - 2014
T2  - Proceedings of 2013 3rd International Conference on Computer Science and Network Technology, ICCSNT 2013
C7  - 6967341
SP  - 1306
EP  - 1309
DO  - 10.1109/ICCSNT.2013.6967341
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919459581&doi=10.1109%2fICCSNT.2013.6967341&partnerID=40&md5=18356258cee6e34c442b4eda76732d4d
AD  - Zhengzhou Chenggong University of Finance and Economics, Department of Information Engineering, Zhengzhou, 451200, China
AB  - This paper presented a novel approach XCS-GD to research on swarm robots reinforcement learning convergence. XCS-GD combines covering operator and genetic algorithm. XCS-GD is responsible for adjusting precision and reducing search space according to some reward obtained from the environment, XCS-GD's innovation discovery component is responsible for discovering new better reinforcement learning rules. The experiment and simulation showed that XCS-GD approach can achieved convergence very quickly in swarm robots reinforcement learning. © 2013 IEEE.
KW  - Accuracy-based learning classifier system with Gradient descent (XCS-GD)
KW  - Convergence
KW  - Fitness function
KW  - Reinforcement learning
KW  - swarm robots
KW  - Clustering algorithms
KW  - Data communication equipment
KW  - Genetic algorithms
KW  - Gradient methods
KW  - Machine learning
KW  - Reinforcement learning
KW  - Robots
KW  - Convergence
KW  - Fitness functions
KW  - Gradient descent
KW  - Learning classifier system
KW  - Search spaces
KW  - Swarm robots
KW  - Swarm intelligence
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-147990561-4 (ISBN)
LA  - English
J2  - Proc. Int. Conf. Comput. Sci. Netw. Technol., ICCSNT
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2; Conference name: 2013 3rd International Conference on Computer Science and Network Technology, ICCSNT 2013; Conference date: 12 October 2013 through 13 October 2013; Conference code: 109493
ER  -

TY  - CONF
AU  - Quiñonez, Y.
AU  - Maravall, D.
AU  - De Lope, J.
TI  - Stochastic learning automata for self-coordination in heterogeneous multi-tasks selection in multi-robot systems
PY  - 2011
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 7094 LNAI
IS  - PART 1
SP  - 443
EP  - 453
DO  - 10.1007/978-3-642-25324-9_38
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-82555183998&doi=10.1007%2f978-3-642-25324-9_38&partnerID=40&md5=a4ac17c4ca1a629fa4f7a6e6211d9507
AD  - Computational Cognitive Robotics, Dept. Artificial Intelligence, Universidad Politécnica de Madrid, Spain
AB  - This paper focuses on the general problem of coordinating multiple robots. More specifically, it addresses the self-election of heterogeneous specialized tasks by autonomous robots, as opposed to the usual multi-tasks allocation problem in multi-robot systems in which an external controller distributes the existing tasks among the individual robots. In this work we are considering a specifically distributed or decentralized approach in which we are particularly interested on decentralized solution where the robots themselves autonomously and in an individual manner, are responsible of selecting a particular task so that all the existing tasks are optimally distributed and executed. In this regard, we have established an experimental scenario and we propose a solution through automata learning-based probabilistic algorithm, to solve the corresponding multi-tasks distribution problem. The paper ends with a critical discussion of experimental results. © 2011 Springer-Verlag.
KW  - Multi-Heterogeneous Specialized Tasks Distribution
KW  - Multi-robot Systems
KW  - Multi-tasks Distribution
KW  - Reinforcement Learning
KW  - Self-Coordination of Multiple Robots
KW  - Stochastic Learning Automata
KW  - Artificial intelligence
KW  - Automata theory
KW  - Industrial robots
KW  - Multipurpose robots
KW  - Probability distributions
KW  - Reinforcement learning
KW  - Robot learning
KW  - Stochastic systems
KW  - Multi-Heterogeneous Specialized Tasks Distribution
KW  - Multi-robot systems
KW  - Multi-tasks Distribution
KW  - Multiple robot
KW  - Stochastic learning automata
KW  - Intelligent robots
SN  - 16113349 (ISSN); 978-364225323-2 (ISBN)
LA  - English
J2  - Lect. Notes Comput. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 9; Correspondence Address: Y. Quiñonez; Computational Cognitive Robotics, Dept. Artificial Intelligence, Universidad Politécnica de Madrid, Spain; email: ay.quinonez@alumnos.upm.es; Conference name: 10th Mexican International Conference on Artificial Intelligence, MICAI 2011; Conference date: 26 November 2011 through 4 December 2011; Conference code: 87491
ER  -

TY  - JOUR
AU  - Rahaie, Z.
AU  - Beigy, H.
TI  - Expertness framework in multi-agent systems and its application in credit assignment problem
PY  - 2014
T2  - Intelligent Data Analysis
VL  - 18
IS  - 3
SP  - 511
EP  - 528
DO  - 10.3233/IDA-140654
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902250713&doi=10.3233%2fIDA-140654&partnerID=40&md5=c97dc89b166a28f090c2627d5deb140a
AD  - Intelligent Systems Laboratory, Department of Computer Engineering, Sharif University of Technology, Tehran, Iran
AB  - One of the challenging problems in artificial intelligence is credit assignment which simply means distributing the credit among a group, such as a group of agents. We made an attempt to meet this problem with the aid of the reinforcement learning paradigm. In this paper, expertness framework is defined and applied to the multi-agent credit assignment problem. In the expertness framework, the critic agent, who is responsible for distributing credit among agents, is equipped with learning capability, and the proposed credit assignment solution is based on the critic to learn to assign a proportion of the credit to each agent, and the used proportion should be learned by reinforcement learning. The paper also reports the degree of expertness framework robustness and the amount of performance decline in noisy environments. Experimental results show the superiority of the method over the common methods of credit assignment used in lots of different domains and also show that performance reduction with respect to the quantity of the noise is tolerable and the system ultimately converges to the stable and correct behavior, therefore the agents are still capable of efficiently performing in the noisy environments. © 2014 - IOS Press and the authors. All rights reserved.
KW  - cooperative learning
KW  - Credit assignment
KW  - critic learning
KW  - expertness framework
KW  - multi-agent systems
KW  - noise
KW  - Combinatorial optimization
KW  - Reinforcement learning
KW  - Cooperative learning
KW  - Credit assignment
KW  - critic learning
KW  - expertness framework
KW  - noise
KW  - Multi agent systems
PB  - IOS Press
SN  - 1088467X (ISSN)
LA  - English
J2  - Intell. Data Anal.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 6; Correspondence Address: Z. Rahaie; Intelligent Systems Laboratory, Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; email: z_rahaie@ce.sharif.edu
ER  -

TY  - JOUR
AU  - Bonneau, M.
AU  - Gaba, S.
AU  - Peyrard, N.
AU  - Sabbadin, R.
TI  - Reinforcement learning-based design of sampling policies under cost constraints in Markov random fields: Application to weed map reconstruction
PY  - 2014
T2  - Computational Statistics and Data Analysis
VL  - 72
SP  - 30
EP  - 44
DO  - 10.1016/j.csda.2013.10.002
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888114909&doi=10.1016%2fj.csda.2013.10.002&partnerID=40&md5=e9c6221fe9fdee9bd5b999b71aafbbe4
AD  - INRA - UR875, Applied Mathematics and Computer Science Unit, Auzeville - CS 52627, 31326 Castanet Tolosan cedex, 24 Chemin de Borde Rouge, France
AD  - INRA - UMR 1347, Agroécologie, F-21065 Dijon Cedex, 17 rue Sully, France
AB  - Weeds are responsible for yield losses in arable fields, whereas the role of weeds in agro-ecosystem food webs and in providing ecological services has been well established. Innovative weed management policies have to be designed to handle this trade-off between production and regulation services. As a consequence, there has been a growing interest in the study of the spatial distribution of weeds in crops, as a prerequisite to management. Such studies are usually based on maps of weed species. The issues involved in building probabilistic models of spatial processes as well as plausible maps of the process on the basis of models and observed data are frequently encountered and important. As important is the question of designing optimal sampling policies that make it possible to build maps of high probability when the model is known. This optimization problem is more complex to solve than the pure reconstruction problem and cannot generally be solved exactly. A generic approach to spatial sampling for optimizing map construction, based on Markov Random Fields (MRF), is provided and applied to the problem of weed sampling for mapping. MRF offer a powerful representation for reasoning on large sets of random variables in interaction. In the field of spatial statistics, the design of sampling policies has been largely studied in the case of continuous variables, using tools from the geostatistics domain. In the MRF case with finite state space variables, some heuristics have been proposed for the design problem but no universally accepted solution exists, particularly when considering adaptive policies as opposed to static ones. The problem of designing an adaptive sampling policy in an MRF can be formalized as an optimization problem. By combining tools from the fields of Artificial Intelligence (AI) and Computational Statistics, an original algorithm is then proposed for approximate resolution. This generic procedure, referred to as Least-Squares Dynamic Programming (LSDP), combines an approximation of the value of a sampling policy based on a linear regression, the construction of a batch of MRF realizations and a backwards induction algorithm. Based on an empirical comparison of the performance of LSDP with existing one-step-look-ahead sampling heuristics and solutions provided by classical AI algorithms, the following conclusions can be derived: (i) a naïve heuristic consisting of sampling sites where marginals are the most uncertain is already an efficient sampling approach; (ii) LSDP outperforms all the classical approaches we have tested; and (iii) LSDP outperforms the naïve heuristic approach in cases where sampling costs are not uniform over the set of variables or where sampling actions are constrained. © 2013 Elsevier B.V. All rights reserved.
KW  - Dynamic programming
KW  - Gibbs sampling
KW  - Least-squares linear regression
KW  - Markov decision process
KW  - Sampling design
KW  - Weed mapping
KW  - Approximation algorithms
KW  - Artificial intelligence
KW  - Design
KW  - Dynamic programming
KW  - Ecology
KW  - Heuristic algorithms
KW  - Heuristic methods
KW  - Least squares approximations
KW  - Markov processes
KW  - Optimization
KW  - Random variables
KW  - Reinforcement learning
KW  - Sampling
KW  - Tools
KW  - Gibbs sampling
KW  - Least Square
KW  - Markov Decision Processes
KW  - Sampling design
KW  - Weed mappings
KW  - Importance sampling
SN  - 01679473 (ISSN)
LA  - English
J2  - Comput. Stat. Data Anal.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 10; Correspondence Address: N. Peyrard; INRA - UR875, Applied Mathematics and Computer Science Unit, Auzeville - CS 52627, 31326 Castanet Tolosan cedex, 24 Chemin de Borde Rouge, France; email: nathalie.peyrard@toulouse.inra.fr; CODEN: CSDAD
ER  -

TY  - JOUR
AU  - Reignier, P.
TI  - Supervised incremental learning of fuzzy rules
PY  - 1995
T2  - Robotics and Autonomous Systems
VL  - 16
IS  - 1
SP  - 57
EP  - 71
DO  - 10.1016/0921-8890(95)00034-D
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029405509&doi=10.1016%2f0921-8890%2895%2900034-D&partnerID=40&md5=70ab5b32f41a2595d629e6912c999c2a
AD  - LIFIA-IMAG, 38031 Grenoble Cedex, 46 Avenue Félix Viallet, France
AB  - This paper is concerned with on-going research results on fuzzy logic and learning. The general context of this study is reactive navigation in mobile robotics. Reactive navigation can be defined as a mapping between perception and action. Because there exists no exact model of the system to control, it is not possible to determine mathematically what command the robot should receive to act in a proper way. Machine learning and more precisely reinforcement learning seems to be a promising research axis to solve this problem. Learning to drive a robot is a complicated task. Some initial knowledge must be embedded inside the system to help it. This is done using fuzzy logic. One of the fundamental components of such a system is the supervised learning algorithm. This algorithm is responsible for learning the critical module of the reinforcement scheme, using the Sutton approach. Existing supervised algorithms for fuzzy logic are not adapted to specific robotic constraints. They are non-incremental or (and) use a limited fixed structure (the number of rules cannot grow for instance). This paper proposes an algorithm that overcomes these limitations. © 1995.
KW  - Actuators
KW  - Constraint theory
KW  - Control theory
KW  - Fuzzy sets
KW  - Knowledge based systems
KW  - Learning algorithms
KW  - Learning systems
KW  - Mobile robots
KW  - Navigation
KW  - Problem solving
KW  - Robotics
KW  - Sensors
KW  - Fuzzy rules
KW  - Mobile robotics
KW  - Reactive navigation
KW  - Reinforcement learning
KW  - Robotic constraints
KW  - Supervised learning algorithm
KW  - Sutton approach
KW  - Robot learning
SN  - 09218890 (ISSN)
LA  - English
J2  - Rob Autom Syst
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 6; Correspondence Address: P. Reignier; LIFIA-IMAG, 38031 Grenoble Cedex, 46 Avenue Félix Viallet, France; email: Patrick.Reignier@imag.fr; CODEN: RASOE
ER  -

TY  - CONF
AU  - Shao, J.
AU  - Chen, S.
AU  - Zhao, C.
TI  - Robot reinforcement learning methods based on XCSG
PY  - 2012
T2  - Advances in Intelligent and Soft Computing
VL  - 169 AISC
IS  - VOL. 2
SP  - 223
EP  - 228
DO  - 10.1007/978-3-642-30223-7_36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864297621&doi=10.1007%2f978-3-642-30223-7_36&partnerID=40&md5=ede4b77f80af9c373372ada48e1ed7ed
AD  - School of Computer Science, Shangqiu Institute of Technology, Shangqiu 476000, China
AB  - This paper proposed a robot reinforcement learning method based on learning classifier system. A Learning Classifier System is a accuracy-based machine learning system with gradient descent that combines reinforcement learning and rule discovery system. The genetic algorithm and the covering operator act as innovation discovery components which are responsible for discovering new better reinforcement learning rules. The reinforcement learning component is responsible for adjusting the fitness of rules in the system according to some reward obtained from the environment. The advantage of this approach is its accuracy-based representation, which can easily reduce learning space, improve online learning ability and robot robustness. © 2012 Springer-Verlag GmbH.
KW  - Accuracy-based machine learning system with gradient descent (XCSG)
KW  - Genetic algorithm
KW  - Reinforcement learning
KW  - Computer science
KW  - Genetic algorithms
KW  - Robots
KW  - Gradient descent
KW  - Learning classifier system
KW  - Online learning
KW  - Reinforcement learning method
KW  - Rule discovery
KW  - Reinforcement learning
SN  - 18675662 (ISSN); 978-364230222-0 (ISBN)
LA  - English
J2  - Adv. Intell. Soft Comput.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: J. Shao; School of Computer Science, Shangqiu Institute of Technology, Shangqiu 476000, China; email: sj012328@163.com; Conference name: Computer Science and Information Engineering, CSIE 2012; Conference date: 19 May 2012 through 20 May 2012; Conference code: 91392
ER  -

TY  - JOUR
AU  - Chi, M.
AU  - Vanlehn, K.
AU  - Litman, D.
AU  - Jordan, P.
TI  - Empirically evaluating the application of reinforcement learning to the induction of effective and adaptive pedagogical strategies
PY  - 2011
T2  - User Modeling and User-Adapted Interaction
VL  - 21
IS  - 1-2
SP  - 137
EP  - 180
DO  - 10.1007/s11257-010-9093-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955803320&doi=10.1007%2fs11257-010-9093-1&partnerID=40&md5=323b63de8758235b5e6b32c969e95c6b
AD  - Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, 5000 Forbes Ave., United States
AD  - School of Computing, Informatics and Decision Science Engineering, Arizona State University, Tempe, AZ, United States
AD  - Department of Computer Science and Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA, 5105 Sennott Square, 210 South Bouquet Street, United States
AD  - Learning Research and Development Center, University of Pittsburgh, Pittsburgh, PA 15260, United States
AB  - For many forms of e-learning environments, the system's behavior can be viewed as a sequential decision process wherein, at each discrete step, the system is responsible for selecting the next action to take. Pedagogical strategies are policies to decide the next system action when there are multiple ones available. In this project we present a Reinforcement Learning (RL) approach for inducing effective pedagogical strategies and empirical evaluations of the induced strategies. This paper addresses the technical challenges in applying RL to Cordillera, a Natural Language Tutoring System teaching students introductory college physics. The algorithm chosen for this project is a model-based RL approach, Policy Iteration, and the training corpus for the RL approach is an exploratory corpus, which was collected by letting the system make random decisions when interacting with real students. Overall, our results show that by using a rather small training corpus, the RL-induced strategies indeed measurably improved the effectiveness of Cordillera in that the RL-induced policies improved students' learning gains significantly. © 2011 Springer Science+Business Media B.V.
KW  - Human learning
KW  - Machine learning
KW  - Pedagogical strategy
KW  - Reinforcement learning
KW  - E-learning
KW  - Students
KW  - Teaching
KW  - Discrete step
KW  - E-learning environment
KW  - Empirical evaluations
KW  - Human learning
KW  - Learning gain
KW  - Machine learning
KW  - Model-based
KW  - Natural languages
KW  - Pedagogical strategy
KW  - Policy iteration
KW  - Sequential decision process
KW  - Small training
KW  - Technical challenges
KW  - Training corpus
KW  - Tutoring system
KW  - Reinforcement learning
SN  - 15731391 (ISSN)
LA  - English
J2  - User Modell User Adapt Interact
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 101; Correspondence Address: M. Chi; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, 5000 Forbes Ave., United States; email: minchi@cs.cmu.edu; CODEN: UMUIE
ER  -

TY  - CONF
AU  - Rawlinson, D.
AU  - Kowadlo, G.
TI  - Computational neuroscience offers hints for more general machine learning
PY  - 2017
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 10414 LNAI
SP  - 123
EP  - 132
DO  - 10.1007/978-3-319-63703-7_12
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028470935&doi=10.1007%2f978-3-319-63703-7_12&partnerID=40&md5=3f5002a24a8443b99038b4dc63fa4758
AD  - Incubator 491 Pty Ltd, Melbourne, Australia
AB  - Machine Learning has traditionally focused on narrow artificial intelligence - solutions for specific problems. Despite this, we observe two trends in the state-of-the-art: One, increasing architectural homogeneity in algorithms and models. Two, algorithms having more general application: New techniques often beat many benchmarks simultaneously. We review the changes responsible for these trends and look to computational neuroscience literature to anticipate future progress. © Springer International Publishing AG 2017.
KW  - Biological plausibility
KW  - Credit assignment problem
KW  - Machine learning
KW  - Predictive coding
KW  - Reinforcement learning
KW  - Sparse coding
KW  - Spike timing dependent plasticity
KW  - Artificial intelligence
KW  - Benchmarking
KW  - Combinatorial optimization
KW  - Neurology
KW  - Reinforcement learning
KW  - Biological plausibility
KW  - Credit assignment problems
KW  - Predictive coding
KW  - Sparse coding
KW  - Spike timing dependent plasticities
KW  - Learning systems
A2  - Everitt T.
A2  - Goertzel B.
A2  - Potapov A.
PB  - Springer Verlag
SN  - 03029743 (ISSN); 978-331963702-0 (ISBN)
LA  - English
J2  - Lect. Notes Comput. Sci.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Correspondence Address: D. Rawlinson; Incubator 491 Pty Ltd, Melbourne, Australia; email: dave@agi.io; Conference name: 10th International Conference on Artificial General Intelligence, AGI 2017; Conference date: 15 August 2017 through 18 August 2017; Conference code: 196189
ER  -

TY  - JOUR
AU  - Crites, R.H.
AU  - Barto, A.G.
TI  - Elevator Group Control Using Multiple Reinforcement Learning Agents
PY  - 1998
T2  - Machine Learning
VL  - 12
IS  - 4
SP  - 235
EP  - 262
DO  - 10.1023/a:1007518724497
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032208335&doi=10.1023%2fa%3a1007518724497&partnerID=40&md5=29184866af23cc43bb29b2cd42981357
AD  - Unica Technologies, Inc., Lincoln, MA 01773, 55 Old Bedford Road, United States
AD  - University of Massachusetts, Department of Computer Science, Amherst, MA 01003, United States
AB  - Recent algorithmic and theoretical advances in reinforcement learning (RL) have attracted widespread interest. RL algorithms have appeared that approximate dynamic programming on an incremental basis. They can be trained on the basis of real or simulated experiences, focusing their computation on areas of state space that are actually visited during control, making them computationally tractable on very large problems. If each member of a team of agents employs one of these algorithms, a new collective learning algorithm emerges for the team as a whole. In this paper we demonstrate that such collective RL algorithms can be powerful heuristic methods for addressing large-scale control problems. Elevator group control serves as our testbed. It is a difficult domain posing a combination of challenges not seen in most multi-agent learning research to date. We use a team of RL agents, each of which is responsible for controlling one elevator car. The team receives a global reward signal which appears noisy to each agent due to the effects of the actions of the other agents, the random nature of the arrivals and the incomplete observation of the state. In spite of these complications, we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware. These results demonstrate the power of multi-agent RL on a very large scale stochastic dynamic optimization problem of practical utility.
KW  - Discrete event dynamic systems
KW  - Elevator group control
KW  - Multiple agents
KW  - Reinforcement learning
KW  - Teams
KW  - Artificial intelligence
KW  - Computer simulation
KW  - Discrete time control systems
KW  - Dynamic programming
KW  - Elevators
KW  - Learning algorithms
KW  - Problem solving
KW  - Random processes
KW  - Discrete event dynamic systems
KW  - Elevator group control
KW  - Reinforcement learning (RL)
KW  - Learning systems
PB  - Springer Netherlands
SN  - 08856125 (ISSN)
LA  - English
J2  - Mach Learn
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 194; Correspondence Address: R.H. Crites; Unica Technologies, Inc., Lincoln, MA 01773, 55 Old Bedford Road, United States; email: rcrites@unica-usa.com; CODEN: MALEE
ER  -

TY  - CONF
AU  - Song, Q.L.
AU  - Zhao, J.B.
TI  - Multi-robot path planning based on learning classifier system with policy gradient reinforcement learning and support vector machine(PGRL-SVM)
PY  - 2013
T2  - Applied Mechanics and Materials
VL  - 347-350
SP  - 3208
EP  - 3211
DO  - 10.4028/www.scientific.net/AMM.347-350.3208
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883189506&doi=10.4028%2fwww.scientific.net%2fAMM.347-350.3208&partnerID=40&md5=844561c643aaa857bf242e20b32d099a
AD  - Shangqiu Institute of Technology, Shangqiu School of Mechanical Engineering, China
AB  - This paper presented a novel approach to solving the problem of robot avoidance collision planning. A Learning Classifier System is a accuracy-based machine learning system using gradient descent that combines covering operator and genetic algorithm. The covering operator is responsible for adjusting precision and large search space according to some reward obtained from the environment. The genetic algorithm acts as an innovation discovery component which is responsible for discovering new better path planning rules. The advantages of this approach are its accuracy-based representation, that can be easily reduce learning space, online learning ability, robustness due to the use of genetic algorithm. © 2013 Trans Tech Publications Ltd, Switzerland.
KW  - Accuracy-based learning classifier system (XCS)
KW  - Avoidance collision
KW  - Multi- robot
KW  - Path planning
KW  - Support vector machine (SVM)
KW  - Genetic algorithms
KW  - Motion planning
KW  - Reinforcement learning
KW  - Robot programming
KW  - Support vector machines
KW  - Avoidance collision
KW  - Gradient descent
KW  - Learning classifier system
KW  - Multi-robot path planning
KW  - Online learning
KW  - Policy gradient reinforcement learning
KW  - Search spaces
KW  - E-learning
SN  - 16627482 (ISSN)
LA  - English
J2  - Appl. Mech. Mater.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 1; Conference name: 2013 International Conference on Precision Mechanical Instruments and Measurement Technology, ICPMIMT 2013; Conference date: 25 May 2013 through 26 May 2013; Conference code: 99138
ER  -

TY  - JOUR
AU  - Castro, G.B.
AU  - Martini, J.S.C.
AU  - Hirakawa, A.R.
TI  - Multilayer distributed model predictive control of urban traffic
PY  - 2013
T2  - WIT Transactions on Ecology and the Environment
VL  - 179 VOLUME 2
SP  - 967
EP  - 976
DO  - 10.2495/sc130822
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896084023&doi=10.2495%2fsc130822&partnerID=40&md5=7aa83aa9c5ffb14fbc5e87e0fee8de85
AD  - São Paulo University, Brazil
AB  - Urban traffic is a relevant topic for the population of cities that suffer from daily traffic jams. People not only spend time on their journeys but also lose energy. With the purpose of improving the traffic flow, it is possible to control the temporization of traffic lights. However, because of the intrinsic characteristics of the system - multivariable, stochastic and dynamic - controlling the traffic of a city through the temporization of its actuators is insufficient to achieve an optimal level. This way, the objective of this work is to elaborate a multilayer distributed model predictive control (ML-DMPC) of the traffic. The proposed model is composed of two layers of control: local and global. Each agent of control is responsible for the local control logic of a set of interdependent semaphores - in other words, semaphores that belong to the same intersection between streets. In order to achieve the local control objective, each control agent is modelled as a neural network and the following components are weighted: the waiting time of the vehicles; the waiting time of the pedestrians; and synchronization of the semaphores. The input variables of the system - flow of vehicles and pedestrians in all directions - are treated by fuzzy logic for the purpose of improving the quality of the information. The principles of consensus between control agents and behavioural coordination are used to conciliate local and global control objectives. In the global control layer, reinforcement learning is applied to improve the modelling process of complex dynamic systems, as the city traffic, as well as to provide adaptability to the model through learning. By means of the proposed model, it is possible to achieve the dynamic control of the traffic lights system of a city. © 2013 WIT Press.
KW  - Artificial intelligence
KW  - Distributed control
KW  - Interconnected dynamic systems
KW  - Model predictive control
KW  - Multilayer control
KW  - Urban traffic control
KW  - artificial intelligence
KW  - artificial neural network
KW  - fuzzy mathematics
KW  - model test
KW  - modeling
KW  - pedestrian
KW  - planning method
KW  - temporal variation
KW  - traffic management
KW  - urban transport
PB  - WITPress
SN  - 17433541 (ISSN); 978-184564746-9 (ISBN)
LA  - English
J2  - WIT Trans. Ecol. Environ.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 2
ER  -

TY  - CONF
AU  - Abel, D.
AU  - MacGlashan, J.
AU  - Littman, M.L.
TI  - Reinforcement learning as a framework for ethical decision making
PY  - 2016
T2  - AAAI Workshop - Technical Report
VL  - WS-16-01 - WS-16-15
SP  - 54
EP  - 61
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021952784&partnerID=40&md5=0b4a49ef4997f5bd8a453f86a7a2bf00
AD  - Brown University, Computer Science Department, 115 Waterman Street, Providence, 02912-1910, RI, United States
AB  - Emerging AI systems will be making more and more decisions that impact the lives of humans in a significant way. It is essential, then, that these AI systems make decisions that take into account the desires, goals, and preferences of other people, while simultaneously learning about what those preferences are. In this work, we argue that the reinforcementlearning framework achieves the appropriate generality required to theorize about an idealized ethical artificial agent, and offers the proper foundations for grounding specific questions about ethical learning and decision making that can promote further scientific investigation. We define an idealized formalism for an ethical learner, and conduct experiments on two toy ethical dilemmas, demonstrating the soundness and flexibility of our approach. Lastly, we identify several critical challenges for future advancement in the area that can leverage our proposed framework. Copyright © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
KW  - Artificial intelligence
KW  - Behavioral research
KW  - Big data
KW  - Cognitive systems
KW  - Computer games
KW  - Computer programming
KW  - Computer systems programming
KW  - Data mining
KW  - Decision making
KW  - Hybrid systems
KW  - Philosophical aspects
KW  - Population statistics
KW  - Reinforcement learning
KW  - AI systems
KW  - Artificial agents
KW  - Critical challenges
KW  - Ethical decision making
KW  - Ethical dilemma
KW  - Scientific investigation
KW  - Education
PB  - AI Access Foundation
SN  - 978-157735759-9 (ISBN)
LA  - English
J2  - AAAI Workshop Tech. Rep.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 81; Conference name: 30th AAAI Conference on Artificial Intelligence, AAAI 2016; Conference date: 12 February 2016 through 13 February 2016; Conference code: 128195
ER  -

TY  - CONF
AU  - Plebe, A.
TI  - Biologically plausible modelling of morality
PY  - 2014
T2  - CEUR Workshop Proceedings
VL  - 1315
SP  - 50
EP  - 60
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919663426&partnerID=40&md5=3f01779d63c224deba20b7f30804f50b
AD  - Department of Cognitive Science, University of Messina, v. Concezione 8, Messina, Italy
AB  - Neural computation has an extraordinarily influential role in the study of several human capacities and behavior. It has been the dominant approach in the vision science of the last half century, and it is currently one of the fundamental methods of investigation for several higher cognitive functions. Yet, no neurocomputational models have been proposed for morality. Computational modeling in general has been scarcely pursued in morality, and existent non-neural attempts have failed to account for the mental processes involved during moral judgments. In this paper we argue that in the past decade the situation has evolved in a way that subverted the insufficient knowledge on the basic organization of moral cognition in brain circuits, making the project of modeling morality in neurocomputational terms feasible. We will sketch an original architecture that combines reinforcement learning and Hebbian learning, aimed at simulating forms of moral behavior in a simple artificial context. © 2014, International Workshop on Artificial Intelligence and Cognition.
KW  - Amygdala
KW  - Moral cognition
KW  - Orbitofrontal cortex
KW  - Artificial intelligence
KW  - Behavioral research
KW  - Brain
KW  - Reinforcement learning
KW  - Amygdala
KW  - Cognitive functions
KW  - Computational model
KW  - Hebbian learning
KW  - Moral cognition
KW  - Neural computations
KW  - Neurocomputational models
KW  - Orbitofrontal cortex
KW  - Cognitive systems
A2  - Cruciani M.
A2  - Lieto A.
A2  - Lieto A.
A2  - Radicioni D.P.
PB  - CEUR-WS
SN  - 16130073 (ISSN)
LA  - English
J2  - CEUR Workshop Proc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0; Conference name: 2nd International Workshop on Artificial Intelligence and Cognition, AIC 2014; Conference date: 26 November 2014 through 27 November 2014; Conference code: 109643
ER  -

TY  - CONF
AU  - Livingston, S.
AU  - Garvey, J.
AU  - Elhanany, I.
TI  - On the broad implications of reinforcement learning based AGI
PY  - 2008
T2  - Frontiers in Artificial Intelligence and Applications
VL  - 171
IS  - 1
SP  - 478
EP  - 482
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875937272&partnerID=40&md5=f992092d0617772f655db7d7472688d9
AD  - Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN, United States
AD  - Department of Nuclear Engineering, University of Tennessee, Knoxville, TN, United States
AB  - Reinforcement learning (RL) is an attractive machine learning discipline in the context of Artificial General Intelligence (AGI). This paper focuses on the intersection between RL and AGI by first speculating on what are the missing components that would facilitate the realization of RL-based AGI. Based on this paradigm, we touch on several of the key moral and practical issues that will inevitably arise. © 2008 The authors and IOS Press. All rights reserved.
KW  - Machine Intelligence
KW  - Moral Implications of AGI
KW  - Reinforcement Learning
KW  - Artificial general intelligences
KW  - Machine intelligence
KW  - Machine-learning
KW  - Moral implication of artificial general intelligence
KW  - Moral issues
KW  - Practical issues
KW  - Reinforcement learnings
KW  - Reinforcement learning
PB  - IOS Press BV
SN  - 09226389 (ISSN); 978-158603833-5 (ISBN)
LA  - English
J2  - Front. Artif. Intell. Appl.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 0
ER  -

TY  - JOUR
AU  - Weng, J.
AU  - Paslaski, S.
AU  - Daly, J.
AU  - VanDam, C.
AU  - Brown, J.
TI  - Modulation for emergent networks: Serotonin and dopamine
PY  - 2013
T2  - Neural Networks
VL  - 41
SP  - 225
EP  - 239
DO  - 10.1016/j.neunet.2012.11.008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875909807&doi=10.1016%2fj.neunet.2012.11.008&partnerID=40&md5=19d2d8411e098e79e97fda30f5a55523
AD  - Michigan State University, East Lansing, MI, United States
AB  - In autonomous learning, value-sensitive experiences can improve the efficiency of learning. A learning network needs be motivated so that the limited computational resources and the limited lifetime are devoted to events that are of high value for the agent to compete in its environment. The neuromodulatory system of the brain is mainly responsible for developing such a motivation system. Although reinforcement learning has been extensively studied, many existing models are symbolic whose internal nodes or modules have preset meanings. Neural networks have been used to automatically generate internal emergent representations. However, modeling an emergent motivational system for neural networks is still a great challenge. By emergent, we mean that the internal representations emerge autonomously through interactions with the external environments. This work proposes a generic emergent modulatory system for emergent networks, which includes two subsystems - the serotonin system and the dopamine system. The former signals a large class of stimuli that are intrinsically aversive (e.g., stress or pain). The latter signals a large class of stimuli that are intrinsically appetitive (e.g., pleasure or sweet). We experimented with this motivational system for two settings. The first is a visual recognition setting to investigate how such a system can learn through interactions with a teacher, who does not directly give answers, but only punishments and rewards. The second is a setting for wandering in the presence of a friend and a foe. © 2012 Elsevier Ltd.
KW  - Brain architecture
KW  - Dopamine
KW  - Glutamate
KW  - Modulation
KW  - Reinforcement learning
KW  - Serotonin
KW  - Animals
KW  - Artificial Intelligence
KW  - Brain
KW  - Dopamine
KW  - Glutamic Acid
KW  - Humans
KW  - Learning
KW  - Models, Neurological
KW  - Motivation
KW  - Neural Networks (Computer)
KW  - Reinforcement (Psychology)
KW  - Serotonin
KW  - Symbolism
KW  - Modulation
KW  - Neural networks
KW  - Neurophysiology
KW  - Reinforcement learning
KW  - chloride ion
KW  - dopamine
KW  - glutamic acid
KW  - neurotransmitter
KW  - serotonin
KW  - sodium ion
KW  - Brain architecture
KW  - Computational resources
KW  - Dopamine
KW  - Emergent representations
KW  - External environments
KW  - Glutamate
KW  - Internal representation
KW  - Serotonin
KW  - article
KW  - calculation
KW  - dopaminergic system
KW  - learning
KW  - mathematical computing
KW  - nerve cell network
KW  - neuroanatomy
KW  - neuromodulation
KW  - priority journal
KW  - serotoninergic system
KW  - synapse
KW  - Amines
SN  - 18792782 (ISSN)
C2  - 23294763
LA  - English
J2  - Neural Netw.
M3  - Article
DB  - Scopus
N1  - Export Date: 03 May 2024; Cited By: 14; Correspondence Address: J. Weng; Michigan State University, East Lansing, MI, United States; email: weng@cse.msu.edu; CODEN: NNETE
ER  -


TY  - JOUR
AU  - Wells, L
AU  - Bednarz, T
TI  - Explainable AI and Reinforcement Learning-A Systematic Review of Current Approaches and Trends
T2  - FRONTIERS IN ARTIFICIAL INTELLIGENCE
LA  - English
KW  - explainable AI
KW  - reinforcement learning
KW  - artificial intelligence
KW  - visualization
KW  - machine learning
AB  - Research into Explainable Artificial Intelligence (XAI) has been increasing in recent years as a response to the need for increased transparency and trust in AI. This is particularly important as AI is used in sensitive domains with societal, ethical, and safety implications. Work in XAI has primarily focused on Machine Learning (ML) for classification, decision, or action, with detailed systematic reviews already undertaken. This review looks to explore current approaches and limitations for XAI in the area of Reinforcement Learning (RL). From 520 search results, 25 studies (including 5 snowball sampled) are reviewed, highlighting visualization, query-based explanations, policy summarization, human-in-the-loop collaboration, and verification as trends in this area. Limitations in the studies are presented, particularly a lack of user studies, and the prevalence of toy-examples and difficulties providing understandable explanations. Areas for future study are identified, including immersive visualization, and symbolic representation.
AD  - Univ New South Wales, Fac Art & Design, Expanded Percept & Interact Ctr, Sydney, NSW, AustraliaAD  - CSIRO, Data61, Sydney, NSW, AustraliaC3  - University of New South Wales SydneyC3  - Commonwealth Scientific & Industrial Research Organisation (CSIRO)FU  - UNSW [RG190540]
FX  - This work was supported by UNSW Grant number: RG190540.
PU  - FRONTIERS MEDIA SA
PI  - LAUSANNE
PA  - AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN  - 2624-8212
J9  - FRONT ARTIF INTELL
JI  - Front. Artif. Intell.
PY  - 2021
VL  - 4
C7  - 550030
DO  - 10.3389/frai.2021.550030
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000751704800005
N1  - Times Cited in Web of Science Core Collection:  48
Total Times Cited:  49
Cited Reference Count:  48
ER  -

TY  - CPAPER
AU  - Holzinger, A
AU  - Goebel, R
AU  - Fong, R
AU  - Moon, T
AU  - Müller, KR
AU  - Samek, W
ED  - Holzinger, A
ED  - Goebel, R
ED  - Fong, R
ED  - Moon, T
ED  - Muller, KR
ED  - Samek, W
TI  - xxAI - Beyond Explainable Artificial Intelligence
T2  - XXAI - BEYOND EXPLAINABLE AI: International Workshop, Held in Conjunction with ICML 2020, July 18, 2020, Vienna, Austria, Revised and Extended Papers
LA  - English
CP  - International Workshop on Beyond Explainable Artificial Intelligence (xxAI)
KW  - Artificial intelligence
KW  - Explainable AI
KW  - Machine learning
KW  - Explainability
KW  - NEURAL-NETWORKS
KW  - DEEP
AB  - The success of statistical machine learning from big data, especially of deep learning, has made artificial intelligence (AI) very popular. Unfortunately, especially with the most successful methods, the results are very difficult to comprehend by human experts. The application of AI in areas that impact human life (e.g., agriculture, climate, forestry, health, etc.) has therefore led to an demand for trust, which can be fostered if the methods can be interpreted and thus explained to humans. The research field of explainable artificial intelligence (XAI) provides the necessary foundations and methods. Historically, XAI has focused on the development of methods to explain the decisions and internal mechanisms of complex AI systems, with much initial research concentrating on explaining how convolutional neural networks produce image classification predictions by producing visualizations which highlight what input patterns are most influential in activating hidden units, or are most responsible for a model's decision. In this volume, we summarize research that outlines and takes next steps towards a broader vision for explainable AI in moving beyond explaining classifiers via such methods, to include explaining other kinds of models (e.g., unsupervised and reinforcement learning models) via a diverse array of XAI techniques (e.g., question-and-answering systems, structured explanations). In addition, we also intend to move beyond simply providing model explanations to directly improving the transparency, efficiency and generalization ability of models. We hope this volume presents not only exciting research developments in explainable AI but also a guide for what next areas to focus on within this fascinating and highly relevant research field as we enter the second decade of the deep learning revolution. This volume is an outcome of the ICML 2020 workshop on "XXAI: Extending Explainable AI Beyond Deep Models and Classifiers."
AD  - Univ Nat Resources & Life Sci, Human Ctr Lab, Vienna, AustriaAD  - Med Univ Graz, Graz, AustriaAD  - XAI Lab, Alberta Machine Intelligence Inst, Edmonton, AB, CanadaAD  - Princeton Univ, Princeton, NJ USAAD  - Seoul Natl Univ, Seoul, South KoreaAD  - Korea Univ, Dept Artificial Intelligence, Seoul, South KoreaAD  - Max Planck Inst Informat, Saarbrucken, GermanyAD  - Tech Univ Berlin, Machine Learning Grp, Berlin, GermanyAD  - Fraunhofer Heinrich Hertz Inst, Dept Artificial Intelligence, Berlin, GermanyAD  - BIFOLD Berlin Inst Foundat Data & Learning, Berlin, GermanyC3  - BOKU UniversityC3  - Medical University of GrazC3  - Princeton UniversityC3  - Seoul National University (SNU)C3  - Korea UniversityC3  - Max Planck SocietyC3  - Technical University of BerlinC3  - Fraunhofer GesellschaftFU  - Austrian Science Fund (FWF) [P-32554]; German Ministry for Education and Research (BMBF) [01IS18025A, 01IS18037A]; European Union [965221]
FX  - The authors declare that there are no conflict of interests. This work does not raise any ethical issues. Parts of this work have been funded by the Austrian Science Fund (FWF), Project: P-32554, explainable AI, by the German Ministry for Education and Research (BMBF) through BIFOLD (refs. 01IS18025A and 01IS18037A), and by the European Union's Horizon 2020 programme (grant no. 965221), Project iToBoS.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 2945-9133
SN  - 1611-3349
SN  - 978-3-031-04082-5
SN  - 978-3-031-04083-2
J9  - LECT NOTES ARTIF INT
PY  - 2022
VL  - 13200
SP  - 3
EP  - 10
DO  - 10.1007/978-3-031-04083-2_1
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001048254300001
N1  - Times Cited in Web of Science Core Collection:  13
Total Times Cited:  13
Cited Reference Count:  42
ER  -

TY  - CPAPER
AU  - Zhao, C
AU  - Chen, F
AU  - Wu, XT
AU  - Chen, HF
AU  - Zhou, JY
A1  - ACM
TI  - 2nd Workshop on Ethical Artificial Intelligence: Methods and Applications (EAI)
T2  - PROCEEDINGS OF THE 29TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2023
LA  - English
CP  - 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)
KW  - ethical artificial intelligence
KW  - fairness-aware
KW  - machine learning
AB  - Ethical AI has become increasingly important, and it has been attracting attention from academia and industry, due to its increased popularity in real-world applications with fairness concerns. It also places fundamental importance on ethical considerations in determining legitimate and illegitimate uses of AI. Organizations that apply ethical AI have clearly stated well-defined review processes to ensure adherence to legal guidelines. Therefore, the wave of research at the intersection of ethical AI in data mining and machine learning has also influenced other fields of science, including computer vision, natural language processing, reinforcement learning, and social science. Despite these successes, ethical AI still faces many challenges, such as a lack of interpretable and explainable methods for fairness-aware deep learning models, etc. Consequently, there is an urgent need to bring experts and researchers together at prestigious venues to discuss ethical AI, which has been rarely seen in previous KDD conferences. This workshop will provide a premium platform for both research and industry from different backgrounds to exchange ideas on opportunities, challenges, and cutting-edge techniques in ethical AI.
AD  - Baylor Univ, Waco, TX 76798 USAAD  - Univ Texas Dallas, Richardson, TX USAAD  - Univ Arkansas, Fayetteville, AR USAAD  - NEC Labs Amer, Princeton, NY USAAD  - Michigan State Univ, E Lansing, MI USAC3  - Baylor UniversityC3  - University of Texas SystemC3  - University of Texas DallasC3  - University of Arkansas SystemC3  - University of Arkansas FayettevilleC3  - NEC CorporationC3  - Michigan State UniversityFU  - National Science Foundation [2147375, 1750911]
FX  - The worksop was supported by the National Science Foundation under grant number 2147375 and 1750911.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0103-0
PY  - 2023
SP  - 5903
EP  - 5904
DO  - 10.1145/3580305.3599215
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001118896305136
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  4
ER  -

TY  - CPAPER
AU  - Ho, JH
AU  - Wang, CM
ED  - Nurnberger, A
ED  - Fortino, G
ED  - Guerrieri, A
ED  - Kaber, D
ED  - Mendonca, D
ED  - Schilling, M
ED  - Yu, Z
TI  - Human-Centered AI using Ethical Causality and Learning Representation for Multi-Agent Deep Reinforcement Learning
T2  - PROCEEDINGS OF THE 2021 IEEE INTERNATIONAL CONFERENCE ON HUMAN-MACHINE SYSTEMS (ICHMS)
LA  - English
CP  - 2nd IEEE International Conference on Human-Machine Systems (ICHMS)
KW  - Multi-Agent Deep Reinforcement Learning
KW  - Ethical Causality
KW  - Successor Representation
KW  - Human-Centered AI
AB  - Human-Centered Computing and AI are two fields devoted to several cross-intersecting interests in the modern AI design. They consider human factors and the machine learning algorithms to enhance compatibility and reliability for human-robot interaction and cooperation. In this work, we propose a novel design concept for the challenging issues that have raised ethical dilemmas; an augmented ethical causality with successor representation for policy gradient models Human-Centered AI with environments. The proposed system leverages Human-Centered AI for using explainable knowledge to construct the ethical causality, and shows it significantly outperformed the statistical approach and baselines alone by further considering meta parametric Human-Centered ethical priorities, when compared to other approaches in the simulated game theory Deep Reinforcement Learning environments. The experimental results aim to efficiently and effectively access the cause, effect and impact of causal inference and multi-agent heterogeneity in the DRL environments for natural, general and significant causal learning representations.
AD  - Taiwan Int Grad Program, Social Networks & Human Ctr Comp Program, Taipei, TaiwanAD  - Acad Sinica, Inst Informat Sci, Taipei 115, TaiwanAD  - Natl Tsing Hua Univ, Inst Informat Syst & Applicat, Hsinchu 30013, TaiwanC3  - Academia Sinica - TaiwanC3  - National Tsing Hua UniversityPU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-0170-8
PY  - 2021
SP  - 143
EP  - 148
DO  - 10.1109/ICHMS53169.2021.9582667
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:000873141600030
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  32
ER  -

TY  - CPAPER
AU  - Zhao, C
AU  - Chen, F
AU  - Wu, XT
AU  - Funk, C
AU  - Hoogs, A
A1  - ACM
TI  - 1st ACM SIGKDD Workshop on Ethical Artificial Intelligence: Methods and Applications (EAI-KDD'22)
T2  - PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022
LA  - English
CP  - 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KKD)
KW  - Ethical artificial intelligence
KW  - fairness-aware
KW  - machine learning
AB  - Ethical AI has become increasingly important, and it has been attracting attention from academia and industry, due to its increased popularity in real-world applications with fairness concerns. It also places fundamental importance on ethical considerations in determining legitimate and illegitimate uses of AI. Organizations that apply ethical AI have clearly stated well-defined review processes to ensure adherence to legal guidelines. Therefore, the wave of research at the intersection of ethical AI in data mining and machine learning has also influenced other fields of science, including computer vision, natural language processing, reinforcement learning, and social science. Despite these successes, ethical AI still faces many challenges, such as lack of interpretable and explainable methods for fairness-aware deep learning models, etc. Consequently, there is an urgent need to bring experts and researchers together at prestigious venues to discuss ethical AI, which has been rarely seen in previous KDD conferences. This workshop will provide a premium platform for both research and industry from different backgrounds to exchange ideas on opportunities, challenges, and cutting-edge techniques in ethical AI.
AD  - Kitware Inc, Clifton Pk, NY 12065 USAAD  - Univ Texas Dallas, Richardson, TX 75083 USAAD  - Univ Arkansas, Fayetteville, AR 72701 USAC3  - Kitware, Inc.C3  - University of Texas SystemC3  - University of Texas DallasC3  - University of Arkansas SystemC3  - University of Arkansas FayettevilleFU  - National Science Foundation (NSF) [2147375]
FX  - The workshop was supported by the National Science Foundation (NSF) under grant number 2147375.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-9385-0
PY  - 2022
SP  - 4914
EP  - 4915
DO  - 10.1145/3534678.3542912
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001119000304150
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  0
ER  -

TY  - JOUR
AU  - Ledzinski, L
AU  - Grzesk, G
TI  - Artificial Intelligence Technologies in Cardiology
T2  - JOURNAL OF CARDIOVASCULAR DEVELOPMENT AND DISEASE
LA  - English
KW  - artificial intelligence
KW  - cardiology
KW  - machine learning
KW  - MULTIVARIABLE PREDICTION MODEL
KW  - INDIVIDUAL PROGNOSIS
KW  - DIAGNOSIS TRIPOD
KW  - HEALTH
KW  - REGISTRY
KW  - FUTURE
KW  - DISEASE
KW  - SYSTEM
KW  - RISK
AB  - As the world produces exabytes of data, there is a growing need to find new methods that are more suitable for dealing with complex datasets. Artificial intelligence (AI) has significant potential to impact the healthcare industry, which is already on the road to change with the digital transformation of vast quantities of information. The implementation of AI has already achieved success in the domains of molecular chemistry and drug discoveries. The reduction in costs and in the time needed for experiments to predict the pharmacological activities of new molecules is a milestone in science. These successful applications of AI algorithms provide hope for a revolution in healthcare systems. A significant part of artificial intelligence is machine learning (ML), of which there are three main types-supervised learning, unsupervised learning, and reinforcement learning. In this review, the full scope of the AI workflow is presented, with explanations of the most-often-used ML algorithms and descriptions of performance metrics for both regression and classification. A brief introduction to explainable artificial intelligence (XAI) is provided, with examples of technologies that have developed for XAI. We review important AI implementations in cardiology for supervised, unsupervised, and reinforcement learning and natural language processing, emphasizing the used algorithm. Finally, we discuss the need to establish legal, ethical, and methodical requirements for the deployment of AI models in medicine.
AD  - Nicolaus Copernicus Univ Torun, Fac Hlth Sci, Dept Cardiol & Clin Pharmacol, Coll Medicum Bydgoszcz, Ujejskiego 75, PL-85168 Bydgoszcz, PolandC3  - Nicolaus Copernicus UniversityPU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2308-3425
J9  - J CARDIOVASC DEV DIS
JI  - J. Cardiovasc. Dev. Dis.
DA  - MAY 6
PY  - 2023
VL  - 10
IS  - 5
C7  - 202
DO  - 10.3390/jcdd10050202
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000997576800001
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  94
ER  -

TY  - JOUR
AU  - Heuillet, A
AU  - Couthouis, F
AU  - Díaz-Rodríguez, N
TI  - Explainability in deep reinforcement learning
T2  - KNOWLEDGE-BASED SYSTEMS
LA  - English
KW  - Reinforcement Learning
KW  - Explainable artificial intelligence
KW  - Machine Learning
KW  - Deep Learning
KW  - Responsible artificial intelligence
KW  - Representation learning
AB  - A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent's behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainability. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems. (C) 2020 Elsevier B.V. All rights reserved.
AD  - Bordeaux INP, ENSEIRB MATMECA, 1 Ave Docteur Albert Schweitzer, F-33400 Talence, FranceAD  - Bordeaux INP, ENSC, 109 Ave Roul, F-33400 Talence, FranceAD  - Inst Polytech Paris, Inria Flowers Team, ENSTA Paris, 828 Blvd Marechaux, F-91762 Palaiseau, FranceC3  - Universite de BordeauxC3  - Universite de BordeauxC3  - Institut Polytechnique de ParisPU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0950-7051
SN  - 1872-7409
J9  - KNOWL-BASED SYST
JI  - Knowledge-Based Syst.
DA  - FEB 28
PY  - 2021
VL  - 214
C7  - 106685
DO  - 10.1016/j.knosys.2020.106685
C6  - JAN 2021
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000618603300002
N1  - Times Cited in Web of Science Core Collection:  110
Total Times Cited:  117
Cited Reference Count:  116
ER  -

TY  - JOUR
AU  - Rodriguez-Soto, M
AU  - Serramia, M
AU  - Lopez-Sanchez, M
AU  - Rodriguez-Aguilar, JA
TI  - Instilling moral value alignment by means of multi-objective reinforcement learning
T2  - ETHICS AND INFORMATION TECHNOLOGY
LA  - English
KW  - Value alignment
KW  - Reinforcement learning
KW  - Multi-objective reinforcement learning
KW  - Ethics
KW  - BEHAVIOR
KW  - ETHICS
KW  - CARE
AB  - AI research is being challenged with ensuring that autonomous agents learn to behave ethically, namely in alignment with moral values. Here, we propose a novel way of tackling the value alignment problem as a two-step process. The first step consists on formalising moral values and value aligned behaviour based on philosophical foundations. Our formalisation is compatible with the framework of (Multi-Objective) Reinforcement Learning, to ease the handling of an agent's individual and ethical objectives. The second step consists in designing an environment wherein an agent learns to behave ethically while pursuing its individual objective. We leverage on our theoretical results to introduce an algorithm that automates our two-step approach. In the cases where value-aligned behaviour is possible, our algorithm produces a learning environment for the agent wherein it will learn a value-aligned behaviour.
AD  - Artificial Intelligence Res Inst IIIA CSIC, Campus UAB, Bellaterra 08193, SpainAD  - Univ Barcelona, Dept Math & Comp Sci, Gran Via De Les Corts Catalanes 585, Barcelona 08007, SpainC3  - Autonomous University of BarcelonaC3  - Consejo Superior de Investigaciones Cientificas (CSIC)C3  - CSIC - Instituto de Investigacion en Inteligencia Artificial (IIIA)C3  - University of BarcelonaFU  - CRUE-CSIC agreement; Springer Nature; project AI4EU [H2020-825619]; project Crowd4SDG [H2020-872944]; project TAILOR [H2020-952215]; project COREDEM [H2020-785907]; project NANOMOOC [COMR-DI18-1-0010-02]; Barcelona City Council through the Fundacio Solidaritat de la UB [21S01802-001]; MCIN/AEI [PID2019-104156GB-I00]; Spanish Government [FPU18/03387]
FX  - project TAILOR from Barcelona City Council through the Fundacio Solidaritat de la UB Open Access funding provided thanks to the CRUECSIC agreement with Springer Nature. Research supported by projects AI4EU (H2020-825619), Crowd4SDG (H2020-872944), TAILOR (H2020-952215), COREDEM (H2020-785907), NANOMOOC (COMR-DI18-1-0010-02), and 21S01802-001 from Barcelona City Council through the Fundacio Solidaritat de la UB. Financial support was also received from grant PID2019-104156GB-I00 funded by MCIN/AEI/10.13039/501100011033. Manel Rodriguez-Soto was funded by the Spanish Government with an FPU grant (ref. FPU18/03387).
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1388-1957
SN  - 1572-8439
J9  - ETHICS INF TECHNOL
JI  - Ethics Inf. Technol.
DA  - MAR
PY  - 2022
VL  - 24
IS  - 1
C7  - 9
DO  - 10.1007/s10676-022-09635-0
WE  - Social Science Citation Index (SSCI)WE  - Arts &amp; Humanities Citation Index (A&amp;HCI)AN  - WOS:000748917000001
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  6
Cited Reference Count:  55
ER  -

TY  - CPAPER
AU  - Roberts, JS
AU  - Montoya, LN
ED  - Arai, K
TI  - Contextualizing Artificially Intelligent Morality: A Meta-ethnography of Theoretical, Political and Applied Ethics
T2  - ADVANCES IN INFORMATION AND COMMUNICATION, FICC, VOL 2
LA  - English
CP  - 8th Future of Information and Communication Conference (FICC)
KW  - Artificial intelligence
KW  - Ethics
KW  - Reinforcement learning
KW  - Politics
AB  - In this meta-ethnography, we explore three different angles of ethical artificial intelligence (AI) design implementation including the philosophical ethical viewpoint, the technical perspective, and framing through a political lens. Our qualitative research includes a literature review which highlights the cross referencing of these angles through discussing the value and drawbacks of contrastive top-down, bottom-up, and hybrid approaches previously published. The novel contribution to this framework is the political angle, which constitutes ethics in AI either being determined by corporations and governments and imposed through policies or law(coming from the top), or ethics being called for by the people (coming from the bottom), as well as top-down, bottom-up, and hybrid technicalities of how AI is developed within a moral construct and in consideration of its users, with expected and unexpected consequences and long-term impact in the world. There is a focus on reinforcement learning as an example of a bottom-up applied technical approach and AI ethics principles as a practical top-down approach. This investigation includes real-world case studies to impart a global perspective, as well as philosophical debate on the ethics of AI and theoretical future thought experimentation based on historical fact, current world circumstances, and possible ensuing realities.
AD  - Accel AI Inst, San Francisco, CA 94612 USAPU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 2367-3370
SN  - 2367-3389
SN  - 978-3-031-28072-6
SN  - 978-3-031-28073-3
J9  - LECT NOTE NETW SYST
PY  - 2023
VL  - 652
SP  - 482
EP  - 501
DO  - 10.1007/978-3-031-28073-3_35
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001004145900035
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  30
ER  -

TY  - JOUR
AU  - Fard, NE
AU  - Selmic, RR
AU  - Khorasani, K
TI  - A Review of Techniques and Policies on Cybersecurity Using Artificial Intelligence and Reinforcement Learning Algorithms
T2  - IEEE TECHNOLOGY AND SOCIETY MAGAZINE
LA  - English
KW  - Ethics
KW  - Defense industry
KW  - Focusing
KW  - Reinforcement learning
KW  - Cyber-physical systems
KW  - Artificial intelligence
KW  - Cyberattack
KW  - EDGE
KW  - ATTACKS
AB  - Cybersecurity is a critical process that safeguards networks, systems, and applications against cyber-attacks, wherein digital information is targeted for unauthorized access, manipulation, or destruction. As attackers continually evolve their tactics, addressing cybersecurity challenges has become paramount, especially in sensitive domains like the military and defense industries. This article delves into the challenges that artificial intelligence (AI) faces in the military domain, specifically focusing on defense applications. We review AI algorithms relevant to defense, examining their potential applications and benefits: much of this study revolves around cybersecurity in defense applications, particularly within cyber-physical systems (CPS). We explore reinforcement learning (RL) and deep RL (DRL) algorithms in CPS, aiming to enhance understanding of the cybersecurity implications in this domain. In this context, we present RL and DRL algorithms employed in cyber-attacks and their potential threats and vulnerabilities. Furthermore, we discuss how RL and DRL algorithms can be effectively leveraged for cyber-attack detection and defense applications, providing usable insights into bolstering CPS cybersecurity. By addressing both technical aspects and ethical considerations, this article offers a comprehensive view of the challenges and opportunities surrounding cybersecurity in defense applications.
AD  - Concordia Univ, Dept Elect & Comp Engn, Montreal, PQ H3G 1M8, CanadaC3  - Concordia University - CanadaFU  - Department of National Defense (DND) through Mobilizing Insights in Defense and Security (MINDS) Program [XF0224]
FX  - The content of this manuscript has appeared in part on the Concordia University website for Security-Policy Nexus of Emerging Technology, Concordia's Collaborative Network, SPNET: https://www.concordia.ca/ginacody/research/spnet.html. This work was supported by the Department of National Defense (DND) through the Mobilizing Insights in Defense and Security (MINDS) Program under Grant #XF0224. The statements made herein are solely the responsibility of the authors
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 0278-0097
SN  - 1937-416X
J9  - IEEE TECHNOL SOC MAG
JI  - IEEE Technol. Soc. Mag.
DA  - SEP
PY  - 2023
VL  - 42
IS  - 3
SP  - 57
EP  - 68
DO  - 10.1109/MTS.2023.3306540
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001075283300010
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  41
ER  -

TY  - CPAPER
AU  - Hawkins, W
AU  - Mittelstadt, B
A1  - ASSOC COMPUTING MACHINERY
TI  - The ethical ambiguity of AI data enrichment: Measuring gaps in research ethics norms and practices
T2  - PROCEEDINGS OF THE 6TH ACM CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, FACCT 2023
LA  - English
CP  - 6th ACM Conference on Fairness, Accountability, and Transparency (FAccT)
KW  - data enrichment
KW  - artificial intelligence
KW  - research ethics
AB  - The technical progression of artificial intelligence (AI) research has been built on breakthroughs in fields such as computer science, statistics, and mathematics. However, in the past decade AI researchers have increasingly looked to the social sciences, turning to human interactions to solve the challenges of model development. Paying crowdsourcing workers to generate or curate data, or 'data enrichment', has become indispensable for many areas of AI research, from natural language processing to reinforcement learning from human feedback (RLHF). Other fields that routinely interact with crowdsourcing workers, such as Psychology, have developed common governance requirements and norms to ensure research is undertaken ethically. This study explores how, and to what extent, comparable research ethics requirements and norms have developed for AI research and data enrichment. We focus on the approach taken by two leading conferences: ICLR and NeurIPS, and journal publisher Springer. In a longitudinal study of accepted papers, and via a comparison with Psychology and CHI papers, this work finds that leading AI venues have begun to establish protocols for human data collection, but these are are inconsistently followed by authors. Whilst Psychology papers engaging with crowdsourcing workers frequently disclose ethics reviews, payment data, demographic data and other information, similar disclosures are far less common in leading AI venues despite similar guidance. The work concludes with hypotheses to explain these gaps in research ethics practices and considerations for its implications.
AD  - Univ Oxford, Oxford Internet Inst, Oxford, EnglandC3  - University of OxfordFU  - Wellcome Trust [223765/Z/21/Z]; Sloan Foundation [G-2021-16779]; Department of Health and Social Care (via the AI Lab at NHSx); Luminate Group; Wellcome Trust [223765/Z/21/Z] Funding Source: Wellcome Trust
FX  - This work has been supported through research funding provided by the Wellcome Trust (grant nr 223765/Z/21/Z), Sloan Foundation grant nr G-2021-16779), the Department of Health and Social Care (via the AI Lab at NHSx), and Luminate Group. During the course of this work Will Hawkins held an employed position at DeepMind.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-7252-7
PY  - 2023
SP  - 261
EP  - 270
DO  - 10.1145/3593013.3593995
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:001062819300026
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  75
ER  -

TY  - CPAPER
AU  - Tassella, M
AU  - Chaput, R
AU  - Guillermin, M
ED  - Cheong, M
ED  - Herkert, J
ED  - Hess, J
TI  - Artificial Moral Advisors: enhancing human ethical decision-making
T2  - 2023 IEEE INTERNATIONAL SYMPOSIUM ON ETHICS IN ENGINEERING, SCIENCE, AND TECHNOLOGY, ETHICS
LA  - English
CP  - IEEE International Symposium on Ethics in Engineering, Science, and Technology (IEEE ETHICS) - Ethics in the Global Innovation Helix
KW  - Moral Dilemmas
KW  - Artificial Moral Advisors
KW  - AI Moral Enhancement
KW  - Reinforcement Learning
AB  - This short paper focuses on understanding moral dilemmas, Artificial Moral Advisors, and their possible roles in ethical decision-making. After a brief analysis of the philosophical debate around dilemmas, we propose three different classes of dilemmas. We then discuss how AI-based advisors could be used to enhance human ethical decision-making, with a particular focus on three possible AI skills (identifying, presenting and settling dilemmas), as well as on their role as ethical experts. The resulting proposal opens up to new possible uses of AI moral advisors, and to the help they might offer in difficult decisions.
AD  - Lyon Catholic Univ, LUMSA Univ, Rome, ItalyAD  - Lyon Catholic Univ, CONFLUENCE Sci & Humanities, Res Unit EA 1598, Lyon, FranceC3  - Universita LUMSAFU  - CONFLUENCE: Sciences et Humanites research unit of the Catholic University of Lyon [EA 1598]
FX  - This work was carried out within the framework of the NHNAI (nhnai.org) project, supported by the CONFLUENCE: Sciences et Humanites research unit (EA 1598) of the Catholic University of Lyon.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-5713-2
PY  - 2023
DO  - 10.1109/ETHICS57328.2023.10155026
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:001022323800030
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  23
ER  -

TY  - JOUR
AU  - Neufeld, EA
AU  - Bartocci, E
AU  - Ciabattoni, A
AU  - Governatori, G
TI  - Enforcing ethical goals over reinforcement-learning policies
T2  - ETHICS AND INFORMATION TECHNOLOGY
LA  - English
KW  - Deontic defeasible logic
KW  - Reinforcement learning
KW  - Normative reasoning
KW  - Ethical artificial intelligence
KW  - NORMS
KW  - LOGIC
AB  - Recent years have yielded many discussions on how to endow autonomous agents with the ability to make ethical decisions, and the need for explicit ethical reasoning and transparency is a persistent theme in this literature. We present a modular and transparent approach to equip autonomous agents with the ability to comply with ethical prescriptions, while still enacting pre-learned optimal behaviour. Our approach relies on a normative supervisor module, that integrates a theorem prover for defeasible deontic logic within the control loop of a reinforcement learning agent. The supervisor operates as both an event recorder and an on-the-fly compliance checker w.r.t. an external norm base. We successfully evaluated our approach with several tests using variations of the game Pac-Man, subject to a variety of "ethical" constraints.
AD  - TU Wien, Vienna, AustriaAD  - CSIRO, Data61, Brisbane, Qld, AustraliaC3  - Technische Universitat WienC3  - Commonwealth Scientific & Industrial Research Organisation (CSIRO)FU  - WWTF project [MA16-28]; DC-RES
FX  - This work was partially supported by WWTF project MA16-28 and the DC-RES run by the TU Wien's Faculty of Informatics and the FH-Technikum Wien.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1388-1957
SN  - 1572-8439
J9  - ETHICS INF TECHNOL
JI  - Ethics Inf. Technol.
DA  - DEC
PY  - 2022
VL  - 24
IS  - 4
C7  - 43
DO  - 10.1007/s10676-022-09665-8
WE  - Social Science Citation Index (SSCI)WE  - Arts &amp; Humanities Citation Index (A&amp;HCI)AN  - WOS:000861892000001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  40
ER  -

TY  - JOUR
AU  - Haas, J
TI  - Moral Gridworlds: A Theoretical Proposal for Modeling Artificial Moral Cognition
T2  - MINDS AND MACHINES
LA  - English
KW  - Artificial intelligence
KW  - Moral AI
KW  - Moral cognition
KW  - Machine ethics
KW  - Moral psychology
KW  - Reinforcement learning
KW  - Fairness
KW  - DECISION-MAKING
KW  - ULTIMATUM GAME
KW  - AUTONOMOUS VEHICLES
KW  - FAIRNESS
KW  - ETHICS
KW  - AI
KW  - EVOLUTION
KW  - MECHANISMS
KW  - REPRESENTATIONS
KW  - ARCHITECTURE
AB  - I describe a suite of reinforcement learning environments in which artificial agents learn to value and respond to moral content and contexts. I illustrate the core principles of the framework by characterizing one such environment, or "gridworld," in which an agent learns to trade-off between monetary profit and fair dealing, as applied in a standard behavioral economic paradigm. I then highlight the core technical and philosophical advantages of the learning approach for modeling moral cognition, and for addressing the so-called value alignment problem in AI.
AD  - Rhodes Coll, Dept Philosophy, Memphis, TN 38112 USAPU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0924-6495
SN  - 1572-8641
J9  - MIND MACH
JI  - Minds Mach.
DA  - JUN
PY  - 2020
VL  - 30
IS  - 2
SP  - 219
EP  - 246
DO  - 10.1007/s11023-020-09524-9
C6  - APR 2020
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)AN  - WOS:000528667300001
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  118
ER  -

TY  - JOUR
AU  - Taherdoost, H
AU  - Madanchian, M
TI  - AI Advancements: Comparison of Innovative Techniques
T2  - AI
LA  - English
KW  - artificial intelligence (AI)
KW  - emerging frontiers
KW  - cutting-edge techniques
KW  - evaluation frameworks
KW  - ethical considerations
KW  - performance metrics
KW  - generative adversarial networks
KW  - ARTIFICIAL-INTELLIGENCE
KW  - NETWORKS
KW  - EVOLUTION
AB  - In recent years, artificial intelligence (AI) has seen remarkable advancements, stretching the limits of what is possible and opening up new frontiers. This comparative review investigates the evolving landscape of AI advancements, providing a thorough exploration of innovative techniques that have shaped the field. Beginning with the fundamentals of AI, including traditional machine learning and the transition to data-driven approaches, the narrative progresses through core AI techniques such as reinforcement learning, generative adversarial networks, transfer learning, and neuroevolution. The significance of explainable AI (XAI) is emphasized in this review, which also explores the intersection of quantum computing and AI. The review delves into the potential transformative effects of quantum technologies on AI advancements and highlights the challenges associated with their integration. Ethical considerations in AI, including discussions on bias, fairness, transparency, and regulatory frameworks, are also addressed. This review aims to contribute to a deeper understanding of the rapidly evolving field of AI. Reinforcement learning, generative adversarial networks, and transfer learning lead AI research, with a growing emphasis on transparency. Neuroevolution and quantum AI, though less studied, show potential for future developments.
AD  - Univ Canada West, Dept Arts Commun & Social Sci, Vancouver, BC V6B 1V9, CanadaAD  - Q Minded Quark Minded Technol Inc, Vancouver, BC V6E 1C9, CanadaAD  - Hamta Business Corp, Res & Dev Dept, Vancouver, BC V6E 1C9, CanadaPU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2673-2688
J9  - AI-BASEL
JI  - AI
DA  - MAR
PY  - 2024
VL  - 5
IS  - 1
SP  - 38
EP  - 54
DO  - 10.3390/ai5010003
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:001191509300001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  107
ER  -

TY  - CPAPER
AU  - Ecoffet, A
AU  - Lehman, J
ED  - Meila, M
ED  - Zhang, T
TI  - Reinforcement Learning Under Moral Uncertainty
T2  - INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 139
LA  - English
CP  - International Conference on Machine Learning (ICML)
KW  - SOCIAL RATE
AB  - An ambitious goal for machine learning is to create agents that behave ethically: The capacity to abide by human moral norms would greatly expand the context in which autonomous agents could be practically and safely deployed, e.g. fully autonomous vehicles will encounter charged moral decisions that complicate their deployment. While ethical agents could be trained by rewarding correct behavior under a specific moral theory (e.g. utilitarianism), there remains widespread disagreement about the nature of morality. Acknowledging such disagreement, recent work in moral philosophy proposes that ethical behavior requires acting under moral uncertainty, i.e. to take into account when acting that one's credence is split across several plausible ethical theories. This paper translates such insights to the field of reinforcement learning, proposes two training methods that realize different points among competing desiderata, and trains agents in simple environments to act under moral uncertainty. The results illustrate (1) how such uncertainty can help curb extreme behavior from commitment to single theories and (2) several technical complications arising from attempting to ground moral philosophy in RL (e.g. how can a principled trade-off between two competing but incomparable reward functions be reached). The aim is to catalyze progress towards morally-competent agents and highlight the potential of RL to contribute towards the computational grounding of moral philosophy.
AD  - Uber AI Labs, San Francisco, CA 94103 USAAD  - OpenAI, San Francisco, CA 94110 USAC3  - OpenAIPU  - JMLR-JOURNAL MACHINE LEARNING RESEARCH
PI  - SAN DIEGO
PA  - 1269 LAW ST, SAN DIEGO, CA, UNITED STATES
SN  - 2640-3498
J9  - PR MACH LEARN RES
PY  - 2021
VL  - 139
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000683104602085
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  47
ER  -

TY  - JOUR
AU  - Habehh, H
AU  - Gohel, S
TI  - Machine Learning in Healthcare
T2  - CURRENT GENOMICS
LA  - English
KW  - Machine learning
KW  - healthcare
KW  - support vector machine
KW  - EHR
KW  - genomics
KW  - artificial intelligence
KW  - BIG DATA
KW  - DEEP
KW  - PREDICTION
KW  - CLASSIFICATION
KW  - PHARMACOGENOMICS
KW  - PSYCHOSIS
KW  - CANCER
AB  - Recent advancements in Artificial Intelligence (AI) and Machine Learning (ML) technol-ogy have brought on substantial strides in predicting and identifying health emergencies, disease populations, and disease state and immune response, amongst a few. Although, skepticism remains regarding the practical application and interpretation of results from ML-based approaches in healthcare settings, the inclusion of these approaches is increasing at a rapid pace. Here we provide a brief overview of machine learning-based approaches and learning algorithms including super -vised, unsupervised, and reinforcement learning along with examples. Second, we discuss the appli-cation of ML in several healthcare fields, including radiology, genetics, electronic health records, and neuroimaging. We also briefly discuss the risks and challenges of ML application to healthcare such as system privacy and ethical concerns and provide suggestions for future applications.
AD  - Rutgers State Univ, Sch Hlth Profess, Dept Hlth Informat, 65 Bergen St, Newark, NJ 07107 USAC3  - Rutgers University SystemC3  - Rutgers University New BrunswickC3  - Rutgers University NewarkFU  - NIH/NCATS [UL1TR003017]
FX  - This work was partly supported by NIH/NCATS UL1TR003017. This work is solely the responsibility of the authors and does not necessarily represent the official views of the NIH, NCATS.
PU  - BENTHAM SCIENCE PUBL LTD
PI  - SHARJAH
PA  - EXECUTIVE STE Y-2, PO BOX 7917, SAIF ZONE, 1200 BR SHARJAH, U ARAB EMIRATES
SN  - 1389-2029
SN  - 1875-5488
J9  - CURR GENOMICS
JI  - Curr. Genomics
PY  - 2021
VL  - 22
IS  - 4
SP  - 291
EP  - 300
DO  - 10.2174/1389202922666210705124359
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000729048700005
N1  - Times Cited in Web of Science Core Collection:  44
Total Times Cited:  48
Cited Reference Count:  103
ER  -

TY  - JOUR
AU  - Pinka, R
TI  - Synthetic Deliberation: Can Emulated Imagination Enhance Machine Ethics?
T2  - MINDS AND MACHINES
LA  - English
KW  - Artificial intelligence
KW  - Machine ethics
KW  - Pragmatism
KW  - Machine learning
KW  - STS
KW  - Philosophy of technology
AB  - Artificial intelligence is becoming increasingly entwined with our daily lives: AIs work as assistants through our phones, control our vehicles, and navigate our vacuums. As these objects become more complex and work within our societies in ways that affect our well-being, there is a growing demand for machine ethics-we want a guarantee that the various automata in our lives will behave in a way that minimizes the amount of harm they create. Though many technologies exist as moral artifacts (and perhaps moral agents), the development of a truly ethical AI system is highly contentious; theorists have proposed and critiqued countless possibilities for programming these agents to become ethical. Many of these arguments, however, presuppose the possibility that an artificially intelligent system can actually be ethical. In this essay, I will explore a potential path to AI ethics by considering the role of imagination in the deliberative process via the work of John Dewey and his interpreters, showcasing one form of reinforcement learning that mimics imaginative deliberation. With these components in place, I contend that such an artificial agent is capable of something very near ethical behavior-close enough that we may consider it so.
AD  - Univ North Carolina Charlotte, Dept Philosophy, Charlotte, NC 28223 USAC3  - University of North CarolinaC3  - University of North Carolina CharlottePU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0924-6495
SN  - 1572-8641
J9  - MIND MACH
JI  - Minds Mach.
DA  - MAR
PY  - 2021
VL  - 31
IS  - 1
SP  - 121
EP  - 136
DO  - 10.1007/s11023-020-09531-w
C6  - JUL 2020
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000555521900001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  16
ER  -

TY  - JOUR
AU  - Livingston, S
AU  - Risse, M
TI  - The Future Impact of Artificial Intelligence on Humans and Human Rights
T2  - ETHICS & INTERNATIONAL AFFAIRS
LA  - English
KW  - artificial intelligence
KW  - machine learning
KW  - deep learning
KW  - reinforcement learning
KW  - superintelligence
KW  - artificial general intelligence
KW  - ethical impact agents
KW  - implicit ethical agents
KW  - categorical imperative
KW  - human rights
AB  - What are the implications of artificial intelligence (AI) on human rights in the next three decades? Precise answers to this question are made difficult by the rapid rate of innovation in AI research and by the effects of human practices on the adaption of new technologies. Precise answers are also challenged by imprecise usages of the term "AI." There are several types of research that all fall under this general term. We begin by clarifying what we mean by AI. Most of our attention is then focused on the implications of artificial general intelligence (AGI), which entail that an algorithm or group of algorithms will achieve something like superintelligence. While acknowledging that the feasibility of superintelligence is contested, we consider the moral and ethical implications of such a potential development. What do machines owe humans and what do humans owe superintelligent machines?
AD  - George Washington Univ, Media & Publ Affairs & Int Afairs, Washington, DC 20052 USAAD  - Harvard Univ, Carr Ctr Human Rights Policy, John F Kennedy Sch Govt, Cambridge, MA 02138 USAAD  - Harvard Univ, John F Kennedy Sch Govt, Philosophy & Publ Adm, Cambridge, MA 02138 USAC3  - George Washington UniversityC3  - Harvard UniversityC3  - Harvard UniversityPU  - CAMBRIDGE UNIV PRESS
PI  - NEW YORK
PA  - 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN  - 0892-6794
SN  - 1747-7093
J9  - ETHICS INT AFF
JI  - Ethics Int. Aff.
DA  - SUM
PY  - 2019
VL  - 33
IS  - 2
SP  - 141
EP  - 158
DO  - 10.1017/S089267941900011X
WE  - Social Science Citation Index (SSCI)AN  - WOS:000470740100003
N1  - Times Cited in Web of Science Core Collection:  27
Total Times Cited:  30
Cited Reference Count:  69
ER  -

TY  - JOUR
AU  - Rodriguez-Soto, M
AU  - Lopez-Sanchez, M
AU  - Rodriguez-Aguilar, JA
TI  - Multi-objective reinforcement learning for designing ethical multi-agent environments
T2  - NEURAL COMPUTING & APPLICATIONS
LA  - English
KW  - Value alignment
KW  - Multi-agent reinforcement learning
KW  - Multi-objective reinforcement learning
KW  - Ethics
AB  - This paper tackles the open problem of value alignment in multi-agent systems. In particular, we propose an approach to build an ethical environment that guarantees that agents in the system learn a joint ethically-aligned behaviour while pursuing their respective individual objectives. Our contributions are founded in the framework of Multi-Objective Multi-Agent Reinforcement Learning. Firstly, we characterise a family of Multi-Objective Markov Games (MOMGs), the so-called ethical MOMGs, for which we can formally guarantee the learning of ethical behaviours. Secondly, based on our characterisation we specify the process for building single-objective ethical environments that simplify the learning in the multi-agent system. We illustrate our process with an ethical variation of the Gathering Game, where agents manage to compensate social inequalities by learning to behave in alignment with the moral value of beneficence.
AD  - CSIC, Artificial intelligence Res Inst IIIA, Carrer Can Planas,Campus UAB,, Bellaterra 08193, SpainAD  - Univ Barcelona, Dept Math & Comp Sci, Gran Via Corts Catalanes 585, E-08007 Barcelona, SpainC3  - Autonomous University of BarcelonaC3  - Consejo Superior de Investigaciones Cientificas (CSIC)C3  - CSIC - Instituto de Investigacion en Inteligencia Artificial (IIIA)C3  - University of BarcelonaFU  - Barcelona City Council through the Fundacio Solidaritat de la UB [HE-101070930, H2020-872944, H2020-952215, H2020-785907, 22S01386-001]; MCIN/AEI [PID2019-104156GB-I00]; Spanish Government with an FPU;  [FPU18/03387]
FX  - Work funded by projects VALAWAI (HE-101070930), Crowd4SDG (H2020-872944), TAILOR (H2020-952215), COREDEM (H2020-785907), and 22S01386-001 from Barcelona City Council through the Fundacio Solidaritat de la UB. Financial support was also received from grant PID2019-104156GB-I00 funded by MCIN/AEI/10.13039/501100011033. Manel Rodriguez-Soto was funded by the Spanish Government with an FPU grant (ref. FPU18/03387).
PU  - SPRINGER LONDON LTD
PI  - LONDON
PA  - 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN  - 0941-0643
SN  - 1433-3058
J9  - NEURAL COMPUT APPL
JI  - Neural Comput. Appl.
DA  - 2023 AUG 23
PY  - 2023
DO  - 10.1007/s00521-023-08898-y
C6  - AUG 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001151010600001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  80
ER  -

TY  - JOUR
AU  - Reinecke, MG
AU  - Mao, YR
AU  - Kunesch, M
AU  - Duóñez-Guzmán, EA
AU  - Haas, J
AU  - Leibo, JZ
TI  - The Puzzle of Evaluating Moral Cognition in Artificial Agents
T2  - COGNITIVE SCIENCE
LA  - English
KW  - Moral cognition
KW  - Artificial intelligence
KW  - Multi-agent reinforcement learning
KW  - JUDGMENTS
KW  - INTENT
AB  - In developing artificial intelligence (AI), researchers often benchmark against human performance as a measure of progress. Is this kind of comparison possible for moral cognition? Given that human moral judgment often hinges on intangible properties like "intention" which may have no natural analog in artificial agents, it may prove difficult to design a "like-for-like" comparison between the moral behavior of artificial and human agents. What would a measure of moral behavior for both humans and AI look like? We unravel the complexity of this question by discussing examples within reinforcement learning and generative AI, and we examine how the puzzle of evaluating artificial agents' moral cognition remains open for further investigation within cognitive science.
AD  - Google DeepMind, London, EnglandAD  - Yale Univ, Dept Psychol, New Haven, CT USAAD  - Yale Univ, Dept Psychol, 100 Coll St, New Haven, CT 06510 USAAD  - Google Deep Mind, London N1C 4DN, EnglandC3  - Google IncorporatedC3  - Yale UniversityC3  - Yale UniversityC3  - Google IncorporatedPU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN  - 0364-0213
SN  - 1551-6709
J9  - COGNITIVE SCI
JI  - Cogn. Sci.
DA  - AUG
PY  - 2023
VL  - 47
IS  - 8
C7  - e13315
DO  - 10.1111/cogs.13315
WE  - Social Science Citation Index (SSCI)AN  - WOS:001044873800001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  52
ER  -

TY  - JOUR
AU  - Zhang, GX
AU  - Kashima, H
TI  - Learning state importance for preference-based reinforcement learning
T2  - MACHINE LEARNING
LA  - English
KW  - Interpretable reinforcement learning
KW  - Preference-based reinforcement learning
KW  - Human-in-the-loop reinforcement learning
KW  - Interpretability artificial intelligence
AB  - Preference-based reinforcement learning (PbRL) develops agents using human preferences. Due to its empirical success, it has prospect of benefiting human-centered applications. Meanwhile, previous work on PbRL overlooks interpretability, which is an indispensable element of ethical artificial intelligence (AI). While prior art for explainable AI offers some machinery, there lacks an approach to select samples to construct explanations. This becomes an issue for PbRL, as transitions relevant to task solving are often outnumbered by irrelevant ones. Thus, ad-hoc sample selection undermines the credibility of explanations. The present study proposes a framework for learning reward functions and state importance from preferences simultaneously. It offers a systematic approach for selecting samples when constructing explanations. Moreover, the present study proposes a perturbation analysis to evaluate the learned state importance quantitatively. Through experiments on discrete and continuous control tasks, the present study demonstrates the proposed framework's efficacy for providing interpretability without sacrificing task performance.
AD  - Kyoto Univ, Grad Sch Informat, Yoshida Honmachi, Kyoto 6068501, JapanAD  - RIKEN Guardian Robot Project, Kyoto, JapanC3  - Kyoto UniversityFU  - JST CREST [JPMJCR21D1]
FX  - Hisashi Kashima is supported by the JST CREST (Grant No.: JPMJCR21D1).
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0885-6125
SN  - 1573-0565
J9  - MACH LEARN
JI  - Mach. Learn.
DA  - 2023 JAN 9
PY  - 2023
DO  - 10.1007/s10994-022-06295-5
C6  - JAN 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000911232800001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  39
ER  -

TY  - CPAPER
AU  - Naderializadeh, N
AU  - Soleyman, S
AU  - Hung, F
AU  - Khosla, D
AU  - Chen, Y
AU  - Fadaie, JG
ED  - Pham, T
ED  - Solomon, L
TI  - Distributed hierarchical reinforcement learning in multi-agent adversarial environments
T2  - ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR MULTI-DOMAIN OPERATIONS APPLICATIONS IV
LA  - English
CP  - Conference on Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications IV
KW  - distributed
KW  - decentralized
KW  - neuroevolution
KW  - reinforcement learning
KW  - machine learning
KW  - artificial intelligence
KW  - multi-agent
KW  - adversarial
KW  - NEURAL-NETWORKS
AB  - We develop a hierarchical approach for controlling a team of aircraft in multi-agent adversarial environments. Each individual aircraft is equipped with a high-level agent that is solely responsible for target assignment decisions, and a low-level agent that generates actions based only on the selected target. We use distributed deep reinforcement learning to train the high-level agents, and neuroevolution to train the low-level agents. This approach leverages centralized training for decentralized execution to enable individual autonomy when communication is limited. Simulation results confirm the superiority of our proposed approach as compared to non-hierarchical multi-agent reinforcement learning methods.
AD  - HRL Labs LLC, 3011 Malibu Canyon Rd, Malibu, CA 90265 USAAD  - Univ Penn, 200 South 33rd St, Philadelphia, PA 19104 USAC3  - HRL LaboratoriesC3  - University of PennsylvaniaPU  - SPIE-INT SOC OPTICAL ENGINEERING
PI  - BELLINGHAM
PA  - 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN  - 0277-786X
SN  - 1996-756X
SN  - 978-1-5106-5103-6
SN  - 978-1-5106-5102-9
J9  - PROC SPIE
PY  - 2022
VL  - 12113
C7  - 121130L
DO  - 10.1117/12.2616582
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000844508600017
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  35
ER  -

TY  - JOUR
AU  - Kang, YH
AU  - Gao, S
AU  - Roth, RE
TI  - Artificial intelligence studies in cartography: a review and synthesis of methods, applications, and ethics
T2  - CARTOGRAPHY AND GEOGRAPHIC INFORMATION SCIENCE
LA  - English
KW  - GeoAI
KW  - cartography
KW  - maps
KW  - Artificial intelligence
KW  - deep learning
KW  - ethics
KW  - GEOGRAPHIC INFORMATION-SYSTEMS
KW  - MAP GENERALIZATION
KW  - NEURAL-NETWORKS
KW  - AI
KW  - CLASSIFICATION
KW  - UNCERTAINTY
KW  - GEOPRIVACY
KW  - KNOWLEDGE
KW  - PATTERNS
AB  - The past decade has witnessed the rapid development of geospatial artificial intelligence (GeoAI) primarily due to the ground-breaking achievements in deep learning and machine learning. A growing number of scholars from cartography have demonstrated successfully that GeoAI can accelerate previously complex cartographic design tasks and even enable cartographic creativity in new ways. Despite the promise of GeoAI, researchers and practitioners have growing concerns about the ethical issues of GeoAI for cartography. In this paper, we conducted a systematic content analysis and narrative synthesis of research studies integrating GeoAI and cartography to summarize current research and development trends regarding the usage of GeoAI for cartographic design. Based on this review and synthesis, we first identify dimensions of GeoAI methods for cartography such as data sources, data formats, map evaluations, and six contemporary GeoAI models, each of which serves a variety of cartographic tasks. These models include decision trees, knowledge graph and semantic web technologies, deep convolutional neural networks, generative adversarial networks, graph neural networks, and reinforcement learning. Further, we summarize seven cartographic design applications where GeoAI have been effectively employed: generalization, symbolization, typography, map reading, map interpretation, map analysis, and map production. We also raise five potential ethical challenges that need to be addressed in the integration of GeoAI for cartography: commodification, responsibility, privacy, bias, and (together) transparency, explainability, and provenance. We conclude by identifying four potential research directions for future cartographic research with GeoAI: GeoAI-enabled active cartographic symbolism, human-in-the-loop GeoAI for cartography, GeoAI-based mapping-as-a-service, and generative GeoAI for cartography.
AD  - Univ Wisconsin Madison, Dept Geog, Madison, WI 53715 USAAD  - Univ South Carolina, Dept Geog, GISense Lab, Columbia, SC USAC3  - University of Wisconsin SystemC3  - University of Wisconsin MadisonC3  - University of South Carolina SystemC3  - University of South Carolina ColumbiaFU  - Trewartha Research Award, Department of Geography, University of Wisconsin-Madison, and the Master's Thesis Research Grant of the AAG Cartography Specialty Group; Trewartha Research Award at the University of Wisconsin-Madison, Master's Thesis Research Grant of the AAG Cartography Specialty Group
FX  - The authors would like to thank people who attended the AutoCarto 2022 Conference and CartoAI workshop at the GIScience 2023 conference. The feedback and insights from those who participated the discussions have been valuable for this paper. In particular, Dr. Liqiu Meng, Mark Cygan, Dr. Alexander Kent, and Dr. Feng Yu. The authors would also like to acknowledge the anonymous reviewers whose critical reviews and suggestions have helped improve the quality of this paper. This work is supported by the Trewartha Research Award at the University of Wisconsin-Madison, Master's Thesis Research Grant of the AAG Cartography Specialty Group. During the writing of the manuscript, ChatGPT was utilized as a tool solely for proofreading purposes, without contributing any ideas or perspectives to the content of the paper.
PU  - TAYLOR & FRANCIS INC
PI  - PHILADELPHIA
PA  - 530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA
SN  - 1523-0406
SN  - 1545-0465
J9  - CARTOGR GEOGR INF SC
JI  - Cartogr. Geogr. Inf. Sci.
DA  - 2024 JAN 18
PY  - 2024
DO  - 10.1080/15230406.2023.2295943
C6  - JAN 2024
WE  - Social Science Citation Index (SSCI)AN  - WOS:001143020900001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  238
ER  -

TY  - CPAPER
AU  - Chaput, R
AU  - Duval, J
AU  - Boissier, O
AU  - Guillermin, M
AU  - Hassas, S
A1  - ASSOC COMP MACHINERY
TI  - A Multi-Agent Approach to Combine Reasoning and Learning for an Ethical Behavior
T2  - AIES '21: PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY
LA  - English
CP  - 4th AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - Ethics
KW  - Machine Ethics
KW  - Multi-Agent Learning
KW  - Reinforcement Learning
KW  - Hybrid Neural-Symbolic Learning
KW  - Ethical Judgment
AB  - The recent field of Machine Ethics is experiencing rapid growth to answer the societal need for Artificial Intelligence (AI) algorithms imbued with ethical considerations, such as benevolence toward human users and actors. Several approaches already exist for this purpose, mostly either by reasoning over a set of predefined ethical principles (Top-Down), or by learning new principles (Bottom-Up). While both methods have their own advantages and drawbacks, only few works have explored hybrid approaches, such as using symbolic rules to guide the learning process for instance, combining the advantages of each. This paper draws upon existing works to propose a novel hybrid method using symbolic judging agents to evaluate the ethics of learning agents' behaviors, and accordingly improve their ability to ethically behave in dynamic multi-agent environments. Multiple benefits ensue from this separation between judging and learning agents: agents can evolve (or be updated by human designers) separately, benefiting from co-construction processes; judging agents can act as accessible proxies for non-expert human stakeholders or regulators; and finally, multiple points of view (one per judging agent) can be adopted to judge the behavior of the same agent, which produces a richer feedback. Our proposed approach is applied to an energy distribution problem, in the context of a Smart Grid simulator, with continuous and multi-dimensional states and actions. The experiments and results show the ability of learning agents to correctly adapt their behaviors to comply with the judging agents' rules, including when rules evolve over time.
AD  - Univ Lyon, Univ Lyon 1, LIRIS, UMR5205, F-69622 Lyon, FranceAD  - Univ Clermont Auvergne, Mines St Etienne, CNRS, UMR 6158,LIMOS,Inst Henri Fayol, F-42023 St Etienne, FranceAD  - Lyon Catholic Univ UCLy, Sci & Humanities Confluence Res Ctr EA 1598, F-69288 Lyon, FranceC3  - Universite Claude Bernard Lyon 1C3  - Institut National des Sciences Appliquees de Lyon - INSA LyonC3  - Centre National de la Recherche Scientifique (CNRS)C3  - Universite Clermont Auvergne (UCA)C3  - IMT - Institut Mines-TelecomC3  - Mines Saint-EtienneFU  - French Region Auvergne Rhones-Alpes (AURA), as part of the Ethics.AI project (Pack Ambition Recherche)
FX  - This work is funded by the French Region Auvergne Rhones-Alpes (AURA), as part of the Ethics.AI project (Pack Ambition Recherche). We gratefully acknowledge support from the CNRS/IN2P3 Computing Center (Lyon -France) for providing computing and dataprocessing resources needed for this work
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-8473-5
PY  - 2021
SP  - 13
EP  - 23
DO  - 10.1145/3461702.3462515
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000767973400004
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  35
ER  -

TY  - JOUR
AU  - Kim, MY
AU  - Atakishiyev, S
AU  - Babiker, HKB
AU  - Farruque, N
AU  - Goebel, R
AU  - Zaïane, OR
AU  - Motallebi, MH
AU  - Rabelo, J
AU  - Syed, T
AU  - Yao, HS
AU  - Chun, P
TI  - A Multi-Component Framework for the Analysis and Design of Explainable Artificial Intelligence
T2  - MACHINE LEARNING AND KNOWLEDGE EXTRACTION
LA  - English
KW  - interpretation
KW  - explanation
KW  - explainable artificial intelligence
KW  - causal explanation
KW  - explainee-specific explanation
KW  - evaluation of explainable AI
AB  - The rapid growth of research in explainable artificial intelligence (XAI) follows on two substantial developments. First, the enormous application success of modern machine learning methods, especially deep and reinforcement learning, have created high expectations for industrial, commercial, and social value. Second, the emerging and growing concern for creating ethical and trusted AI systems, including compliance with regulatory principles to ensure transparency and trust. These two threads have created a kind of "perfect storm" of research activity, all motivated to create and deliver any set of tools and techniques to address the XAI demand. As some surveys of current XAI suggest, there is yet to appear a principled framework that respects the literature of explainability in the history of science and which provides a basis for the development of a framework for transparent XAI. We identify four foundational components, including the requirements for (1) explicit explanation knowledge representation, (2) delivery of alternative explanations, (3) adjusting explanations based on knowledge of the explainee, and (4) exploiting the advantage of interactive explanation. With those four components in mind, we intend to provide a strategic inventory of XAI requirements, demonstrate their connection to a basic history of XAI ideas, and then synthesize those ideas into a simple framework that can guide the design of AI systems that require XAI.
AD  - Univ Alberta, Alberta Machine Intelligence Inst, Dept Comp Sci, XAI Lab, Edmonton, AB T6G 2E8, CanadaAD  - Univ Alberta, Augustana Fac, Dept Sci, Camrose, AB T4V 2R3, CanadaAD  - Huawei Technol, Edmonton, AB T6G 2E8, CanadaAD  - Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2E8, CanadaAD  - Huawei Technol Canada, Markham, ON L3R 5A4, CanadaAD  - Univ Alberta, Dept Comp Sci, 2-21 Athabasca Hall, Edmonton, AB T6G 2E8, CanadaC3  - University of AlbertaC3  - University of AlbertaC3  - Huawei TechnologiesC3  - University of AlbertaC3  - Huawei TechnologiesC3  - University of AlbertaPU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2504-4990
J9  - MACH LEARN KNOW EXTR
JI  - Mach. Learn. Knowl. Extr.
DA  - DEC
PY  - 2021
VL  - 3
IS  - 4
SP  - 900
EP  - 921
DO  - 10.3390/make3040045
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000737172800001
N1  - Times Cited in Web of Science Core Collection:  14
Total Times Cited:  14
Cited Reference Count:  69
ER  -

TY  - CPAPER
AU  - Neufeld, E
ED  - Rocha, AP
ED  - Steels, L
ED  - VandenHerik, J
TI  - Reinforcement Learning Guided by Provable Normative Compliance
T2  - ICAART: PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE - VOL 3
LA  - English
CP  - 14th International Conference on Agents and Artificial Intelligence (ICAART)
KW  - Reinforcement Learning
KW  - Ethical AI
KW  - Deontic Logic
AB  - Reinforcement learning (RL) has shown promise as a tool for engineering safe, ethical, or legal behaviour in autonomous agents. Its use typically relies on assigning punishments to state-action pairs that constitute unsafe or unethical choices. Despite this assignment being a crucial step in this approach, however, there has been limited discussion on generalizing the process of selecting punishments and deciding where to apply them. In this paper, we adopt an approach that leverages an existing framework - the normative supervisor of (Neufeld et al., 2021) - during training. This normative supervisor is used to dynamically translate states and the applicable normative system into defeasible deontic logic theories, feed these theories to a theorem prover, and use the conclusions derived to decide whether or not to assign a punishment to the agent. We use multi-objective RL (MORL) to balance the ethical objective of avoiding violations with a non-ethical objective; we will demonstrate that our approach works for a multiplicity of MORL techniques, and show that it is effective regardless of the magnitude of the punishment we assign.
AD  - Fac Informat, Vienna, AustriaPU  - SCITEPRESS
PI  - SETUBAL
PA  - AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL
SN  - 2184-433X
SN  - 978-989-758-547-0
J9  - ICAART
PY  - 2022
SP  - 444
EP  - 453
DO  - 10.5220/0010835600003116
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000774776400053
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  26
ER  -

TY  - JOUR
AU  - Triguero, I
AU  - Molina, D
AU  - Poyatos, J
AU  - Del Ser, J
AU  - Herrera, F
TI  - General Purpose Artificial Intelligence Systems (GPAIS): Properties, definition, taxonomy, societal implications and responsible governance
T2  - INFORMATION FUSION
LA  - English
KW  - General-purpose AI
KW  - Meta -learning
KW  - Reinforcement learning
KW  - Neuroevolution
KW  - Few -shot learning
KW  - AutoML
KW  - Transfer learning
KW  - Generative AI
KW  - Large language models
KW  - NEURAL-NETWORKS
AB  - Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research. This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and ability based on several factors such as adaptation to new tasks, competence in domains not intentionally trained for, ability to learn from few data, or proactive acknowledgement of their own limitations. We then propose a taxonomy of approaches to realise GPAIS, describing research trends such as the use of AI techniques to improve another AI (commonly referred to as AI-powered AI) or (single) foundation models. As a prime example, we delve into generative AI (GenAI), aligning them with the terms and concepts presented in the taxonomy. Similarly, we explore the challenges and prospects of multi-modality, which involves fusing various types of data sources to expand the capabilities of GPAIS. Through the proposed definition and taxonomy, our aim is to facilitate research collaboration across different areas that are tackling general purpose tasks, as they share many common aspects. Finally, with the goal of providing a holistic view of GPAIS, we discuss the current state of GPAIS, its prospects, implications for our society, and the need for regulation and governance of GPAIS to ensure their responsible and trustworthy development.
AD  - Univ Granada, Andalusian Res Inst Data Sci & Computat Intelligen, Dept Comp Sci & Artificial Intelligence, Granada 18071, SpainAD  - TECNALIA, Basque Res & Technol Alliance BRTA, Derio 48160, SpainAD  - Univ Basque Country UPV EHU, Bilbao 48013, SpainAD  - Univ Nottingham, Sch Comp Sci, Nottingham NG8 1BB, EnglandC3  - University of GranadaC3  - University of Basque CountryC3  - University of NottinghamFU  - Maria Zambrano Senior Fellowship at the University of Granada; R&D and Innovation project [PID2020-119478GB-I00]; Spain's Ministry of Science and Innovation; European Regional Development Fund (ERDF); Basque Government, Spain [KK-2023/00012, IT1456-22]; Department of Education of this institution
FX  - I. Triguero is funded by a Maria Zambrano Senior Fellowship at the University of Granada. I. Triguero, F. Herrera, D. Molina, and J. Poyatos are supported by the R&D and Innovation project with reference PID2020-119478GB-I00 granted by Spain's Ministry of Science and Innovation and European Regional Development Fund (ERDF) . J. Del Ser would like to thank the Basque Government, Spain for the funding support received through the EMAITEK and ELKARTEK programs (ref. KK-2023/00012) , as well as the Consolidated Research Group MATHMODE (IT1456-22) granted by the Department of Education of this institution.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 1566-2535
SN  - 1872-6305
J9  - INFORM FUSION
JI  - Inf. Fusion
DA  - MAR
PY  - 2024
VL  - 103
C7  - 102135
DO  - 10.1016/j.inffus.2023.102135
C6  - NOV 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001112351800001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  138
ER  -

TY  - JOUR
AU  - Mushtaq, AH
AU  - Shafqat, A
AU  - Salah, HT
AU  - Hashmi, SK
AU  - Muhsen, IN
TI  - Machine learning applications and challenges in graft-versus-host disease: a scoping review
T2  - CURRENT OPINION IN ONCOLOGY
LA  - English
KW  - allogeneic hematopoietic stem cell transplantation
KW  - artificial intelligence
KW  - graft versus host disease
KW  - machine learning
KW  - HEMATOPOIETIC-CELL TRANSPLANTATION
KW  - QUALITY-OF-LIFE
KW  - TELOMERE LENGTH
KW  - ARTIFICIAL-INTELLIGENCE
KW  - MARROW TRANSPLANTATION
KW  - SERUM-ALBUMIN
KW  - SURVIVAL
KW  - BLOOD
KW  - RISK
KW  - CLASSIFICATION
AB  - Purpose of review
   This review delves into the potential of artificial intelligence (AI), particularly machine learning (ML), in enhancing graft-versus-host disease (GVHD) risk assessment, diagnosis, and personalized treatment.
   Recent findings
   Recent studies have demonstrated the superiority of ML algorithms over traditional multivariate statistical models in donor selection for allogeneic hematopoietic stem cell transplantation. ML has recently enabled dynamic risk assessment by modeling time-series data, an upgrade from the static, "snapshot" assessment of patients that conventional statistical models and older ML algorithms offer. Regarding diagnosis, a deep learning model, a subset of ML, can accurately identify skin segments affected with chronic GVHD with satisfactory results. ML methods such as Q-learning and deep reinforcement learning have been utilized to develop adaptive treatment strategies (ATS) for the personalized prevention and treatment of acute and chronic GVHD.
   Summary
   To capitalize on these promising advancements, there is a need for large-scale, multicenter collaborations to develop generalizable ML models. Furthermore, addressing pertinent issues such as the implementation of stringent ethical guidelines is crucial before the widespread introduction of AI into GVHD care.
AD  - Cleveland Clin Fdn, Dept Internal Med, 9500 Euclid Ave, Cleveland, OH 44195 USAAD  - Alfaisal Univ, Coll Med, Riyadh, Saudi ArabiaAD  - Houston Methodist Hosp, Dept Pathol & Genom Med, Houston, TX 77030 USAAD  - Mayo Clin, Div Hematol, Dept Med, Rochester, MN USAAD  - Sheikh Shakbout Med City, Dept Med, Abu Dhabi, U Arab EmiratesAD  - Khalifa Univ, Med Affairs, Abu Dhabi, U Arab EmiratesAD  - Baylor Coll Med, Dept Med, Sect Hematol & Oncol, Houston, TX 77030 USAC3  - Cleveland Clinic FoundationC3  - Alfaisal UniversityC3  - The Methodist Hospital SystemC3  - The Methodist Hospital - HoustonC3  - Mayo ClinicC3  - Khalifa University of Science & TechnologyC3  - Baylor College of MedicinePU  - LIPPINCOTT WILLIAMS & WILKINS
PI  - PHILADELPHIA
PA  - TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA, PA 19103 USA
SN  - 1040-8746
SN  - 1531-703X
J9  - CURR OPIN ONCOL
JI  - Curr. Opin. Oncol.
DA  - NOV
PY  - 2023
VL  - 35
IS  - 6
SP  - 594
EP  - 600
DO  - 10.1097/CCO.0000000000000996
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001144598200017
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  98
ER  -

TY  - JOUR
AU  - Cheungpasitporn, W
AU  - Thongprayoon, C
AU  - Kashani, KB
TI  - Artificial Intelligence in Heart Failure and Acute Kidney Injury: Emerging Concepts and Controversial Dimensions
T2  - CARDIORENAL MEDICINE
LA  - English
KW  - Heart failure
KW  - Acute kidney injury
KW  - Artificial intelligence
KW  - Machine learning
KW  - Analytical approaches
KW  - Care enhancement
KW  - Transformative potential
KW  - Multifaceted technologies
KW  - RENAL-FUNCTION
KW  - EPIDEMIOLOGY
KW  - DYSFUNCTION
KW  - PREDICTION
KW  - OUTCOMES
KW  - IMPACT
AB  - Background: The growing complexity of patient data and the intricate relationship between heart failure (HF) and acute kidney injury (AKI) underscore the potential benefits of integrating artificial intelligence (AI) and machine learning into healthcare. These advanced analytical tools aim to improve the understanding of the pathophysiological relationship between kidney and heart, provide optimized, individualized, and timely care, and improve outcomes of HF with AKI patients. Summary: This comprehensive review article examines the transformative potential of AI and machine-learning solutions in addressing the challenges within this domain. The article explores a range of methodologies, including supervised and unsupervised learning, reinforcement learning, and AI-driven tools like chatbots and large language models. We highlight how these technologies can be tailored to tackle the complex issues prevalent among HF patients with AKI. The potential applications identified span predictive modeling, personalized interventions, real-time monitoring, and collaborative treatment planning. Additionally, we emphasize the necessity of thorough validation, the importance of collaborative efforts between cardiologists and nephrologists, and the consideration of ethical aspects. These factors are critical for the effective application of AI in this area. Key Messages: As the healthcare field evolves, the synergy of advanced analytical tools and clinical expertise holds significant promise to enhance the care and outcomes of individuals who deal with the combined challenges of HF and AKI.
AD  - Mayo Clin, Dept Med, Div Nephrol & Hypertens, Rochester, MN 55902 USAAD  - Mayo Clin, Dept Med, Div Pulm & Crit Care Med, Rochester, MN 55902 USAC3  - Mayo ClinicC3  - Mayo ClinicPU  - KARGER
PI  - BASEL
PA  - ALLSCHWILERSTRASSE 10, CH-4009 BASEL, SWITZERLAND
SN  - 1664-3828
SN  - 1664-5502
J9  - CARDIORENAL MED
JI  - CardioRenal Med.
PY  - 2024
VL  - 14
IS  - 1
SP  - 147
EP  - 159
DO  - 10.1159/000537751
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001205348900001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  43
ER  -

TY  - JOUR
AU  - Besinovic, N
AU  - De Donato, L
AU  - Flammini, F
AU  - Goverde, RMP
AU  - Lin, ZY
AU  - Liu, RH
AU  - Marrone, S
AU  - Nardone, R
AU  - Tang, TL
AU  - Vittorini, V
TI  - Artificial Intelligence in Railway Transport: Taxonomy, Regulations, and Applications
T2  - IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS
LA  - English
KW  - Rail transportation
KW  - Artificial intelligence
KW  - Taxonomy
KW  - Rails
KW  - Maintenance engineering
KW  - Software
KW  - Safety
KW  - Artificial intelligence
KW  - railway transport
KW  - machine learning
KW  - computer vision
KW  - traffic management
KW  - predictive maintenance
KW  - REINFORCEMENT LEARNING APPROACH
KW  - AIRLINE REVENUE MANAGEMENT
KW  - GAME-THEORETIC MODEL
KW  - NATURAL-LANGUAGE
KW  - DEFECT DETECTION
KW  - FRAMEWORK
KW  - SYSTEM
KW  - ALGORITHM
KW  - INTERNET
KW  - DESIGN
AB  - Artificial Intelligence (AI) is becoming pervasive in most engineering domains, and railway transport is no exception. However, due to the plethora of different new terms and meanings associated with them, there is a risk that railway practitioners, as several other categories, will get lost in those ambiguities and fuzzy boundaries, and hence fail to catch the real opportunities and potential of machine learning, artificial vision, and big data analytics, just to name a few of the most promising approaches connected to AI. The scope of this paper is to introduce the basic concepts and possible applications of AI to railway academics and practitioners. To that aim, this paper presents a structured taxonomy to guide researchers and practitioners to understand AI techniques, research fields, disciplines, and applications, both in general terms and in close connection with railway applications such as autonomous driving, maintenance, and traffic management. The important aspects of ethics and explainability of AI in railways are also introduced. The connection between AI concepts and railway subdomains has been supported by relevant research addressing existing and planned applications in order to provide some pointers to promising directions.
AD  - Delft Univ Technol, Dept Transport & Planning, NL-2600 GA Delft, NetherlandsAD  - Univ Naples Federico II, Dept Elect Engn & Informat Technolo2y, I-80125 Naples, ItalyAD  - Malardalen Univ, Sch Innovat Design & Engn, S-72220 Vasteras, SwedenAD  - Linnaeus Univ, Dept Comp Sci & Media Technol, S-35195 Vaxjo, SwedenAD  - Univ Leeds, Inst Transport Studies, Leeds LS2 9JT, W Yorkshire, EnglandAD  - Univ Naples Parthenope, Dept Engn, I-80143 Naples, ItalyAD  - Southeast Univ, Sch Transportat, Jiangsu Prov Collaborat Innovat Ctr Modern Urban, Jiangsu Key Lab Urban ITS, Nanjing 211189, Peoples R ChinaC3  - Delft University of TechnologyC3  - University of Naples Federico IIC3  - Malardalen UniversityC3  - Linnaeus UniversityC3  - University of LeedsC3  - Parthenope University NaplesC3  - Southeast University - ChinaFU  - European Union [881782 RAILS]; Assisted Very Short Term Planning (VSTP)/Dynamic Timetabling Project - U.K. Rail Safety and Standards Board (RSSB) via Bellvedi Ltd. [RSSB/494204565]
FX  - This work was supported in part by the Shift2Rail Joint Undertaking under the European Union's Horizon 2020 Research and Innovation Programme under Grant n.881782 RAILS. The work of Zhiyuan Lin and Ronghui Liu was supported in part by the Assisted Very Short Term Planning (VSTP)/Dynamic Timetabling Project funded by U.K. Rail Safety and Standards Board (RSSB) via Bellvedi Ltd., under Grant RSSB/494204565.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1524-9050
SN  - 1558-0016
J9  - IEEE T INTELL TRANSP
JI  - IEEE Trans. Intell. Transp. Syst.
DA  - SEP
PY  - 2022
VL  - 23
IS  - 9
SP  - 14011
EP  - 14024
DO  - 10.1109/TITS.2021.3131637
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000858988900008
N1  - Times Cited in Web of Science Core Collection:  18
Total Times Cited:  18
Cited Reference Count:  125
ER  -

TY  - JOUR
AU  - Yu, P
AU  - Xu, H
AU  - Hu, X
AU  - Deng, C
TI  - Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration
T2  - HEALTHCARE
LA  - English
KW  - generative artificial intelligence
KW  - generative AI
KW  - large language models
KW  - LLM
KW  - ethics
KW  - healthcare
KW  - medicine
AB  - Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.
AD  - Univ Wollongong, Sch Comp & Informat Technol, Wollongong, NSW 2522, AustraliaAD  - Yale Sch Med, Sect Biomed Informat & Data Sci, 100 Coll St,Fl 9, New Haven, CT 06510 USAAD  - Rice Univ, Dept Comp Sci, POB 1892, Houston, TX 77251 USAAD  - Univ Wollongong, Sch Med Indigenous & Hlth Sci, Wollongong, NSW 2522, AustraliaC3  - University of WollongongC3  - Yale UniversityC3  - Rice UniversityC3  - University of WollongongPU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2227-9032
J9  - HEALTHCARE-BASEL
JI  - Healthcare
DA  - OCT
PY  - 2023
VL  - 11
IS  - 20
C7  - 2776
DO  - 10.3390/healthcare11202776
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)AN  - WOS:001089835500001
N1  - Times Cited in Web of Science Core Collection:  9
Total Times Cited:  9
Cited Reference Count:  60
ER  -

TY  - JOUR
AU  - Flamini, F
AU  - Hamann, A
AU  - Jerbi, S
AU  - Trenkwalder, LM
AU  - Nautrup, HP
AU  - Briegel, HJ
TI  - Photonic architecture for reinforcement learning
T2  - NEW JOURNAL OF PHYSICS
LA  - English
KW  - machine learning
KW  - reinforcement learning
KW  - quantum photonics
KW  - integrated photonic circuits
KW  - PHASE-CHANGE MATERIALS
KW  - NEURAL-NETWORKS
KW  - GO
AB  - The last decade has seen an unprecedented growth in artificial intelligence and photonic technologies, both of which drive the limits of modern-day computing devices. In line with these recent developments, this work brings together the state of the art of both fields within the framework of reinforcement learning. We present the blueprint for a photonic implementation of an active learning machine incorporating contemporary algorithms such as SARSA, Q-learning, and projective simulation. We numerically investigate its performance within typical reinforcement learning environments, showing that realistic levels of experimental noise can be tolerated or even be beneficial for the learning process. Remarkably, the architecture itself enables mechanisms of abstraction and generalization, two features which are often considered key ingredients for artificial intelligence. The proposed architecture, based on single-photon evolution on a mesh of tunable beamsplitters, is simple, scalable, and a first integration in quantum optical experiments appears to be within the reach of near-term technology.
AD  - Univ Innsbruck, Inst Theoret Phys, Tech Str 25, A-6020 Innsbruck, AustriaC3  - University of InnsbruckFU  - European Union [801110]; Austrian Federal Ministry of Education, Science and Research (BMBWF); Austrian Science Fund (FWF) [DK-ALM:W1259-N27, SFB BeyondC F71]; Ministerium fur Wissenschaft, Forschung, und Kunst Baden-Wurttemberg [AZ:33-7533.-30-10/41/1]
FX  - This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 801110 and the Austrian Federal Ministry of Education, Science and Research (BMBWF). It reflects only the author's view and the Agency is not responsible for any use that may be made of the information it contains. HPN, LMT, SJ, and HJB acknowledge support from the Austrian Science Fund (FWF) through the projects DK-ALM:W1259-N27 and SFB BeyondC F71. HJB was also supported by the Ministerium fur Wissenschaft, Forschung, und Kunst Baden-Wurttemberg (AZ:33-7533.-30-10/41/1).
PU  - IOP PUBLISHING LTD
PI  - BRISTOL
PA  - TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND
SN  - 1367-2630
J9  - NEW J PHYS
JI  - New J. Phys.
DA  - APR
PY  - 2020
VL  - 22
IS  - 4
C7  - 045002
DO  - 10.1088/1367-2630/ab783c
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000523462900001
N1  - Times Cited in Web of Science Core Collection:  18
Total Times Cited:  18
Cited Reference Count:  58
ER  -

TY  - JOUR
AU  - Ferigo, A
AU  - Custode, LL
AU  - Iacca, G
TI  - Quality-diversity optimization of decision trees for interpretable reinforcement learning
T2  - NEURAL COMPUTING & APPLICATIONS
LA  - English
KW  - Quality-diversity optimization
KW  - Explainability
KW  - Decision trees
KW  - Reinforcement learning
KW  - BLACK-BOX
AB  - In the current Artificial Intelligence (AI) landscape, addressing explainability and interpretability in Machine Learning (ML) is of critical importance. In fact, the vast majority of works on AI focus on Deep Neural Networks (DNNs), which are not interpretable, as they are extremely hard to inspect and understand for humans. This is a crucial disadvantage of these methods, which hinders their trustability in high-stakes scenarios. On the other hand, interpretable models are considerably easier to inspect, which allows humans to test them exhaustively, and thus trust them. While the fields of eXplainable Artificial Intelligence (XAI) and Interpretable Artificial Intelligence (IAI) are progressing in supervised settings, the field of Interpretable Reinforcement Learning (IRL) is falling behind. Several approaches leveraging Decision Trees (DTs) for IRL have been proposed in recent years. However, all of them use goal-directed optimization methods, which may have limited exploration capabilities. In this work, we extend a previous study on the applicability of Quality-Diversity (QD) algorithms to the optimization of DTs for IRL. We test the methods on two well-known Reinforcement Learning (RL) benchmark tasks from OpenAI Gym, comparing their results in terms of score and "illumination" patterns. We show that using QD algorithms is an effective way to explore the search space of IRL models. Moreover, we find that, in the context of DTs for IRL, QD approaches based on MAP-Elites (ME) and its variant Covariance Matrix Adaptation MAP-Elites (CMA-ME) can significantly improve convergence speed over the goal-directed approaches.
AD  - Univ Trento, Dept Informat Engn & Comp Sci, Via Sommar 9, I-38123 Povo, Trento, ItalyC3  - University of TrentoFU  - Funded by the European Union (project no. 101071179). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or EISMEA. Neither the European Union nor the granting authority can be he [101071179]; European Union; Horizon Europe - Pillar III [101071179] Funding Source: Horizon Europe - Pillar III
FX  - Funded by the European Union (project no. 101071179). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or EISMEA. Neither the European Union nor the granting authority can be held responsible for them.
PU  - SPRINGER LONDON LTD
PI  - LONDON
PA  - 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN  - 0941-0643
SN  - 1433-3058
J9  - NEURAL COMPUT APPL
JI  - Neural Comput. Appl.
DA  - 2023 NOV 14
PY  - 2023
DO  - 10.1007/s00521-023-09124-5
C6  - NOV 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001100507200001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  36
ER  -

TY  - JOUR
AU  - Singh, H
AU  - Nim, DK
AU  - Randhawa, AS
AU  - Ahluwalia, S
TI  - Integrating clinical pharmacology and artificial intelligence: potential benefits, challenges, and role of clinical pharmacologists
T2  - EXPERT REVIEW OF CLINICAL PHARMACOLOGY
LA  - English
KW  - Clinical trial
KW  - machine learning
KW  - natural language processing
KW  - pharmacovigilance
KW  - precision medicine
KW  - HEALTH-CARE
KW  - MACHINE
KW  - PREDICTION
KW  - CANCER
KW  - SYNERGY
KW  - FUTURE
KW  - MODEL
AB  - IntroductionThe integration of artificial intelligence (AI) into clinical pharmacology could be a potential approach for accelerating drug discovery and development, improving patient care, and streamlining medical research processes.Areas coveredWe reviewed the current state of AI applications in clinical pharmacology, focusing on drug discovery and development, precision medicine, pharmacovigilance, and other ventures. Key AI applications in clinical pharmacology are examined, including machine learning, natural language processing, deep learning, and reinforcement learning etc. Additionally, the evolving role of clinical pharmacologists, ethical considerations, and challenges in implementing AI in clinical pharmacology are discussed.Expert opinionThe AI could be instrumental in accelerating drug discovery, predicting drug safety and efficacy, and optimizing clinical trial designs. It can play a vital role in precision medicine by helping in personalized drug dosing, treatment selection, and predicting drug response based on genetic, clinical, and environmental factors. The role of AI in pharmacovigilance, such as signal detection and adverse event prediction, is also promising. The collaboration between clinical pharmacologists and AI experts also poses certain ethical and practical challenges. Clinical pharmacologists can be instrumental in shaping the future of AI-driven clinical pharmacology and contribute to the improvement of healthcare systems.
AD  - Govt Med Coll & Hosp, Dept Pharmacol, Room E503,Level 5,E Block, Chandigarh, IndiaAD  - Moti Lal Nehru Med Coll, Dept Pharmacol, Allahabad, IndiaAD  - Govt Med Coll & Hosp, Chandigarh, IndiaC3  - Moti Lal Nehru Medical CollegePU  - TAYLOR & FRANCIS LTD
PI  - ABINGDON
PA  - 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN  - 1751-2433
SN  - 1751-2441
J9  - EXPERT REV CLIN PHAR
JI  - Expert Rev. Clin. Pharmacol.
DA  - 2024 FEB 17
PY  - 2024
DO  - 10.1080/17512433.2024.2317963
C6  - FEB 2024
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001162973800001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  105
ER  -

TY  - CPAPER
AU  - Iyer, R
AU  - Li, YZ
AU  - Li, H
AU  - Lewis, M
AU  - Sundar, R
AU  - Sycara, K
A1  - ACM
TI  - Transparency and Explanation in Deep Reinforcement Learning Neural Networks
T2  - PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18)
LA  - English
CP  - AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - system transparency
KW  - explainable AI
KW  - deep reinforcement learning
KW  - human-AI interaction
KW  - human factors
KW  - AUTOMATION
AB  - Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of "object saliency maps", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.
AD  - Carnegie Mellon Univ, Pittsburgh, PA 15213 USAAD  - Google Inc, Mountain View, CA USAAD  - Univ Pittsburgh, Pittsburgh, PA USAC3  - Carnegie Mellon UniversityC3  - Google IncorporatedC3  - Pennsylvania Commonwealth System of Higher Education (PCSHE)C3  - University of PittsburghFU  -  [FA9550-15-1-0442]
FX  - This research was supported by award FA9550-15-1-0442.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-6012-8
PY  - 2018
SP  - 144
EP  - 150
DO  - 10.1145/3278721.3278776
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:000510018100025
N1  - Times Cited in Web of Science Core Collection:  48
Total Times Cited:  55
Cited Reference Count:  30
ER  -

TY  - JOUR
AU  - Rampini, L
AU  - Cecconi, FR
TI  - ARTIFICIAL INTELLIGENCE IN CONSTRUCTION ASSET MANAGEMENT: A REVIEW OF PRESENT STATUS, CHALLENGES AND FUTURE OPPORTUNITIES
T2  - JOURNAL OF INFORMATION TECHNOLOGY IN CONSTRUCTION
LA  - English
KW  - Asset Management
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Neural Network
KW  - Computer Vision
KW  - MODEL-PREDICTIVE CONTROL
KW  - NEURAL-NETWORK MODEL
KW  - ENERGY-CONSUMPTION
KW  - THERMAL SENSATIONS
KW  - EXISTING BUILDINGS
KW  - DAMAGE DETECTION
KW  - CRACK DETECTION
KW  - BIG DATA
KW  - PROJECT
KW  - INFORMATION
AB  - The built environment is responsible for roughly 40% of global greenhouse emissions, making the sector a crucial factor for climate change and sustainability. Meanwhile, other sectors (like manufacturing) adopted Artificial Intelligence (AI) to solve complex, non-linear problems to reduce waste, inefficiency, and pollution. Therefore, many research efforts in the Architecture, Engineering, and Construction community have recently tried introducing AI into building asset management (AM) processes. Since AM encompasses a broad set of disciplines, an overview of several AI applications, current research gaps, and trends is needed. In this context, this study conducted the first state-of-the-art research on AI for building asset management. A total of 578 papers were analyzed with bibliometric tools to identify prominent institutions, topics, and journals. The quantitative analysis helped determine the most researched areas of AM and which AI techniques are applied. The areas were furtherly investigated by reading in-depth the 83 most relevant studies selected by screening the articles' abstracts identified in the bibliometric analysis. The results reveal many applications for Energy Management, Condition assessment, Risk management, and Project management areas. Finally, the literature review identified three main trends that can be a reference point for future studies made by practitioners or researchers: Digital Twin, Generative Adversarial Networks (with synthetic images) for data augmentation, and Deep Reinforcement Learning.
AD  - Politecn Milan, Milan, ItalyC3  - Polytechnic University of MilanPU  - INT COUNCIL RESEARCH & INNOVATION BUILDING & CONSTRUCTION
PI  - ROTTERDAM
PA  - KRUISPLEIN 25 G, PO BOX 1837, ROTTERDAM, 3000 BV, NETHERLANDS
SN  - 1874-4753
J9  - J INF TECHNOL CONSTR
JI  - J. Inf. Technol. Constr.
PY  - 2022
VL  - 27
DO  - 10.36680/j.itcon.2022.043
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000868340500001
N1  - Times Cited in Web of Science Core Collection:  5
Total Times Cited:  5
Cited Reference Count:  176
ER  -

TY  - CPAPER
AU  - Gilbert, TK
AU  - Lambert, N
AU  - Dean, S
AU  - Zick, T
AU  - Snoswell, A
AU  - Mehta, S
A1  - ACM
TI  - Reward Reports for Reinforcement Learning
T2  - PROCEEDINGS OF THE 2023 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, AIES 2023
LA  - English
CP  - AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES)
KW  - Reward function
KW  - reporting
KW  - documentation
KW  - disaggregated evaluation
KW  - ethical considerations
KW  - MODEL
KW  - GO
AB  - Building systems that are good for society in the face of complex societal effects requires a dynamic approach. Recent approaches to machine learning (ML) documentation have demonstrated the promise of discursive frameworks for deliberation about these complexities. However, these developments have been grounded in a static ML paradigm, leaving the role of feedback and post-deployment performance unexamined. Meanwhile, recent work in reinforcement learning has shown that the effects of feedback and optimization objectives on system behavior can be wide-ranging and unpredictable. In this paper we sketch a framework for documenting deployed and iteratively updated learning systems, which we call Reward Reports. Taking inspiration from technical concepts in reinforcement learning, we outline Reward Reports as living documents that track updates to design choices and assumptions behind what a particular automated system is optimizing for. They are intended to track dynamic phenomena arising from system deployment, rather than merely static properties of models or data. After presenting the elements of a Reward Report, we discuss a concrete example: Meta's BlenderBot 3 chatbot. Several others for game-playing (DeepMind's MuZero), content recommendation (MovieLens), and traffic control (Project Flow) are included in the appendix.
AD  - Cornell Tech, Digital Life Initiat, New York, NY 10044 USAAD  - HuggingFace, Berkeley, CA USAAD  - Cornell Univ, Ithaca, NY USAAD  - Harvard Law Sch, Boston, MA USAAD  - Queensland Univ Technol, Ctr Automated Decis Making & Soc, Brisbane, Qld, AustraliaAD  - Columbia Univ, New York, NY USAC3  - Cornell UniversityC3  - Harvard UniversityC3  - Queensland University of Technology (QUT)C3  - Columbia UniversityFU  - Center for Long-Term Cybersecurity; Mozilla Foundation
FX  - The authors wish to thank the Center for Human Compatible Artificial Intelligence, the Center for Long-Term Cybersecurity, and the Mozilla Foundation for supporting this research.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0231-0
PY  - 2023
SP  - 84
EP  - 130
DO  - 10.1145/3600211.3604698
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:001117838100011
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  87
ER  -

TY  - CPAPER
AU  - Lover, J
AU  - Gjærum, VB
AU  - Lekkas, AM
TI  - Explainable AI methods on a deep reinforcement learning agent for automatic docking
T2  - IFAC PAPERSONLINE
LA  - English
CP  - 13th IFAC Conference on Control Applications in Marine Systems, Robotics, and Vehicles (CAMS)
KW  - Marine control systems
KW  - Explainable Artificial Intelligence
KW  - Deep Reinforcement Learning
KW  - Autonomous ships
KW  - Docking
AB  - Artifical neural networks (ANNs) have made their way into marine robotics in the last years, where they are used in control and perception systems, to name a few examples. At the same time, the black-box nature of ANNs is responsible for key challenges related to interpretability and trustworthiness, which need to be addressed if ANNs are to be deployed safely in real-life operations. In this paper, we implement three XAI methods to provide explanations to the decisions made by a deep reinforcement learning agent: Kernel SHAP, LIME and Linear Model Trees (LMTs). The agent was trained via Proximal Policy Optimization (PPO) to perform automatic docking on a fully-actuated vessel. We discuss the properties and suitability of the three methods, and juxtapose them with important attributes of the docking agent to provide context to the explanations. Copyright (C) 2021 The Authors.
AD  - Norwegian Univ Sci & Technol NTNU, Dept Engn Cybernet, NO-7491 Trondheim, NorwayC3  - Norwegian University of Science & Technology (NTNU)FU  - Research Council of Norway through the EXAIGON project [304843]
FX  - This work was supported by the Research Council of Norway through the EXAIGON project, project number 304843
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 2405-8963
J9  - IFAC PAPERSONLINE
JI  - IFAC PAPERSONLINE
PY  - 2021
VL  - 54
IS  - 16
SP  - 146
EP  - 152
DO  - 10.1016/j.ifacol.2021.10.086
C6  - NOV 2021
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000714877600023
N1  - Times Cited in Web of Science Core Collection:  8
Total Times Cited:  8
Cited Reference Count:  26
ER  -

TY  - JOUR
AU  - Sharif, M
AU  - Uckelmann, D
TI  - Multi-Modal LA in Personalized Education Using Deep Reinforcement Learning Based Approach
T2  - IEEE ACCESS
LA  - English
KW  - Education
KW  - Deep reinforcement learning
KW  - Learning (artificial intelligence)
KW  - Ethics
KW  - Predictive models
KW  - Europe
KW  - Usability
KW  - Artificial intelligence (AI)
KW  - machine learning (ML)
KW  - learning analytics (LA)
KW  - multi-modal learning analytics (MMLA)
KW  - deep learning (DL)
KW  - deep reinforcement learning (DRL)
KW  - personalise learning (PL)
AB  - The demand for personalized learning experiences and effective analytics in education has significantly increased. The integration of technology in education has brought about significant changes in teaching and learning practices. In the era of digital technology, the integration of education technology in the classroom has led to a change in teaching methods and learning strategies. In this paper, we introduce the KNIGHT (AI in Education at Hochschule fur Technik (HFT) Stuttgart) framework, which is a holistic solution designed to tackle the complex issue of personalized education in a digital era. The paper explores the application of multimodal data integration, the novel application of deep reinforcement learning to education analytics, and the ethical consideration of privacy-preserving personalized feedback. The proposed framework's efficacy is substantiated through a case study, demonstrating its potential to revolutionize personalized education. This paper provides a comprehensive overview of the current discourse, providing valuable insights for educators, policymakers, and researchers into the multifaceted landscape of modern education, contributing to ongoing discussions and advancements in educational technology.
AD  - Univ Appl Sci, Dept Informat, D-70174 Stuttgart, GermanyFU  - German Federal Ministry of Education and Research
FX  - No Statement Available
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
J9  - IEEE ACCESS
JI  - IEEE Access
PY  - 2024
VL  - 12
SP  - 54049
EP  - 54065
DO  - 10.1109/ACCESS.2024.3388474
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001205753400001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  48
ER  -

TY  - CPAPER
AU  - Alberta, M
AU  - Lee, D
AU  - Khan, O
A1  - IEEE Comp Soc
TI  - Can Robots Be Responsible: A Bumper Theory Approach to Robot Moral Conditioning
T2  - 2023 SEVENTH IEEE INTERNATIONAL CONFERENCE ON ROBOTIC COMPUTING, IRC 2023
LA  - English
CP  - 7th IEEE International Conference on Robotic Computing (IRC)
KW  - robot cognition
KW  - human-robot interaction
KW  - moral conditioning
KW  - self-driving or autonomous vehicle
KW  - machine learning
KW  - artificial intelligence
KW  - deep learning
KW  - reinforcement learning
KW  - YOLO
KW  - CNN
AB  - This paper examines the main unresolved theoretical controversy question, centered on the gap issue when responsibility meets ambiguity. Our scientific inquiry aims to discuss what is missing in this issue and promote a dialogue that hopes to begin to resolve it. We are proposing a moral framework and planning a series of experiments that, when applied, may start the process of resolving the responsibility gap to better understand moral cognition. The controversy surrounding morality in autonomous robots or vehicles, synonymous with the responsibility gap problem, is being investigated by bumper theory related to the severity of casualty, indicating an immoral level, with a moral rulebook. The experiment proposes moral conditioning to drive the robot's behavior to avoid collision and update the level of morality of the robot. Our scientific method should also promote a dialogue that contends robots can be prescribed a moral rulebook that could give the robot human-like moral cognition. As an experiment to validate robots enabled to morally be conditioned, robots are developed to detect humans, cars, or non-humans using machine learning. A fast single-stage YOLO is used for human detection and multitask convolutional neural network MTCNN is used for detecting human-faces.
AD  - Univ Pacific UOP, Dept Commun, Stockton, CA 95211 USAAD  - Univ Pacific, Dept Comp Engn, Stockton, CA 95211 USAAD  - UOP, Dept Comp Sci, SOECS, Stockton, CA USAC3  - University of the PacificPU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 979-8-3503-9574-7
PY  - 2023
SP  - 284
EP  - 287
DO  - 10.1109/IRC59093.2023.00053
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001195993100047
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  14
ER  -

TY  - CPAPER
AU  - Kasirzadeh, A
AU  - Evans, C
A1  - ACM
TI  - User Tampering in Reinforcement Learning Recommender Systems
T2  - PROCEEDINGS OF THE 2023 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, AIES 2023
LA  - English
CP  - AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES)
KW  - AI Safety
KW  - AI Ethics
KW  - Recommendation Systems
KW  - Recommender Systems
KW  - Reinforcement Learning
KW  - Value Alignment
AB  - In this paper, we introduce new formal methods and provide empirical evidence to highlight a unique safety concern prevalent in reinforcement learning (RL)-based recommendation algorithms 'user tampering.' User tampering is a situation where an RL-based recommender system may manipulate a media user's opinions through its suggestions as part of a policy to maximize long-term user engagement. We use formal techniques from causal modeling to critically analyze prevailing solutions proposed in the literature for implementing scalable RL-based recommendation systems, and we observe that these methods do not adequately prevent user tampering. Moreover, we evaluate existing mitigation strategies for reward tampering issues, and show that these methods are insufficient in addressing the distinct phenomenon of user tampering within the context of recommendations. We further reinforce our findings with a simulation study of an RL-based recommendation system focused on the dissemination of political content. Our study shows that a Q-learning algorithm consistently learns to exploit its opportunities to polarize simulated users with its early recommendations in order to have more consistent success with subsequent recommendations that align with this induced polarization. Our findings emphasize the necessity for developing safer RL-based recommendation systems and suggest that achieving such safety would require a fundamental shift in the design away from the approaches we have seen in the recent literature.
AD  - Univ Edinburgh, Alan Turing Inst, Edinburgh, Midlothian, ScotlandAD  - Australian Natl Univ, Canberra, ACT, AustraliaC3  - University of EdinburghC3  - Australian National UniversityPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0231-0
PY  - 2023
SP  - 58
EP  - 69
DO  - 10.1145/3600211.3604669
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:001117838100009
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  38
ER  -

TY  - CPAPER
AU  - Peysakhovich, A
A1  - Assoc Comp Machinery
TI  - Reinforcement Learning and Inverse Reinforcement Learning with System 1 and System 2
T2  - AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY
LA  - English
CP  - 2nd AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - reinforcement learning
KW  - behavioral economics
KW  - dual-system models
KW  - inverse reinforcement learning
AB  - Inferring a person's goal from their behavior is an important problem in applications of AI (e.g. automated assistants, recommender systems). The workhorse model for this task is the rational actor model - this amounts to assuming that people have stable reward functions, discount the future exponentially, and construct optimal plans. Under the rational actor assumption techniques such as inverse reinforcement learning (IRL) can be used to infer a person's goals from their actions. A competing model is the dual-system model. Here decisions are the result of an interplay between a fast, automatic, heuristic-based system 1 and a slower, deliberate, calculating system 2. We generalize the dual system framework to the case of Markov decision problems and show how to compute optimal plans for dual-system agents. We show that dual-system agents exhibit behaviors that are incompatible with rational actor assumption. We show that naive applications of rational-actor IRL to the behavior of dual-system agents can generate wrong inference about the agents' goals and suggest interventions that actually reduce the agent's overall utility. Finally, we adapt a simple IRL algorithm to correctly infer the goals of dual system decision-makers. This allows us to make interventions that help, rather than hinder, the dual-system agent's ability to reach their true goals.
AD  - Facebook AI Res, Menlo Pk, CA 94025 USAC3  - Facebook IncPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
SN  - 978-1-4503-6324-2
PY  - 2019
SP  - 409
EP  - 415
DO  - 10.1145/3306618.3314259
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000556121100057
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  43
ER  -

TY  - CPAPER
AU  - Schlechtinger, M
AU  - Kosack, D
AU  - Paulheim, H
AU  - Fetzer, T
ED  - Dignum, F
ED  - Corchado, JM
ED  - DeLaPrieta, F
TI  - Winning at Any Cost - Infringing the Cartel Prohibition with Reinforcement Learning
T2  - ADVANCES IN PRACTICAL APPLICATIONS OF AGENTS, MULTI-AGENT SYSTEMS, AND SOCIAL GOOD: THE PAAMS COLLECTION, PAAMS 2021
LA  - English
CP  - 19th International Conference on Advances in Practical Applications of Agents, Multi-Agent Systems, and Social Good (PAAMS)
KW  - Multi agent reinforcement learning
KW  - Pricing agents
KW  - Algorithmic collusion
AB  - Pricing decisions are increasingly made by AI. Thanks to their ability to train with live market data while making decisions on the fly, deep reinforcement learning algorithms are especially effective in taking such pricing decisions. In e-commerce scenarios, multiple reinforcement learning agents can set prices based on their competitor's prices. Therefore, research states that agents might end up in a state of collusion in the long run. To further analyze this issue, we build a scenario that is based on a modified version of a prisoner's dilemma where three agents play the game of rock paper scissors. Our results indicate that the action selection can be dissected into specific stages, establishing the possibility to develop collusion prevention systems that are able to recognize situations which might lead to a collusion between competitors. We furthermore provide evidence for a situation where agents are capable of performing a tacit cooperation strategy without being explicitly trained to do so.
AD  - Univ Mannheim, Chair Data Sci, D-68159 Mannheim, GermanyAD  - Univ Mannheim, Chair Publ Law Regulatory Law & Tax Law, D-68159 Mannheim, GermanyC3  - University of MannheimC3  - University of MannheimFU  - Baden-Wurttemberg Stiftung in the Responsible Artificial Intelligence program
FX  - The work presented in this paper has been conducted in the KarekoKI project, which is funded by the Baden-Wurttemberg Stiftung in the Responsible Artificial Intelligence program.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-85739-4
SN  - 978-3-030-85738-7
J9  - LECT NOTES ARTIF INT
PY  - 2021
VL  - 12946
SP  - 255
EP  - 266
DO  - 10.1007/978-3-030-85739-4_21
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000791045800021
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  25
ER  -

TY  - CPAPER
AU  - Vouros, GA
A1  - ACM
TI  - Tutorial on Explainable Deep Reinforcement Learning: One framework, three paradigms and many challenges
T2  - PROCEEDINGS OF THE 12TH HELLENIC CONFERENCE ON ARTIFICIAL INTELLIGENCE, SETN 2022
LA  - English
CP  - 12th Hellenic Conference on Artificial Intelligence (SETN)
KW  - Deep Learning
KW  - Deep Reinforcement Learning
KW  - Interpretability
KW  - Explainability
KW  - Transparency
AB  - Interpretability, explainability and transparency are key issues to introducing Artificial Intelligence closed-box methods in many critical domains: This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability and fairness, and has important consequences towards keeping the human in the loop in high levels of automation, especially in critical cases for decision making. Reinforcement learning methods, and especially their deep versions, are closed-box methods that support agents to act autonomously in the real world. This tutorial will provide a formal specification of the deep reinforcement learning explainability problems, and will present the necessary components of a general explainable reinforcement learning framework. Based on this framework will provide distinct explainability paradigms towards solving explainability problems, with examples from state-of-the-art methods and real-world cases. The tutorial will conclude identifying open questions and important challenges. The tutorial is based on the survey paper on "Explainable Deep Reinforcement Learning" State of the Art and Challenges" [1].
AD  - Univ Piraeus, Dept Digital Syst, Piraeus, GreeceC3  - University of PiraeusFU  - University of Piraeus Research Center (UPRC)
FX  - This tutorial is supported by the University of Piraeus Research Center (UPRC).
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-9597-7
PY  - 2022
DO  - 10.1145/3549737.3549808
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001117699700067
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  1
ER  -

TY  - JOUR
AU  - Okada, Y
AU  - Mertens, M
AU  - Liu, N
AU  - Lam, SSW
AU  - Ong, MEH
TI  - AI and machine learning in resuscitation: Ongoing research, new concepts, and key challenges
T2  - RESUSCITATION PLUS
LA  - English
KW  - Prediction model
KW  - Natural language processing
KW  - Heterogeneity
KW  - Self-fulfilling prophecy
KW  - Feedback loop
KW  - Large language model
KW  - Emergency medicine
KW  - HOSPITAL CARDIAC-ARREST
KW  - EMERGENCY-DEPARTMENT
KW  - CRITICAL-CARE
KW  - PREDICTION
KW  - VALIDATION
KW  - OUTCOMES
KW  - SUBPHENOTYPES
KW  - EVENTS
KW  - IMPACT
KW  - ASIA
AB  - Aim: Artificial intelligence (AI) and machine learning (ML) are important areas of computer science that have recently attracted attention for their application to medicine. However, as techniques continue to advance and become more complex, it is increasingly challenging for clinicians to stay abreast of the latest research. This overview aims to translate research concepts and potential concerns to healthcare professionals interested in applying AI and ML to resuscitation research but who are not experts in the field.Main text: We present various research including prediction models using structured and unstructured data, exploring treatment heterogeneity, reinforcement learning, language processing, and large-scale language models. These studies potentially offer valuable insights for optimizing treatment strategies and clinical workflows. However, implementing AI and ML in clinical settings presents its own set of challenges. The availability of high quality and reliable data is crucial for developing accurate ML models. A rigorous validation process and the integration of ML into clinical practice is essential for practical implementation. We furthermore highlight the potential risks associated with self-fulfilling prophecies and feedback loops, emphasizing the importance of transparency, interpretability, and trustworthiness in AI and ML models. These issues need to be addressed in order to establish reliable and trustworthy AI and ML models.Conclusion: In this article, we overview concepts and examples of AI and ML research in the resuscitation field. Moving forward, appropriate understanding of ML and collaboration with relevant experts will be essential for researchers and clinicians to overcome the challenges and harness the full potential of AI and ML in resuscitation.
AD  - Natl Univ Singapore, Duke NUS Med Sch, Singapore, SingaporeAD  - Kyoto Univ, Grad Sch Med, Prevent Serv, Kyoto, JapanAD  - Antwerp Univ, Antwerp Ctr Responsible AI, Antwerp, BelgiumAD  - Antwerp Univ, Dept Philosophy, Ctr Ethics, Antwerp, BelgiumAD  - Singapore Gen Hosp, Dept Emergency Med, Singapore, SingaporeAD  - Natl Univ Singapore, Duke NUS Med Sch, Hlth Serv & Syst Res, Singapore, SingaporeC3  - National University of SingaporeC3  - Kyoto UniversityC3  - University of AntwerpC3  - University of AntwerpC3  - Singapore General HospitalC3  - National University of SingaporeFU  - JSPS KAKENHI of Japan [JP22K21143]; Zoll foundation; Japan Society for the Promotion of Science; FUKUDA Foundation for medical technology; International medical research foundation; European Union [101107292]
FX  - This study was supported by a scientific research grant from the JSPS KAKENHI of Japan (JP22K21143) and the Zoll foundation. YO has received an overseas scholarship from the Japan Society for the Promotion of Science, the FUKUDA Foundation for medical technology, and the International medical research foundation. MM is funded by the European Union, through the HORIZON-MSCA2022-PF-01-01 Marie Curie Postdoctoral Fellowship, Project 101107292 'PredicGenX'.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 2666-5204
J9  - RESUSC PLUS
JI  - Resusc. Plus
DA  - SEP
PY  - 2023
VL  - 15
C7  - 100435
DO  - 10.1016/j.resplu.2023.100435
C6  - JUL 2023
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:001051946400001
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  94
ER  -

TY  - CPAPER
AU  - Roselló-Marín, E
AU  - Lopez-Sanchez, M
AU  - Rodríguez, I
AU  - Rodríguez-Soto, M
AU  - Rodríguez-Aguilar, JA
ED  - Cortes, A
ED  - Grimaldo, F
ED  - Flaminio, T
TI  - An Ethical Conversational Agent to Respectfully Conduct In-Game Surveys
T2  - ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT
LA  - English
CP  - 24th International Conference of the Catalan-Association-for-Artificial-Intelligence (CCIA)
KW  - Machine ethics
KW  - Reinforcement Learning
KW  - Conversational Agents
KW  - User Experience Questionnaires
KW  - Video Games
AB  - The improvement of videogames highly relies on feedback, usually gathered through UX questionnaires performed after playing. However, users may not remember all the details. This paper proposes an ethical conversational agent, endowed with the moral value of respect, that interacts with the user to perform a survey during the game session. To do so, we use reinforcement learning and the ethical embedding algorithm to ensure that the agent learns to be respectful (i.e., avoid gameplay interruptions) while pursuing its individual objective of asking questions. The novelty is twofold: firstly, the application of ethical embedding outside toy problems; and secondly, the enrichment of a survey oriented conversational agent with this moral value of respect. Results showcase how our ethical conversational bot manages to avoid disturbing user's engagement while getting even a higher percentage of valid answers than a non-ethically enriched chatbot.
AD  - Univ Barcelona UB, Dept Matemat & Informat, Barcelona, SpainAD  - CSIC, IIIA, Artificial Intelligence Res Inst, Barcelona, SpainC3  - University of BarcelonaC3  - Consejo Superior de Investigaciones Cientificas (CSIC)C3  - CSIC - Instituto de Investigacion en Inteligencia Artificial (IIIA)FU  - Spanish Government [FPU18/03387]; CI-SUSTAIN [PID2019-104156GB-I00]; Crowd4SDG [H2020-872944]; COREDEM [H2020-785907]; Barcelona City Council through the Fundacio Solidaritat UB [21S01802-001]
FX  - Funded by CI-SUSTAIN (Grant PID2019-104156GB-I00), Crowd4SDG (H2020-872944), COREDEM (H2020-785907), Barcelona City Council through the Fundacio Solidaritat UB (code 21S01802-001). Manel Rodriguez-Soto was funded by the Spanish Government with an FPU grant (ref. FPU18/03387).
PU  - IOS PRESS
PI  - AMSTERDAM
PA  - NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS
SN  - 0922-6389
SN  - 1879-8314
SN  - 978-1-64368-326-3
SN  - 978-1-64368-327-0
J9  - FRONT ARTIF INTEL AP
PY  - 2022
VL  - 356
SP  - 335
EP  - 344
DO  - 10.3233/FAIA220356
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001176468400047
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  28
ER  -

TY  - JOUR
AU  - Heuillet, A
AU  - Couthouis, F
AU  - Díaz-Rodríguez, N
TI  - Collective eXplainable AI: Explaining Cooperative Strategies and Agent Contribution in Multiagent Reinforcement Learning With Shapley Values
T2  - IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE
LA  - English
KW  - PREDICTIONS
AB  - While Explainable Artificial Intelligence (XAI) is increasingly expanding more areas of application, little has been applied to make deep Reinforcement Learning (RL) more comprehensible. As RL becomes ubiquitous and used in critical and general public applications, it is essential to develop methods that make it better understood and more interpretable. This study proposes a novel approach to explain cooperative strategies in multiagent RL using Shapley values, a game theory concept used in XAI that successfully explains the rationale behind decisions taken by Machine Learning algorithms. Through testing common assumptions of this technique in two cooperation-centered socially challenging multi-agent environments environments, this article argues that Shapley values are a pertinent way to evaluate the contribution of players in a cooperative multi-agent RL context. To palliate the high overhead of this method, Shapley values are approximated using Monte Carlo sampling. Experimental results on Multiagent Particle and Sequential Social Dilemmas show that Shapley values succeed at estimating the contribution of each agent. These results could have implications that go beyond games in economics, (e.g., for non-discriminatory decision making, ethical and responsible AI-derived decisions or policy making under fairness constraints). They also expose how Shapley values only give general explanations about a model and cannot explain a single run, episode nor justify precise actions taken by agents. Future work should focus on addressing these critical aspects.
AD  - Univ Paris Saclay, Gif Sur Yvette, FranceAD  - Ubisoft, Montreuil, FranceAD  - Univ Granada, Granada, SpainC3  - Universite Paris CiteC3  - Universite Paris SaclayC3  - Ubisoft EntertainmentC3  - University of GranadaFU  - Spanish Government Juan de la Cierva Incorporacion [IJC2019-039152-I]
FX  - We thank Frederic Herbreteau, Adrien Bennetot and Leo Heidelberger for their help and support. N. Diaz-Rodriguez is currently supported by the Spanish Government Juan de la Cierva Incorporacion contract (IJC2019-039152-I).
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1556-603X
SN  - 1556-6048
J9  - IEEE COMPUT INTELL M
JI  - IEEE Comput. Intell. Mag.
DA  - FEB
PY  - 2022
VL  - 17
IS  - 1
SP  - 59
EP  - 71
DO  - 10.1109/MCI.2021.3129959
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000742178400017
N1  - Times Cited in Web of Science Core Collection:  16
Total Times Cited:  17
Cited Reference Count:  45
ER  -

TY  - CPAPER
AU  - Chaput, R
AU  - Matignon, L
AU  - Guillermin, M
A1  - IEEE
TI  - Learning to identify and settle dilemmas through contextual user preferences
T2  - 2023 IEEE 35TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, ICTAI
LA  - English
CP  - 35th IEEE International Conference on Tools with Artificial Intelligence (ICTAI)
KW  - Machine Ethics
KW  - Multi-Objective Reinforcement Learning
KW  - Moral Dilemmas
KW  - Human Preferences
AB  - Artificial Intelligence systems have a significant impact on human lives. Machine Ethics tries to align these systems with human values, by integrating "ethical considerations". However, most approaches consider a single objective, and thus cannot accommodate different, contextual human preferences. Multi-Objective Reinforcement Learning algorithms account for various preferences, but they often are not intelligible nor contextual (e.g., weighted preferences). Our novel approach identifies dilemmas, presents them to users, and learns to settle them, based on intelligible and contextualized preferences over actions. We intend to maximize understandability and opportunities for user-system co-construction by showing dilemmas, and triggering interactions, thus empowering users. The block-based architecture enables leveraging simple mechanisms that can be updated and improved. Validation on a Smart Grid use-case shows that our algorithm finds actions for various trade-offs, and quickly learns to settle dilemmas, reducing the cognitive load on users.
AD  - Univ Lyon, UCBL, CNRS, INSA Lyon,LIRIS,UMR5205, Villeurbanne, FranceAD  - Lyon Catholic Univ, CONFLUENCE Sci & Humanites Res Unit EA 1598, Lyon, FranceC3  - Centre National de la Recherche Scientifique (CNRS)C3  - Universite Claude Bernard Lyon 1C3  - Institut National des Sciences Appliquees de Lyon - INSA LyonFU  - French Region Auvergne-Rhone-Alpes; ANR [ANR-22-CE23-0028-01]
FX  - This work was funded by the French Region Auvergne-Rhone-Alpes (Pack Ambition Recherche; Ethics.AI project); and by ANR (ANR-22-CE23-0028-01; ACCELER-AI project).
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1082-3409
SN  - 979-8-3503-4273-4
J9  - PROC INT C TOOLS ART
PY  - 2023
SP  - 474
EP  - 479
DO  - 10.1109/ICTAI59109.2023.00075
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001139095400067
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  23
ER  -

TY  - JOUR
AU  - Qian, ZY
AU  - Guo, P
AU  - Wang, YF
AU  - Xiao, FC
TI  - Ethical and moral decision-making for self-driving cars based on deep reinforcement learning
T2  - JOURNAL OF INTELLIGENT & FUZZY SYSTEMS
LA  - English
KW  - Rawlsian maximin principle
KW  - carla simulator
KW  - depth-wise separable convolution
KW  - deep Q-network
KW  - ethical decision-making
KW  - PREDICTION
AB  - Self-driving cars are expected to replace human drivers shortly, bringing significant benefits to society. However, they have faced opposition from various organizations that argue it is challenging to respond to instances involving unavoidable personal injury. In situations involving deadly collisions, self-driving cars must make decisions that balance life and death. This paper investigates the ethical and moral decision-making challenges for self-driving cars from an algorithmic perspective. To address this issue, we introduce the accident-prioritized replay mechanism to the Deep Q-Networks (DQN) algorithm based on early humanities research. The mechanism quantifies a reward function that takes priority into account. RGB (red, green, blue) images obtained by the camera installed in front of the self-driving cars are fed into the Xception network for training. To evaluate our approach, we compare it to the conventional DQN algorithm. The simulation results indicate that the Rawlsian DQN algorithm has superior stability and interpretability in decision-making. Furthermore, the majority of respondents to our survey accept the final decision made by our algorithm. Our experiment demonstrates that it is possible to incorporate ethical considerations into self-driving car decision-making, providing a solution for rational decision-making in emergency and dilemma circumstances.
AD  - Southwest Jiaotong Univ, Sch Mech Engn, Chengdu, Peoples R ChinaAD  - Technol & Equipment Rail Transit Operat & Mainten, Chengdu, Peoples R ChinaAD  - Southwest Jiaotong Univ, Sch Comp & Artificial Intelligence, Chengdu, Peoples R ChinaC3  - Southwest Jiaotong UniversityC3  - Southwest Jiaotong UniversityFU  - Sichuan Science and Technology Program [2022NSFSC0459]; Fundamental Research Funds for the Central Universities [202210613020]
FX  - The research of this paper is supported by the Sichuan Science and Technology Program (No. 2022NSFSC0459) and the Fundamental Research Funds for the Central Universities (No. 202210613020).
PU  - IOS PRESS
PI  - AMSTERDAM
PA  - NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS
SN  - 1064-1246
SN  - 1875-8967
J9  - J INTELL FUZZY SYST
JI  - J. Intell. Fuzzy Syst.
PY  - 2023
VL  - 45
IS  - 4
SP  - 5523
EP  - 5540
DO  - 10.3233/JIFS-224553
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001086860200023
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  36
ER  -

TY  - JOUR
AU  - Noothigattu, R
AU  - Bouneffouf, D
AU  - Mattel, N
AU  - Chandra, R
AU  - Madan, P
AU  - Varshney, KR
AU  - Campbell, M
AU  - Singh, M
AU  - Rossi, F
TI  - Teaching AI agents ethical values using reinforcement learning and policy orchestration
T2  - IBM JOURNAL OF RESEARCH AND DEVELOPMENT
LA  - English
AB  - Autonomous cyber-physical agents play an increasingly large role in our lives. To ensure that they behave in ways aligned with the values of society, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspecified constraints from demonstrations and reinforcement learning to learn to maximize environmental rewards. A contextual-bandit-based orchestrator then picks between the two policies: constraint-based and environment reward-based. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward-maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using Pac-Man and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways.
AD  - Carnegie Mellon Univ, Pittsburgh, PA 15213 USAAD  - IBM Res, Yorktown Hts, NY 10598 USAAD  - Tulane Univ, New Orleans, LA 70118 USAAD  - IBM Res, Cambridge, MA 02142 USAC3  - Carnegie Mellon UniversityC3  - International Business Machines (IBM)C3  - Tulane UniversityC3  - International Business Machines (IBM)FU  - IBM Science for Social Good initiative
FX  - This work was conducted under the auspices of the IBM Science for Social Good initiative. An abbreviated version of this article appeared at the 2019 International Joint Conference on Artificial Intelligence (IJCAI 2019) [34].
PU  - IBM CORP
PI  - ARMONK
PA  - 1 NEW ORCHARD ROAD, ARMONK, NY 10504 USA
SN  - 0018-8646
SN  - 2151-8556
J9  - IBM J RES DEV
JI  - IBM J. Res. Dev.
DA  - JUL-SEP
PY  - 2019
VL  - 63
IS  - 4-5
C7  - 2
DO  - 10.1147/JRD.2019.2940428
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000498912200003
N1  - Times Cited in Web of Science Core Collection:  18
Total Times Cited:  21
Cited Reference Count:  34
ER  -

TY  - CPAPER
AU  - Noothigattu, R
AU  - Bouneffouf, D
AU  - Mattei, N
AU  - Chandra, R
AU  - Madan, P
AU  - Varshney, KR
AU  - Campbell, M
AU  - Singh, M
AU  - Rossi, F
ED  - Kraus, S
TI  - Teaching AI Agents Ethical Values Using Reinforcement Learning and Policy Orchestration (Extended Abstract)
T2  - PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE
LA  - English
CP  - 28th International Joint Conference on Artificial Intelligence
AB  - Autonomous cyber-physical agents play an increasingly large role in our lives. To ensure that they behave in ways aligned with the values of society, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspecified constraints from demonstrations and reinforcement learning to learn to maximize environmental rewards. A contextual bandit-based orchestrator then picks between the two policies: constraint-based and environment reward-based. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward-maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using Pac-Man and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways.
AD  - IBM Res, Yorktown Hts, NY USAAD  - IBM Res, Cambridge, MA USAAD  - Carnegie Mellon Univ, Pittsburgh, PA 15213 USAAD  - Tulane Univ, New Orleans, LA 70118 USAC3  - International Business Machines (IBM)C3  - International Business Machines (IBM)C3  - Carnegie Mellon UniversityC3  - Tulane UniversityFU  - IBM Science for Social Good initiative
FX  - This work was conducted under the auspices of the IBM Science for Social Good initiative.
PU  - IJCAI-INT JOINT CONF ARTIF INTELL
PI  - FREIBURG
PA  - ALBERT-LUDWIGS UNIV FREIBURG GEORGES-KOHLER-ALLEE, INST INFORMATIK, GEB 052, FREIBURG, D-79110, GERMANY
SN  - 978-0-9992411-4-1
PY  - 2019
SP  - 6377
EP  - 6381
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000761735106080
N1  - Times Cited in Web of Science Core Collection:  11
Total Times Cited:  11
Cited Reference Count:  20
ER  -

TY  - CPAPER
AU  - Rolf, M
AU  - Crook, N
AU  - Steil, JJ
A1  - IEEE
TI  - From social interaction to ethical AI: a developmental roadmap
T2  - 2018 JOINT IEEE 8TH INTERNATIONAL CONFERENCE ON DEVELOPMENT AND LEARNING AND EPIGENETIC ROBOTICS (ICDL-EPIROB)
LA  - English
CP  - 8th Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)
KW  - ROBOT
KW  - RISK
KW  - ACCEPTABILITY
KW  - BEHAVIOR
KW  - DESIGN
AB  - AI and robot ethics have recently gained a lot of attention because adaptive machines are increasingly involved in ethically sensitive scenarios and cause incidents of public outcry. Much of the debate has been focused on achieving highest moral standards in handling ethical dilemmas on which not even humans can agree, which indicates that the wrong questions are being asked. We suggest to address this ethics debate strictly through the lens of what behavior seems socially acceptable, rather than idealistically ethical. Learning such behavior puts the debate into the very heart of developmental robotics. This paper poses a roadmap of computational and experimental questions to address the development of socially acceptable machines. We emphasize the need for social reward mechanisms and learning architectures that integrate these while reaching beyond limitations of plain reinforcement-learning agents. We suggest to use the metaphor of "needs" to bridge rewards and higher level abstractions such as goals for both communication and action generation in a social context. We then suggest a series of experimental questions and possible platforms and paradigms to guide future research in the area.
AD  - Oxford Brookes Univ, Sch Engn Comp & Maths, Oxford, EnglandAD  - Tech Univ Carolo Wilhelmina Braunschweig, Inst Robot & Proc Control, Braunschweig, GermanyC3  - Oxford Brookes UniversityC3  - Braunschweig University of TechnologyPU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2161-9484
SN  - 978-1-5386-6110-9
J9  - J IEEE I C DEVELOP L
PY  - 2018
SP  - 204
EP  - 211
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000492050700030
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  4
Cited Reference Count:  63
ER  -

TY  - CPAPER
AU  - Gerdes, A
ED  - Caron, B
ED  - Schmitt, KA
ED  - Pearl, Z
ED  - Dara, R
ED  - Love, HA
TI  - AI can turn the clock back before we know it
T2  - 2021 IEEE INTERNATIONAL SYMPOSIUM ON TECHNOLOGY AND SOCIETY (ISTAS21): TECHNOLOGICAL STEWARDSHIP & RESPONSIBLE INNOVATION
LA  - English
CP  - IEEE International Symposium on Technology and Society (ISTAS) - Technological Stewardship and Responsible Innovation
KW  - artificial intelligence
KW  - AI hype
KW  - reinforcement learning
KW  - embodied cognition
KW  - adversarial attacks
KW  - research integrity
KW  - ARTIFICIAL-INTELLIGENCE
AB  - This paper outlines intertwined challenges related to three areas: the hype surrounding AI, the consequences of corporate influence on the AI research agenda, and the public sector's uncritical embracement of AI technologies. We argue that AI can backfire if overconfident predictions influence decisions to introduce AI in high-risk domains. Moreover, the corporate colonization of the AI research agenda may cause a decline in societal trust in science, which is highly problematic considering that AI will increasingly power important domains in society.
AD  - Univ Southern Denmark, Dept Design & Commun, Kolding, DenmarkC3  - University of Southern DenmarkPU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2158-3404
SN  - 978-1-6654-3580-2
J9  - INT SYMP TECHNOL SOC
PY  - 2021
DO  - 10.1109/ISTAS52410.2021.9629161
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000782422800038
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  30
ER  -

TY  - JOUR
AU  - Dasgupta, P
AU  - Kliem, J
TI  - Improved Reinforcement Learning in Asymmetric Real-time Strategy Games via Strategy Diversity
T2  - INTERNATIONAL JOURNAL OF SERIOUS GAMES
LA  - English
KW  - Real-time strategy game
KW  - asymmetric game
KW  - deep reinforcement learning
KW  - strategy diversity
KW  - LEVEL
KW  - GO
AB  - We investigate the use of artificial intelligence (AI)-based techniques in learning to play a 2-player, real-time strategy (RTS) game called Hunting-of-the-Plark. The game is challenging to play for both humans and AI-based techniques because players cannot observe each other's moves while playing the game and one player is at a disadvantage due to the asymmetric nature of the game rules. We analyze the performance of different deep reinforcement learning algorithms to train software agents that can play the game. Existing reinforcement learning techniques for RTS games enable players to converge towards an equilibrium outcome of the game but usually do not facilitate further explo-ration of techniques to exploit and defeat the opponent. To address this shortcoming, we investigate techniques including self-play and strategy diversity that can be used by players to improve their performance beyond the equilibrium outcome. We observe that when players use self-play, their number of wins begins to cycle around an equilibrium value as each player quickly learns to outwit and defeat its opponent and vice-versa. Fi-nally, we show that strategy diversity could be used as an effective means to alleviate the performance of the disadvantaged player caused by the asymmetric nature of the game.
AD  - US Naval Res Lab, Informat Technol Div, Washington, DC 20375 USAC3  - United States Department of DefenseC3  - United States NavyC3  - Naval Research LaboratoryFU  - Naval Research Laboratory 6.1 Base Funding Program
FX  - This research was done as part of the Game Theoretic Machine Learning for Defense Applications project that was supported by a grant from Naval Research Laboratory 6.1 Base Funding Program. Dasgupta was responsible for conceptualizing and directing the research, and, for writing the paper. Kliem was responsible for implementing the code, running experiments and recording experimental results.
PU  - SERIOUS GAMES SOC
PI  - GENOA
PA  - IST INT COMUNICAZIONI, VILLA PIAGGIO, GENOA, 16125, ITALY
SN  - 2384-8766
J9  - INT J SERIOUS GAMES
JI  - Int. J. Serious Games
DA  - MAR
PY  - 2023
VL  - 10
IS  - 1
SP  - 19
EP  - 38
DO  - 10.17083/ijsg.v10i1.548
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000964482100004
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  34
ER  -

TY  - JOUR
AU  - Costa, T
AU  - Laan, A
AU  - Heras, FJH
AU  - de Polavieja, GG
TI  - Automated Discovery of Local Rules for Desired Collective-Level Behavior Through Reinforcement Learning
T2  - FRONTIERS IN PHYSICS
LA  - English
KW  - collective behavior
KW  - multi agent reinforcement learning
KW  - deep learning
KW  - interpretable artificial intelligence
KW  - explainable artificial intelligence
KW  - MOTION
KW  - EVOLUTION
KW  - NETWORKS
KW  - MODELS
AB  - Complex global behavior patterns can emerge from very simple local interactions between many agents. However, no local interaction rules have been identified that generate some patterns observed in nature, for example the rotating balls, rotating tornadoes and the full-core rotating mills observed in fish collectives. Here we show that locally interacting agents modeled with a minimal cognitive system can produce these collective patterns. We obtained this result by using recent advances in reinforcement learning to systematically solve the inverse modeling problem: given an observed collective behavior, we automatically find a policy generating it. Our agents are modeled as processing the information from neighbor agents to choose actions with a neural network and move in an environment of simulated physics. Even though every agent is equipped with its own neural network, all agents have the same network architecture and parameter values, ensuring in this way that a single policy is responsible for the emergence of a given pattern. We find the final policies by tuning the neural network weights until the produced collective behavior approaches the desired one. By using modular neural networks with modules using a small number of inputs and outputs, we built an interpretable model of collective motion. This enabled us to analyse the policies obtained. We found a similar general structure for the four different collective patterns, not dissimilar to the one we have previously inferred from experimental zebrafish trajectories; but we also found consistent differences between policies generating the different collective pattern, for example repulsion in the vertical direction for the more three-dimensional structures of the sphere and tornado. Our results illustrate how new advances in artificial intelligence, and specifically in reinforcement learning, allow new approaches to analysis and modeling of collective behavior.
AD  - Champalimaud Res, Collect Behav Lab, Lisbon, PortugalFU  - Fundacao para a Ciencia e a Tecnologia [PTDC/NEU-SCC/0948/2014]; NVIDIA; Champalimaud Foundation; Congento [LISBOA-01-0145-FEDER-022170]; Fundação para a Ciência e a Tecnologia [PTDC/NEU-SCC/0948/2014] Funding Source: FCT
FX  - This work was supported by Fundacao para a Ciencia e a Tecnologia (www.fct.pt) PTDC/NEU-SCC/0948/2014 (to GP and contract to FH), Congento (congento.org) LISBOA-01-0145-FEDER-022170, NVIDIA (nvidia.com) (FH and GP), and Champalimaud Foundation (fchampalimaud.org) (GP). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.
PU  - FRONTIERS MEDIA SA
PI  - LAUSANNE
PA  - AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN  - 2296-424X
J9  - FRONT PHYS-LAUSANNE
JI  - Front. Physics
DA  - JUN 25
PY  - 2020
VL  - 8
C7  - 200
DO  - 10.3389/fphy.2020.00200
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000551955900001
N1  - Times Cited in Web of Science Core Collection:  9
Total Times Cited:  10
Cited Reference Count:  61
ER  -

TY  - CPAPER
AU  - Calloquispe-Huallpa, R
AU  - Darbali-Zamora, R
AU  - Aponte-Bezares, EE
AU  - Huaman-Rivera, A
A1  - IEEE
TI  - Centralized Secondary Control Through Reinforcement Learning for Isolated Microgrids
T2  - 2023 IEEE PES INNOVATIVE SMART GRID TECHNOLOGIES LATIN AMERICA, ISGT-LA
LA  - English
CP  - IEEE PES Conference on Innovative Smart Grid Technologies Latin America (ISGT-LA)
KW  - microgrid
KW  - secondary control
KW  - reinforcement learning
KW  - machine learning
AB  - Microgrids (MGs) continue to be a subject of extensive research that can enhance their operation and controls. This study addresses frequency control in MGs operating in islanded mode, where distributed energy resource integration and system stability are crucial. Conventionally, secondary control in MGs has relied on proportional-integral controllers. This paper proposes a novel approach for centralized secondary control by employing a deep deterministic policy gradient agent based on reinforcement learning (RL). Through extensive simulations conducted in Simulink, the performance of the RL-based secondary control system was evaluated. The results obtained demonstrate the effectiveness of the RL-based approach in maintaining the frequency within acceptable limits, even in the presence of load variations. The proposed RL-based secondary control showcases the potential of advanced machine learning techniques to revolutionize frequency regulation in MGs. By enabling more precise and adaptive control strategies, this approach improves efficiency, stability, and overall performance of MGs.
AD  - Univ Puerto Rico Mayaguez, Mayaguez, PR 00682 USAAD  - Sandia Natl Labs, Albuquerque, NM 87185 USAC3  - University of Puerto RicoC3  - University of Puerto Rico MayaguezC3  - United States Department of Energy (DOE)C3  - Sandia National LaboratoriesFU  - U.S. DOE [DE-NA0003525]
FX  - This article has been authored by an employee of National Technology & Engineering Solutions of Sandia, LLC under Contract No. DE-NA0003525 with the U.S. DOE. The employee owns all right, title and interest in and to the article and is solely responsible for its contents. The U.S. Government retains and the publisher, by accepting the article for publication, acknowledges that the U.S. Government retains a non-exclusive, paid up, irrevocable, worldwide license to publish or reproduce the published form of this article or allow others to do so, for U.S. Government purposes. The DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2165-4816
SN  - 979-8-3503-3696-2
J9  - IEEE PES INNOV SMART
PY  - 2023
SP  - 285
EP  - 289
DO  - 10.1109/ISGT-LA56058.2023.10328315
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001117994600056
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  20
ER  -

TY  - JOUR
AU  - Wong, RSY
AU  - Ming, LC
AU  - Ali, RAR
TI  - The Intersection of ChatGPT, Clinical Medicine, and Medical Education
T2  - JMIR MEDICAL EDUCATION
LA  - English
KW  - ChatGPT
KW  - clinical research
KW  - large language model
KW  - artificial intelligence
KW  - ethical considerations
KW  - AI
KW  - OpenAI
AB  - As we progress deeper into the digital age, the robust development and application of advanced artificial intelligence (AI) technology, specifically generative language models like ChatGPT (OpenAI), have potential implications in all sectors including medicine. This viewpoint article aims to present the authors' perspective on the integration of AI models such as ChatGPT in clinical medicine and medical education. The unprecedented capacity of ChatGPT to generate human-like responses, refined through Reinforcement Learning with Human Feedback, could significantly reshape the pedagogical methodologies within medical education. Through a comprehensive review and the authors' personal experiences, this viewpoint article elucidates the pros, cons, and ethical considerations of using ChatGPT within clinical medicine and notably, its implications for medical education. This exploration is crucial in a transformative era where AI could potentially augment human capability in the process of knowledge creation and dissemination, potentially revolutionizing medical education and clinical practice. The importance of maintaining academic integrity and professional standards is highlighted. The relevance of establishing clear guidelines for the responsible and ethical use of AI technologies in clinical medicine and medical education is also emphasized.
AD  - Sunway Univ, Sch Med & Life Sci, Dept Med Educ, Selangor, MalaysiaAD  - SEGi Univ, Fac Med Nursing & Hlth Sci, Petaling Jaya, MalaysiaAD  - Sunway Univ, Sch Med & Life Sci, Selangor, MalaysiaAD  - Univ Kebangsaan Malaysia, Fac Med, GUT Res Grp, Kuala Lumpur, MalaysiaAD  - Sunway Univ, Sch Med & Life Sci, 5 Jalan Univ Bandar Sunway, Selangor 47500, MalaysiaC3  - Sunway UniversityC3  - SEGi UniversityC3  - Sunway UniversityC3  - Universiti Kebangsaan MalaysiaC3  - Sunway UniversityPU  - JMIR PUBLICATIONS, INC
PI  - TORONTO
PA  - 130 QUEENS QUAY East, Unit 1100, TORONTO, ON M5A 0P6, CANADA
SN  - 2369-3762
J9  - JMIR MED EDUC
JI  - JMIR Med. Educ.
PY  - 2023
VL  - 9
C7  - e47274
DO  - 10.2196/47274
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:001114957900001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  43
ER  -

TY  - JOUR
AU  - Whittlestone, J
AU  - Arulkumaran, K
AU  - Crosby, M
TI  - The Societal Implications of Deep Reinforcement Learning
T2  - JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH
LA  - English
KW  - COMPREHENSIVE SURVEY
KW  - NEURAL-NETWORKS
KW  - SIMULATION
KW  - LEVEL
KW  - GAME
KW  - GO
AB  - Deep Reinforcement Learning (DRL) is an avenue of research in Artificial Intelligence (AI) that has received increasing attention within the research community in recent years, and is beginning to show potential for real-world application. DRL is one of the most promising routes towards developing more autonomous AI systems that interact with and take actions in complex real-world environments, and can more flexibly solve a range of problems for which we may not be able to precisely specify a correct 'answer'. This could have substantial implications for people's lives: for example by speeding up automation in various sectors, changing the nature and potential harms of online influence, or introducing new safety risks in physical infrastructure. In this paper, we review recent progress in DRL, discuss how this may introduce novel and pressing issues for society, ethics, and governance, and highlight important avenues for future research to better understand DRL's societal implications.
AD  - Univ Cambridge, Leverhulme Ctr Future Intelligence, Cambridge, EnglandAD  - Imperial Coll London, London, EnglandAD  - Imperial Coll London, Leverhulme Ctr Future Intelligence, London, EnglandC3  - University of CambridgeC3  - Imperial College LondonC3  - Imperial College LondonFU  - Leverhulme Trust via the Leverhulme Centre for the Future of Intelligence; Long-Term Future Fund
FX  - Correspondence to jlw84@cam.ac.uk.All authors contributed equally.
FX  - The authors would like to thank Shahar Avin, Ghazi Ahamat, Miles Brundage, and members of the Kinds of Intelligence Reading Group and AI:FAR Group at the Leverhulme Centre for the Future of Intelligence for helpful discussions and comments. We also would like to thank several anonymous reviewers for their comments, which substantially improved the paper. This work was partly funded by a grant from the Leverhulme Trust via the Leverhulme Centre for the Future of Intelligence, and by a grant from the Long-Term Future Fund.
PU  - AI ACCESS FOUNDATION
PI  - MARINA DEL REY
PA  - USC INFORMATION SCIENCES INST, 4676 ADMIRALITY WAY, MARINA DEL REY, CA 90292-6695 USA
SN  - 1076-9757
SN  - 1943-5037
J9  - J ARTIF INTELL RES
JI  - J. Artif. Intell. Res.
PY  - 2021
VL  - 70
SP  - 1003
EP  - 1030
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000743970700002
N1  - Times Cited in Web of Science Core Collection:  15
Total Times Cited:  15
Cited Reference Count:  121
ER  -

TY  - JOUR
AU  - Isufaj, R
AU  - Omeri, M
AU  - Piera, MA
TI  - Multi-UAV Conflict Resolution with Graph Convolutional Reinforcement Learning
T2  - APPLIED SCIENCES-BASEL
LA  - English
KW  - UTM
KW  - UAS
KW  - machine learning
KW  - artificial intelligence
KW  - multi-UAS cooperative control
KW  - multiagent reinforcement learning
AB  - Safety is the primary concern when it comes to air traffic. In-flight safety between Unmanned Aircraft Vehicles (UAVs) is ensured through pairwise separation minima, utilizing conflict detection and resolution methods. Existing methods mainly deal with pairwise conflicts, however, due to an expected increase in traffic density, encounters with more than two UAVs are likely to happen. In this paper, we model multi-UAV conflict resolution as a multiagent reinforcement learning problem. We implement an algorithm based on graph neural networks where cooperative agents can communicate to jointly generate resolution maneuvers. The model is evaluated in scenarios with 3 and 4 present agents. Results show that agents are able to successfully solve the multi-UAV conflicts through a cooperative strategy.
AD  - Autonomous Univ Barcelona, Dept Telecommun & Syst Engn, Logist & Aeronaut Grp, Sabadell 08202, SpainC3  - Autonomous University of BarcelonaFU  - European Union [783287]; Spanish National Project EU-TM [TRA2017-88724-R]; H2020 Societal Challenges Programme [783287] Funding Source: H2020 Societal Challenges Programme
FX  - This research has received funding in part from the SESAR Joint Undertaking under the European Union's Horizon 2020 Research and Innovation Programme under grant agreement No 783287 and in part from Spanish National Project EU-TM No TRA2017-88724-R. The opinions expressed herein reflect the authors' views only. Under no circumstances shall the SESAR Joint Undertaking be responsible for any use that may be made of the information contained herein.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2076-3417
J9  - APPL SCI-BASEL
JI  - Appl. Sci.-Basel
DA  - JAN
PY  - 2022
VL  - 12
IS  - 2
C7  - 610
DO  - 10.3390/app12020610
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000748011900001
N1  - Times Cited in Web of Science Core Collection:  7
Total Times Cited:  8
Cited Reference Count:  47
ER  -

TY  - CPAPER
AU  - Wu, YH
AU  - Lin, SD
A1  - AAAI
TI  - A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents
T2  - THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
LA  - English
CP  - 32nd AAAI Conference on Artificial Intelligence / 30th Innovative Applications of Artificial Intelligence Conference / 8th AAAI Symposium on Educational Advances in Artificial Intelligence
AB  - This paper proposes a low-cost, easily realizable strategy to equip a reinforcement learning (RL) agent the capability of behaving ethically. Our model allows the designers of RL agents to solely focus on the task to achieve, without having to worry about the implementation of multiple trivial ethical patterns to follow. Based on the assumption that the majority of human behavior, regardless which goals they are achieving, is ethical, our design integrates human policy with the RL policy to achieve the target objective with less chance of violating the ethical code that human beings normally obey.
AD  - Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, TaiwanC3  - National Taiwan UniversityPU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-800-8
J9  - AAAI CONF ARTIF INTE
PY  - 2018
SP  - 1687
EP  - 1694
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000485488901093
N1  - Times Cited in Web of Science Core Collection:  36
Total Times Cited:  39
Cited Reference Count:  38
ER  -

TY  - JOUR
AU  - Seeamber, R
AU  - Badea, C
TI  - If Our Aim Is to Build Morality Into an Artificial Agent, How Might We Begin to Go About Doing So?
T2  - IEEE INTELLIGENT SYSTEMS
LA  - English
KW  - Ethics
KW  - Artificial intelligence
KW  - Decision making
KW  - Buildings
KW  - Intelligent systems
KW  - Task analysis
KW  - Reinforcement learning
AB  - As AI becomes pervasive in most fields, from health care to autonomous driving, it is essential that we find successful ways of building morality into our machines, especially for decision making. However, the question of what it means to be moral is still debated, particularly in the context of AI. In this article, we highlight the different aspects that should be considered when building moral agents, including the most relevant moral paradigms and challenges. We also discuss the top-down and bottom-up approaches to design and the role of emotion and sentience in morality. We then propose solutions, including a hybrid approach to design and a hierarchical approach to combining moral paradigms. We emphasize how governance and policy are becoming ever more critical in AI ethics and in ensuring that the tasks we set for moral agents are attainable, that ethical behavior is achieved, and that we obtain good AI.
AD  - Imperial Coll London, London SW7 2BX, EnglandAD  - Imperial Coll London, Dept Comp & philosophy, London SW7 2BX, EnglandC3  - Imperial College LondonC3  - Imperial College LondonFU  - U.K. Research and Innovation Centre for Doctoral Training in AI for Healthcare [EP/S023283/1]
FX  - Reneira Seeamber was supported by the U.K. Research and Innovation Centre for Doctoral Training in AI for Healthcare (http://ai4health.io) (Grant EP/S023283/1).
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 1541-1672
SN  - 1941-1294
J9  - IEEE INTELL SYST
JI  - IEEE Intell. Syst.
DA  - NOV
PY  - 2023
VL  - 38
IS  - 6
SP  - 35
EP  - 41
DO  - 10.1109/MIS.2023.3320875
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001186764900002
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  20
ER  -

TY  - CPAPER
AU  - Apellániz, D
AU  - Pettersson, B
AU  - Gengnagel, C
ED  - Gengnagel, C
ED  - Baverel, O
ED  - Betti, G
ED  - Popescu, M
ED  - Thomsen, MR
ED  - Wurm, J
TI  - A Flexible Reinforcement Learning Framework to Implement Cradle-to-Cradle in Early Design Stages
T2  - TOWARDS RADICAL REGENERATION
LA  - English
CP  - 8th Design Modelling Symposium on Towards Radical Regeneration
KW  - Life-cycle assessment
KW  - Cradle-to-Cradle
KW  - Machine Learning
AB  - Reinforcement Learning (RL) is a paradigm in Machine Learning (ML), along with Supervised Learning and Unsupervised Learning, that aims to create Artificial Intelligence (AI) agents that can take decisions in complex and uncertain environments, with the goal of maximizing their long-term benefit. Although it has not gained as much research interest in the AEC industry in recent years as other ML and optimization techniques, RL has been responsible for recent major scientific breakthroughs, such as Deep Mind's AlphaGo and AlphaFold algorithms. However, due the singularity of the problems and challenges of the AEC industry in contrast to the reduced number of benchmark environments and games in which new RL algorithms are commonly tested, little progress has been noticed so far towards the implementation of RL in this sector.
   This paper presents the development of the new Grasshopper plugin "Pug" to implement RL in Grasshopper in order to serve as a flexible framework to efficiently tackle diverse optimization problems in architecture with special focus on cradle-to-cradle problems based on material circularity. The components of the plugin are introduced, the workflows and principles to train AI agents in Grasshopper are explained and components related to material circularity are presented too. This new plugin is used to solve two RL problems related to the circularity and reuse of steel, timber and bamboo elements. The results are discussed and compared to traditional computational approaches such as genetic algorithms and heuristic rules.
AD  - Berlin Univ Arts UdK, Dept Struct Design & Technol KET, Hardenbergstr 33, D-10623 Berlin, GermanyAD  - Royal Danish Acad, Architecture Design Conservat, Philip de Langes 10, DK-1435 Copenhagen K, DenmarkPU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 978-3-031-13249-0
SN  - 978-3-031-13248-3
PY  - 2023
SP  - 3
EP  - 12
DO  - 10.1007/978-3-031-13249-0_1
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:000870223800001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  13
ER  -

TY  - CPAPER
AU  - Kasenberg, D
AU  - Arnold, T
AU  - Scheutz, M
A1  - ACM
TI  - Norms, Rewards, and the Intentional Stance Comparing Machine Learning Approaches to Ethical Training
T2  - PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18)
LA  - English
CP  - AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - norm inference
KW  - intentional stance
KW  - value alignment
AB  - The challenge of training AI systems to perform responsibly and beneficially has inspired different approaches for teaching a system what people want and how it is acceptable to attain that in the world. In this paper we compare work in reinforcement learning, in particular inverse reinforcement learning, with our norm inference approach. We test those two systems and present results. Using the idea of the "intentional stance", we explain how a norm inference approach can work even when another agent is acting strictly according to reward functions. In this way norm inference presents itself as a promising, more explicitly accountable approach with which to design AI systems from the start.
AD  - Tufts Univ, Medford, MA 02155 USAC3  - Tufts UniversityFU  - ONR MURI grant [N00014-16-1-2278]; NSF IIS grant [1723963]
FX  - This project was supported in part by ONR MURI grant N00014-16-1-2278 and by NSF IIS grant 1723963.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
SN  - 978-1-4503-6012-8
PY  - 2018
SP  - 184
EP  - 190
DO  - 10.1145/3278721.3278774
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:000510018100031
N1  - Times Cited in Web of Science Core Collection:  7
Total Times Cited:  7
Cited Reference Count:  19
ER  -

TY  - JOUR
AU  - Hernandez-Suarez, A
AU  - Sanchez-Perez, G
AU  - Toscano-Medina, LK
AU  - Perez-Meana, H
AU  - Olivares-Mercado, J
AU  - Portillo-Portillo, J
AU  - Benitez-Garcia, G
AU  - Orozco, ALS
AU  - Villalba, LJG
TI  - ReinforSec: An Automatic Generator of Synthetic Malware Samples and Denial-of-Service Attacks through Reinforcement Learning
T2  - SENSORS
LA  - English
KW  - malware
KW  - denial-of-service
KW  - reinforcement learning
KW  - synthetic sampling
KW  - cybersecurity
KW  - machine learning
KW  - cybersecurity datasets
KW  - artificial intelligence
KW  - q-learning
KW  - DDOS
AB  - In recent years, cybersecurity has been strengthened through the adoption of processes, mechanisms and rapid sources of indicators of compromise in critical areas. Among the most latent challenges are the detection, classification and eradication of malware and Denial of Service Cyber-Attacks (DoS). The literature has presented different ways to obtain and evaluate malware- and DoS-cyber-attack-related instances, either from a technical point of view or by offering ready-to-use datasets. However, acquiring fresh, up-to-date samples requires an arduous process of exploration, sandbox configuration and mass storage, which may ultimately result in an unbalanced or under-represented set. Synthetic sample generation has shown that the cost associated with setting up controlled environments and time spent on sample evaluation can be reduced. Nevertheless, the process is performed when the observations already belong to a characterized set, totally detached from a real environment. In order to solve the aforementioned, this work proposes a methodology for the generation of synthetic samples of malicious Portable Executable binaries and DoS cyber-attacks. The task is performed via a Reinforcement Learning engine, which learns from a baseline of different malware families and DoS cyber-attack network properties, resulting in new, mutated and highly functional samples. Experimental results demonstrate the high adaptability of the outputs as new input datasets for different Machine Learning algorithms.
AD  - Inst Politecn Nacl, ESIME Culhuacan, Mexico City 04440, MexicoAD  - Univ Electrocommun, Grad Sch Informat & Engn, Tokyo 1828585, JapanAD  - Univ Complutense Madrid UCM, Fac Comp Sci & Engn, Dept Software Engn & Artificial Intelligence DISIA, Grp Anal Secur & Syst GASS,Off 431, Madrid 28040, SpainC3  - Instituto Politecnico Nacional - MexicoC3  - University of Electro-Communications - JapanC3  - Complutense University of MadridFU  - National Science and Technology Council of Mexico (CONACyT); Instituto Politecnico Nacional; European Commission, LAZARUS [101070303]; Horizon Europe - Pillar II [101070303] Funding Source: Horizon Europe - Pillar II
FX  - The authors thank the National Science and Technology Council of Mexico (CONACyT) and the Instituto Politecnico Nacional for the financial support for this research. This work also was supported by the European Commission under the Horizon Europe Programme, as part of the project LAZARUS (Grant Agreement no. 101070303). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or European Commission - EU. Neither the European Union nor the European Commission can be held responsible for them.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 1424-8220
J9  - SENSORS-BASEL
JI  - Sensors
DA  - FEB
PY  - 2023
VL  - 23
IS  - 3
C7  - 1231
DO  - 10.3390/s23031231
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000929580000001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  76
ER  -

TY  - JOUR
AU  - Vouros, GA
TI  - Explainable Deep Reinforcement Learning: State of the Art and Challenges
T2  - ACM COMPUTING SURVEYS
LA  - English
KW  - Deep learning
KW  - deep reinforcement learning
KW  - interpretability
KW  - explainability
KW  - transparency
AB  - Interpretability, explainability, and transparency are key issues to introducing artificial intelligence methods in many critical domains. This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability, and fairness, and has important consequences toward keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. Although the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article, we aim to provide a review of state-of-the-art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators-that is, of those who make the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state-of-the-art methods, categorizing them into classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes by identifying open questions and important challenges.
AD  - Univ Piraeus, Gr Lambraki 126, Piraeus, GreeceC3  - University of PiraeusFU  - TAPASH2020-SESAR-2019-2 Project [892358]
FX  - Thiswork was partially supported by the TAPASH2020-SESAR-2019-2 Project (GA number 892358) Towards an Automated and exPlainable Air traffic management (ATM) System.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN  - 0360-0300
SN  - 1557-7341
J9  - ACM COMPUT SURV
JI  - ACM Comput. Surv.
DA  - JUN
PY  - 2023
VL  - 55
IS  - 5
C7  - 92
DO  - 10.1145/3527448
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000892356400006
N1  - Times Cited in Web of Science Core Collection:  14
Total Times Cited:  14
Cited Reference Count:  70
ER  -

TY  - JOUR
AU  - Henderson, P
AU  - Hu, JR
AU  - Romoff, J
AU  - Brunskill, E
AU  - Jurafsky, D
AU  - Pineau, J
TI  - Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning
T2  - JOURNAL OF MACHINE LEARNING RESEARCH
LA  - English
KW  - energy efficiency
KW  - green computing
KW  - reinforcement learning
KW  - deep learning
KW  - climate change
KW  - CLIMATE-CHANGE
KW  - SOCIAL COST
KW  - INFORMATION
KW  - BEHAVIOR
AB  - Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.
AD  - Stanford Univ, Stanford, CA 94305 USAAD  - Facebook, Menlo Pk, CA USAAD  - McGill Univ, Mila, Montreal, PQ, CanadaAD  - McGill Univ, Mila, Facebook AI Res, Montreal, PQ, CanadaC3  - Stanford UniversityC3  - Facebook IncC3  - McGill UniversityC3  - McGill UniversityC3  - Facebook IncPU  - MICROTOME PUBL
PI  - BROOKLINE
PA  - 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN  - 1532-4435
J9  - J MACH LEARN RES
JI  - J. Mach. Learn. Res.
PY  - 2020
VL  - 21
C7  - 248
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)AN  - WOS:000608918500001
N1  - Times Cited in Web of Science Core Collection:  135
Total Times Cited:  147
Cited Reference Count:  109
ER  -

TY  - CPAPER
AU  - Flamini, F
AU  - Hamann, A
AU  - Jerbi, S
AU  - Trenkwalder, LM
AU  - Nautrup, HP
AU  - Briegel, HJ
A1  - IEEE
TI  - Photonic architecture for reinforcement learning
T2  - 2020 PHOTONICS NORTH (PN)
LA  - English
CP  - Photonics North (PN) Conference
KW  - Integrated photonic circuits
KW  - Quantum photonics
KW  - Machine learning
KW  - Reinforcement learning
AB  - Reinforcement learning algorithms are a powerful tool to manage the interaction between a system and its environment. Here we present an approach to apply these algorithms within modern-day photonic technologies. Numerical tests, performed on typical learning tasks, show that the architecture is robust against experimental noise, which can even be beneficial for the learning process. The proposed architecture, based on single-photon evolution on a tree structure of tunable beamsplitters, is simple, easy to implement and an integration in quantum optics applications appears to be within the reach of near-term technology.
AD  - Univ Innsbruck, Inst Theoret Phys, Tech Str 25, A-6020 Innsbruck, AustriaC3  - University of InnsbruckFU  - European Union [801110]; Austrian Federal Ministry of Education, Science and Research (BMBWF); Austrian Science Fund (FWF) [DKALM: W1259-N27, SFB BeyondC F71]; Ministerium fur Wissenschaft, Forschung, und Kunst Baden-Wurttemberg [AZ:33-7533.-30-10/41/1]
FX  - This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie SkodowskaCurie grant agreement No 801110 and the Austrian Federal Ministry of Education, Science and Research (BMBWF). It reflects only the author's view and the Agency is not responsible for any use that may be made of the information it contains. HPN, LMT, SJ, and HJB acknowledge support from the Austrian Science Fund (FWF) through the projects DKALM: W1259-N27 and SFB BeyondC F71. HJB was also supported by the Ministerium fur Wissenschaft, Forschung, und Kunst Baden-Wurttemberg (AZ:33-7533.-30-10/41/1).
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-7281-8108-0
PY  - 2020
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000622970300034
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  6
ER  -

TY  - JOUR
AU  - Greene, T
AU  - Shmueli, G
AU  - Ray, S
TI  - Taking the Person Seriously: Ethically Aware IS Research in the Era of Reinforcement Learning-Based Personalization
T2  - JOURNAL OF THE ASSOCIATION FOR INFORMATION SYSTEMS
LA  - English
KW  - Personalization
KW  - Reinforcement Learning
KW  - Sociotechnical
KW  - Data Protection
KW  - AI Ethics
KW  - Digital Platforms
KW  - ARTIFICIAL-INTELLIGENCE
KW  - PRINCIPLES
KW  - AUTONOMY
KW  - BEHAVIOR
KW  - PERSUASION
KW  - CHALLENGES
KW  - FRAMEWORK
KW  - FACEBOOK
KW  - IDENTITY
KW  - PRIVACY
AB  - Advances in reinforcement learning and implicit data collection on large-scale commercial platforms mark the beginning of a new era of personalization aimed at the adaptive control of human user environments. We present five emergent features of this new paradigm of personalization that endanger persons and societies at scale and analyze their potential to reduce personal autonomy, destabilize social and political systems, and facilitate mass surveillance and social control, among other concerns. We argue that current data protection laws, most notably the European Union's General Data Protection Regulation, are limited in their ability to adequately address many of these issues. Nevertheless, we believe that IS researchers are well-situated to engage with and investigate this new era of personalization. We propose three distinct directions for ethically aware reinforcement learning-based personalization research uniquely suited to the strengths of IS researchers across the sociotechnical spectrum.
AD  - Copenhagen Business Sch, Dept Digitalizat, Frederiksberg, DenmarkAD  - Natl Tsing Hua Univ, Inst Serv Sci, Hsinchu, TaiwanC3  - Copenhagen Business SchoolC3  - National Tsing Hua UniversityPU  - ASSOC INFORMATION SYSTEMS
PI  - ATLANTA
PA  - GEORGIA STATE UNIV, 35 BROAD STREET, STE 916-917, ATLANTA, GA 30303 USA
SN  - 1536-9323
SN  - 1558-3457
J9  - J ASSOC INF SYST
JI  - J. Assoc. Inf. Syst.
PY  - 2023
VL  - 24
IS  - 6
SP  - 1527
EP  - 1561
DO  - 10.17705/1jais.00800
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)AN  - WOS:001108677100001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  244
ER  -

TY  - JOUR
AU  - Siegel, J
AU  - Pappas, G
TI  - Morals, ethics, and the technology capabilities and limitations of automated and self-driving vehicles
T2  - AI & SOCIETY
LA  - English
KW  - Self-driving
KW  - Autonomous vehicles
KW  - Automated vehicles
KW  - Ethics
KW  - Morals
KW  - Value systems
KW  - Artificial intelligence
AB  - We motivate the desire for self-driving and explain its potential and limitations, and explore the need for-and potential implementation of-morals, ethics, and other value systems as complementary "capabilities" to the Deep Technologies behind self-driving. We consider how the incorporation of such systems may drive or slow adoption of high automation within vehicles. First, we explore the role for morals, ethics, and other value systems in self-driving through a representative hypothetical dilemma faced by a self-driving car. Through the lens of engineering, we explain in simple terms common moral and ethical frameworks including utilitarianism, deontology, and virtue ethics before characterizing their relationship to the fundamental algorithms enabling self-driving. The concepts of behavior cloning, state-based modeling, and reinforcement learning are introduced, with some algorithms being more suitable for the implementation of value systems than others. We touch upon the contemporary cross-disciplinary landscape of morals and ethics in self-driving systems from a joint philosophical and technical perspective, and close with considerations for practitioners and the public, particularly as individuals may not appreciate the nuance and complexity of using imperfect information to navigate diverse scenarios and tough-to-quantify value systems, while "typical" software development reduces complex problems to black and white decision-making.
AD  - Michigan State Univ, Dept Comp Sci & Engn, 428 S Shaw Lane,Room 3115, E Lansing, MI 48824 USAAD  - Michigan State Univ, Dept Elect & Comp Engn, 428 S Shaw Lane,Room 2120, E Lansing, MI 48824 USAAD  - Natl Tech Univ Athens, Dept Elect & Comp Engn, 9 Iroon Polytech Str, Athens 15780, GreeceC3  - Michigan State UniversityC3  - Michigan State UniversityC3  - National Technical University of AthensPU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0951-5666
SN  - 1435-5655
J9  - AI SOC
JI  - AI Soc.
DA  - FEB
PY  - 2023
VL  - 38
IS  - 1
SP  - 213
EP  - 226
DO  - 10.1007/s00146-021-01277-y
C6  - SEP 2021
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000694788300001
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  6
Cited Reference Count:  53
ER  -

TY  - JOUR
AU  - Erdodi, L
AU  - Zennaro, FM
TI  - The Agent Web Model: modeling web hacking for reinforcement learning
T2  - INTERNATIONAL JOURNAL OF INFORMATION SECURITY
LA  - English
KW  - Agent Web Model
KW  - Penetration testing
KW  - Capture the flag
KW  - Reinforcement learning
KW  - LEVEL
AB  - Website hacking is a frequent attack type used by malicious actors to obtain confidential information, modify the integrity of web pages or make websites unavailable. The tools used by attackers are becoming more and more automated and sophisticated, and malicious machine learning agents seem to be the next development in this line. In order to provide ethical hackers with similar tools, and to understand the impact and the limitations of artificial agents, we present in this paper a model that formalizes web hacking tasks for reinforcement learning agents. Our model, named Agent Web Model, considers web hacking as a capture-the-flag style challenge, and it defines reinforcement learning problems at seven different levels of abstraction. We discuss the complexity of these problems in terms of actions and states an agent has to deal with, and we show that such a model allows to represent most of the relevant web vulnerabilities. Aware that the driver of advances in reinforcement learning is the availability of standardized challenges, we provide an implementation for the first three abstraction layers, in the hope that the community would consider these challenges in order to develop intelligent web hacking agents.
AD  - Univ Oslo, Dept Informat, N-0316 Oslo, NorwayC3  - University of OsloFU  - University of Oslo (Oslo University Hospital)
FX  - Open access funding provided by University of Oslo (incl Oslo University Hospital).
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 1615-5262
SN  - 1615-5270
J9  - INT J INF SECUR
JI  - Int. J. Inf. Secur.
DA  - APR
PY  - 2022
VL  - 21
IS  - 2
SP  - 293
EP  - 309
DO  - 10.1007/s10207-021-00554-7
C6  - JUN 2021
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000659001000001
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  44
ER  -

TY  - CPAPER
AU  - Henderson, P
AU  - Sinha, K
AU  - Angelard-Gontier, N
AU  - Ke, NR
AU  - Fried, G
AU  - Lowe, R
AU  - Pineau, J
A1  - ACM
TI  - Ethical Challenges in Data-Driven Dialogue Systems
T2  - PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18)
LA  - English
CP  - AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - Dialogue Systems
KW  - Natural Language Processing
KW  - Computers and Society
KW  - Ethics and Safety
KW  - Bias
KW  - Machine Learning
KW  - Reinforcement Learning
KW  - Privacy
KW  - Security
KW  - Reproducibility
KW  - Adversarial Examples
AB  - The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.
AD  - McGill Univ, Montreal, PQ, CanadaAD  - Mila, Montreal, PQ, CanadaAD  - Polytechn Montreal, Montreal, PQ, CanadaC3  - McGill UniversityC3  - Universite de MontrealC3  - Polytechnique MontrealFU  - Samsung Advanced Institute of Technology; NSERC Discovery Grant Program
FX  - The authors would like to thank the anonymous referees for their valuable comments and helpful suggestions. The work is supported by the Samsung Advanced Institute of Technology and the NSERC Discovery Grant Program.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-6012-8
PY  - 2018
SP  - 123
EP  - 129
DO  - 10.1145/3278721.3278777
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:000510018100022
N1  - Times Cited in Web of Science Core Collection:  39
Total Times Cited:  47
Cited Reference Count:  43
ER  -

TY  - CPAPER
AU  - Sun, FY
AU  - Chang, YY
AU  - Wu, YH
AU  - Lin, SD
A1  - ACM
TI  - Designing Non-greedy Reinforcement Learning Agents with Diminishing Reward Shaping
T2  - PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18)
LA  - English
CP  - AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - multi-agent reinforcement learning
KW  - reward shaping
KW  - non-greedy
AB  - This paper intends to address an issue in RL that when agents possessing varying capabilities, most resources may be acquired by stronger agents, leaving the weaker ones "starving". We introduce a simple method to train non-greedy agents in multi-agent reinforcement learning scenarios with nearly no extra cost. Our model can achieve the following goals in designing the non-greedy agent: non-homogeneous equality, only need local information, cost-effective, generalizable and configurable. We propose the idea of diminishing reward that makes the agent feel less satisfied for consecutive rewards obtained. This idea allows the agents to behave less greedy without the need to explicitly coding any ethical pattern nor monitor other agents' status. Given our framework, resources can be distributed more equally without running the risk of reaching homogeneous equality. We designed two games, Gathering Game and Hunter Prey to evaluate the quality of the model.
AD  - Natl Taiwan Univ, Taipei, TaiwanC3  - National Taiwan UniversityPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
SN  - 978-1-4503-6012-8
PY  - 2018
SP  - 297
EP  - 302
DO  - 10.1145/3278721.3278759
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:000510018100049
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  6
Cited Reference Count:  23
ER  -

TY  - JOUR
AU  - Wang, J
AU  - Gao, R
AU  - Zha, HY
TI  - Reliable Off-Policy Evaluation for Reinforcement Learning
T2  - OPERATIONS RESEARCH
LA  - English
KW  - uncertainty quanti
KW  - reinforcement learning
KW  - Wasserstein robust optimization
AB  - In a sequential decision-making problem, off-policy evaluation estimates the expected cumulative reward of a target policy using logged trajectory data generated from different behavior policy, without execution of the target policy. Reinforcement learning in high-stake environments, such as healthcare and education, is often limited to off-policy settings due to safety or ethical concerns or inability of exploration. Hence, it is imperative to quantify the uncertainty of the off-policy estimate before deployment of the target policy. In this paper, we propose a novel framework that provides robust and optimistic cumulative reward estimates using one or multiple logged trajectories data. Leveraging methodologies from distributionally robust optimization, we show that with proper selection of the size of the distributional uncertainty set, these estimates serve as confidence bounds with nonasymptotic and asymptotic guarantees under stochastic or adversarial environments. Our results are also generalized to batch reinforcement learning and are supported by empirical analysis.
AD  - Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R ChinaAD  - Univ Texas Austin, Dept Informat Risk & Operat Management, Austin, TX 78705 USAAD  - Chinese Univ Hong Kong, Shenzhen Inst Artificial Intelligence & Robot Soc, Sch Data Sci, Shenzhen 518172, Peoples R ChinaC3  - The Chinese University of Hong Kong, ShenzhenC3  - University of Texas SystemC3  - University of Texas AustinC3  - The Chinese University of Hong Kong, ShenzhenC3  - Shenzhen Institute of Artificial Intelligence & Robotics for SocietyPU  - INFORMS
PI  - CATONSVILLE
PA  - 5521 RESEARCH PARK DR, SUITE 200, CATONSVILLE, MD 21228 USA
SN  - 0030-364X
J9  - OPER RES
JI  - Oper. Res.
DA  - MAR-APR
PY  - 2024
VL  - 72
IS  - 2
DO  - 10.1287/opre.2022.2382
C6  - OCT 2022
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)AN  - WOS:000951937700001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  78
ER  -

TY  - CPAPER
AU  - Holler, J
AU  - Vuorio, R
AU  - Qin, ZW
AU  - Tang, XC
AU  - Jiao, Y
AU  - Jin, TC
AU  - Singh, S
AU  - Wang, CX
AU  - Ye, JP
ED  - Wang, J
ED  - Shim, K
ED  - Wu, X
TI  - Deep Reinforcement Learning for Multi-Driver Vehicle Dispatching and Repositioning Problem
T2  - 2019 19TH IEEE INTERNATIONAL CONFERENCE ON DATA MINING (ICDM 2019)
LA  - English
CP  - 19th IEEE International Conference on Data Mining (ICDM)
KW  - reinforcement learning
KW  - ride-sharing
KW  - fleet management
KW  - order dispatching
AB  - Order dispatching and driver repositioning (also known as fleet management) in the face of spatially and temporally varying supply and demand are central to a ride-sharing platform marketplace. Hand-crafting heuristic solutions that account for the dynamics in these resource allocation problems is difficult, and may be better handled by an end-to-end machine learning method. Previous works have explored machine learning methods to the problem from a high-level perspective, where the learning method is responsible for either repositioning the drivers or dispatching orders, and as a further simplification, the drivers are considered independent agents maximizing their own reward functions. In this paper we present a deep reinforcement learning approach for tackling the full fleet management and dispatching problems. In addition to treating the drivers as individual agents, we consider the problem from a system-centric perspective, where a central fleet management agent is responsible for decision-making for all drivers.
AD  - Univ Michigan, Ann Arbor, MI 48109 USAAD  - Didi Chuxing, Beijing, Peoples R ChinaC3  - University of Michigan SystemC3  - University of MichiganPU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1550-4786
SN  - 978-1-7281-4603-4
J9  - IEEE DATA MINING
PY  - 2019
SP  - 1090
EP  - 1095
DO  - 10.1109/ICDM.2019.00129
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000555729900120
N1  - Times Cited in Web of Science Core Collection:  57
Total Times Cited:  64
Cited Reference Count:  17
ER  -

TY  - JOUR
AU  - Agrawal, S
AU  - Dubey, S
AU  - Naik, KJ
TI  - Deep reinforcement learning for forecasting fish survival in open aquaculture ecosystem
T2  - ENVIRONMENTAL MONITORING AND ASSESSMENT
LA  - English
KW  - Fish survival prediction
KW  - Deep reinforcement learning
KW  - Deep Q-learning
KW  - Classification model
KW  - DECISION-SUPPORT
KW  - CLASSIFICATION
AB  - Ensuring the classification of water bodies suitable for fish habitat is essential for animal preservation and commercial fish farming. However, existing supervised machine learning models for predicting water quality lack specificity regarding fish survival. This study addresses this limitation and presents a novel model for forecasting fish viability in open aquaculture ecosystems. The proposed model combines reinforcement learning through Q-learning and deep feed-forward neural networks, enabling it to capture intricate patterns and relationships in complex aquatic environments. Moreover, the model's reinforcement learning capability reduces the reliance on labeled data and offers potential for continuous improvement over time. By accurately classifying water bodies based on fish suitability, the proposed model provides valuable insights for sustainable aquaculture management and environmental conservation. Experimental results show a significantly improved accuracy of 96% for the proposed DQN-based model, outperforming existing Gaussian Naive Bayes (78%), Random Forest (86%), and K-Nearest Neighbors (92%) classifiers on the same dataset. These findings highlight the effectiveness of the proposed approach in forecasting fish viability and its potential to address the limitations of existing models.
AD  - Natl Inst Technol Raipur, Dept Comp Sci & Engn, Raipur, IndiaC3  - National Institute of Technology (NIT System)C3  - National Institute of Technology RaipurFU  - We are grateful to the National Institute of Technology Raipur for encouraging us and moral support to work on research and innovation.
FX  - We are grateful to the National Institute of Technology Raipur for encouraging us and moral support to work on research and innovation.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0167-6369
SN  - 1573-2959
J9  - ENVIRON MONIT ASSESS
JI  - Environ. Monit. Assess.
DA  - NOV
PY  - 2023
VL  - 195
IS  - 11
C7  - 1389
DO  - 10.1007/s10661-023-11937-9
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001090376700002
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  31
ER  -

TY  - JOUR
AU  - Zhang, Q
AU  - Chen, BY
AU  - Liu, GH
TI  - Artificial intelligence can dynamically adjust strategies for auxiliary diagnosing respiratory diseases and analyzing potential pathological relationships
T2  - JOURNAL OF BREATH RESEARCH
LA  - English
KW  - artificial intelligence
KW  - adaptive disease diagnosis
KW  - auxiliary pathology analysis
AB  - Respiratory diseases are one of the leading causes of human death and exacerbate the global burden of non-communicable diseases. Finding a method to assist clinicians pre-diagnose these diseases is an urgent task. Existing artificial intelligence-based methods can improve the clinical diagnosis efficiency, but still face challenges. For example, the lack of interpretability, the problem of information redundancy or missing caused by only using static data, the difficulty of model to learn the interdependence between features, and the performance of model is limited by sparse datasets, etc. To alleviate these problems, we propose a novel RQPA-Net. It consists of Q & A diagnosis module (QAD) and pathological inference module (PI). The QAD is responsible for interacting with patients, adjusting inquiry strategies dynamically and collecting effective information for disease diagnosis. The designed multi-subspace network can alleviate the problem that classical method is difficult to understand the interdependence between features. The deep reinforcement learning designed also can alleviate the problem of classical methods lack of interpretability. The PI is responsible for reasoning potential pathological relationships between diseases or symptoms based on existing knowledge. Through integrating the advantages of deep learning and reinforcement learning techniques, PI can handle sparse datasets. Finally, for auxiliary diagnosis, the model achieves 0.9780 +/- 0.0002 Recall, 0.9778 +/- 0.0003 Acc, 0.9779 +/- 0.0003 Precision and 0.9780 +/- 0.0003 F1-score on the test set. In terms of assisting pathological analysis, compared with the end-to-end model, our model achieves higher comprehensive performance on different tasks and datasets with different degrees of sparsity. Even in sparse datasets, it can effectively infer potential associations between diseases or symptoms, and has higher potential clinical application. In this paper, we propose a novel network structure, which can not only assist doctors in diagnosing diseases, but also contribute to explore the potential disease mechanisms. It provides a new perspective for integrating AI technology and clinical practice.
AD  - Nankai Univ, Coll Elect Informat & Opt Engn, Tianjin 300350, Peoples R ChinaAD  - Nankai Univ, Tianjin Key Lab Optoelect Sensor & Sensing Network, Tianjin 300350, Peoples R ChinaAD  - Nankai Univ, Gen Terminal IC Interdisciplinary Sci Ctr, Tianjin 300350, Peoples R ChinaAD  - Nankai Univ, Engn Res Ctr Thin Film Optoelect Technol, Minist Educ, Tianjin 300350, Peoples R ChinaC3  - Nankai UniversityC3  - Nankai UniversityC3  - Nankai UniversityC3  - Nankai UniversityFU  - The authors thank the reviewers and editors for their comments and suggestions. Quan Zhang and Binyue Chen contributed equally to this work, and in no particular order.
FX  - The authors thank the reviewers and editors for their comments and suggestions. Quan Zhang and Binyue Chen contributed equally to this work, and in no particular order.
PU  - IOP Publishing Ltd
PI  - BRISTOL
PA  - TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND
SN  - 1752-7155
SN  - 1752-7163
J9  - J BREATH RES
JI  - J. Breath Res.
DA  - OCT 1
PY  - 2023
VL  - 17
IS  - 4
C7  - 046007
DO  - 10.1088/1752-7163/acf065
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001054312100001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  45
ER  -

TY  - JOUR
AU  - Amato, C
AU  - Ammar, HB
AU  - Churchill, E
AU  - Karpas, E
AU  - Kido, T
AU  - Kuniavsky, M
AU  - Lawless, WF
AU  - Oliehoek, FA
AU  - Rossi, F
AU  - Russell, S
AU  - Srivastava, S
AU  - Takadama, K
AU  - van Allen, P
AU  - Venable, KB
AU  - Tuyls, K
AU  - Vrancx, P
AU  - Zhang, SQ
TI  - Reports on the 2018 AAAI Spring Symposium Series
T2  - AI MAGAZINE
LA  - English
AB  - The Association for the Advancement of Artificial Intelligence, in cooperation with Stanford University's Department of Computer Science, presented the 2018 Spring Symposium Series, held March 26-28, 2018, on the campus of Stanford University. The seven symposia held were AI and Society: Ethics, Safety, and Trustworthiness in Intelligent Agents; Artificial Intelligence for the Internet of Everything; Beyond Machine Intelligence: Understanding Cognitive Bias and Humanity for WellBeing AI; Data-Efficient Reinforcement Learning; The Design of the User Experience for Artificial Intelligence (the UX of AI); Integrated Representation, Reasoning, Learning, and Execution for Goal-Directed Autonomy; Learning, Inference, and Control of Multiagent Systems. This report, compiled from organizers of the symposia, summarizes the research of the symposia that took place.
AD  - Northeastern Univ, Boston, MA 02115 USAAD  - PROWLER io, Reinforcement Learning, Cambridge, EnglandAD  - Google, UX, Mountain View, CA USAAD  - Technion Israel Inst Technol, Haifa, IsraelAD  - Stanford Univ, Stanford, CA 94305 USAAD  - Parc, Palo Alto, CA USAAD  - Paine Coll, Augusta, GA USAAD  - Delft Univ Technol, Delft, NetherlandsAD  - Univ Liverpool, Liverpool, Merseyside, EnglandAD  - IBM TJ Watson Res Ctr, Ossining, NY USAAD  - Univ Padua, Comp Sci, Padua, ItalyAD  - US Army Res Lab, Adelphi, MD USAAD  - Arizona State Univ, Tempe, AZ 85287 USAAD  - Univ Electrocommun, Tokyo, JapanAD  - Google DeepMind, London, EnglandAD  - Art Ctr Coll Design, Media Design Practices Dept, Pasadena, CA USAAD  - Tulane Univ, Comp Sci, New Orleans, LA 70118 USAAD  - Florida Inst Human & Machine Cognit IHMC, Pensacola, FL USAAD  - PROWLER io, Cambridge, EnglandAD  - Cleveland State Univ, Cleveland, OH 44115 USAC3  - Northeastern UniversityC3  - Google IncorporatedC3  - Technion Israel Institute of TechnologyC3  - Stanford UniversityC3  - Paine CollegeC3  - Delft University of TechnologyC3  - University of LiverpoolC3  - International Business Machines (IBM)C3  - University of PaduaC3  - United States Department of DefenseC3  - United States ArmyC3  - US Army Research, Development & Engineering Command (RDECOM)C3  - US Army Research Laboratory (ARL)C3  - Arizona State UniversityC3  - Arizona State University-TempeC3  - University of Electro-Communications - JapanC3  - Google IncorporatedC3  - Tulane UniversityC3  - Florida Institute for Human & Machine Cognition (IHMC)C3  - University System of OhioC3  - Cleveland State UniversityPU  - AMER ASSOC ARTIFICIAL INTELL
PI  - MENLO PK
PA  - 445 BURGESS DRIVE, MENLO PK, CA 94025-3496 USA
SN  - 0738-4602
J9  - AI MAG
JI  - AI Mag.
DA  - WIN
PY  - 2018
VL  - 39
IS  - 4
SP  - 29
EP  - 35
DO  - 10.1609/aimag.v39i4.2824
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000453661300007
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  0
ER  -

TY  - JOUR
AU  - Felizardo, LK
AU  - Fadda, E
AU  - Del-Moral-Hernandez, E
AU  - Brandimarte, P
TI  - Reinforcement learning approaches for the stochastic discrete lot-sizing problem on parallel machines
T2  - EXPERT SYSTEMS WITH APPLICATIONS
LA  - English
KW  - Dynamic programming
KW  - Stochastic programming
KW  - Multi-agent systems
KW  - Machine learning
KW  - Reinforcement Learning
KW  - SEQUENCE-DEPENDENT SETUP
KW  - SCHEDULING PROBLEM
KW  - DYNAMIC CONTROL
KW  - TIMES
KW  - EXTENSIONS
AB  - This paper addresses the stochastic discrete lot-sizing problem on parallel machines, which is a computationally challenging problem also for relatively small instances. We propose two heuristics to deal with it by leveraging reinforcement learning. In particular, we propose a technique based on approximate value iteration around post-decision state variables and one based on multi-agent reinforcement learning. We compare these two approaches with other reinforcement learning methods and more classical solution techniques, showing their effectiveness in addressing realistic size instances.
AD  - Univ Sao Paulo, Escola Politecn, Ave Prof Luciano Gualberto 380, Sao Paulo, BrazilAD  - Politecn Torino, Dipartimento Sci Matemat, Corso Duca Abruzzi 24, I-10129 Turin, ItalyC3  - Universidade de Sao PauloC3  - Polytechnic University of TurinFU  - Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES - Coordination for the Improvement of Higher Education Personnel), Brazil [001, 88882.333380/2019-01]
FX  - This research was financed in part by the Coordenacao de Aper-feicoamento de Pessoal de Nivel Superior (CAPES-Coordination for the Improvement of Higher Education Personnel, Finance Code 001, grant 88882.333380/2019-01) , Brazil.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0957-4174
SN  - 1873-6793
J9  - EXPERT SYST APPL
JI  - Expert Syst. Appl.
DA  - JUL 15
PY  - 2024
VL  - 246
C7  - 123036
DO  - 10.1016/j.eswa.2023.123036
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001164205400001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  48
ER  -

TY  - CPAPER
AU  - Estiri, E
AU  - Mirinejad, H
ED  - Chang, CK
ED  - Chang, RN
ED  - Fan, J
ED  - Fox, GC
ED  - Jin, Z
ED  - Pravadelli, G
ED  - Shahriar, H
TI  - Precision Dosing in Critical Care: Application of Machine Learning in Fluid Therapy
T2  - 2023 IEEE INTERNATIONAL CONFERENCE ON DIGITAL HEALTH, ICDH
LA  - English
CP  - IEEE International Conference on Digital Health (IEEE ICDH) at the IEEE World Congress on Services (SERVICES)
KW  - Machine learning
KW  - reinforcement learning
KW  - fluid management
KW  - automated fluid therapy
KW  - mean arterial pressure
KW  - MANAGEMENT
AB  - Fluid therapy is a common treatment for hypovolemic scenarios to restore the lost blood volume and stabilize acutely ill patients. Automating fluid therapy can lead to a reduction of delay in care, a decrement in dosing errors, and a reduction of cognitive load on clinicians responsible for patient care resulting in improved patient outcomes. However, this process is highly challenging due to the complexity of patient's physiology and the variability of hemodynamic responses among patients. This work presents a novel machine learning approach based on reinforcement learning (RL) for automated fluid management, where the RL agent is designed to recommend subject-specific infusion dosages without having the knowledge of dose-response models and only by interacting with the environment (virtual subject generator). Compared to the state-of-the-art focusing on the entire population's data, the proposed approach uses individual patient's data to recommend patient-specific fluid dosage adjustment. Simulation results demonstrate that the proposed approach outperforms a proportional-integral-derivative (PID) and a rule-based fluid resuscitation controller previously reported for an animal study.
AD  - Kennesaw State Univ, Coll Aeronaut & Engn, Kennesaw, GA 30144 USAC3  - University System of GeorgiaC3  - Kennesaw State UniversityFU  - National Science Foundation [2138929]
FX  - This material is based upon work supported by the National Science Foundation under Grant NO. 2138929.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 979-8-3503-4103-4
PY  - 2023
SP  - 348
EP  - 352
DO  - 10.1109/ICDH60066.2023.00058
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001062475200048
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  21
ER  -

TY  - CPAPER
AU  - Mofrad, MH
AU  - Melhem, R
AU  - Hammoud, M
A1  - IEEE
TI  - Revolver: Vertex-centric Graph Partitioning Using Reinforcement Learning
T2  - PROCEEDINGS 2018 IEEE 11TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING (CLOUD)
LA  - English
CP  - 11th IEEE International Conference on Cloud Computing (CLOUD) Part of the IEEE World Congress on Services
KW  - Graph partitioning
KW  - reinforcement learning
KW  - learning automata
KW  - label propagation
KW  - AUTOMATA
AB  - Big graph analytics is gaining a widespread momentum across different fields, including biology, computer vision, social networks, recommendation systems and transportation logistics, to mention just a few. Distributed systems for graph analytics are utilized as a mean to process big graphs. To distribute and balance computation and communication loads within a distributed graph analytics system, graph partitioning algorithms can be leveraged. In this paper, we propose Revolver, a machine learning-based graph partitioning algorithm. In particular, Revolver uses reinforcement learning and label propagation to efficiently and effectively carry out the task of graph partitioning. It employs a vertex-centric approach where each vertex in a graph is associated with an autonomous agent responsible for assigning a suitable partition to the vertex. In addition, it uses label propagation to evaluate the decency of partitioning. Evaluation results show that Revolver can produce highly balanced and localized partitions compared to three popular and state-of-the-art graph partitioning algorithms.
AD  - Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15260 USAAD  - Carnegie Mellon Univ Qatar, Doha, QatarC3  - Pennsylvania Commonwealth System of Higher Education (PCSHE)C3  - University of PittsburghC3  - Qatar Foundation (QF)C3  - Carnegie Mellon University in QatarFU  - Qatar National Research Fund (a member of Qatar Foundation) [7-1330-2-483]; University of Pittsburgh Center for Research Computing
FX  - This publication was made possible by NPRP grant #7-1330-2-483 from the Qatar National Research Fund (a member of Qatar Foundation). The statements made herein are solely the responsibility of the authors. This research was supported in part by the University of Pittsburgh Center for Research Computing through the resources provided.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-5386-7235-8
PY  - 2018
SP  - 818
EP  - 821
DO  - 10.1109/CLOUD.2018.00111
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000454741700105
N1  - Times Cited in Web of Science Core Collection:  7
Total Times Cited:  8
Cited Reference Count:  19
ER  -

TY  - JOUR
AU  - Arranz, R
AU  - Carramiñana, D
AU  - de Miguel, G
AU  - Besada, JA
AU  - Bernardos, AM
TI  - Application of Deep Reinforcement Learning to UAV Swarming for Ground Surveillance
T2  - SENSORS
LA  - English
KW  - artificial intelligence
KW  - swarm
KW  - drones
KW  - search
KW  - tracking
KW  - obstacle avoidance
KW  - centralized
KW  - GENETIC ALGORITHM
KW  - OPTIMIZATION
KW  - ARCHITECTURE
KW  - TRACKING
AB  - This paper summarizes in depth the state of the art of aerial swarms, covering both classical and new reinforcement-learning-based approaches for their management. Then, it proposes a hybrid AI system, integrating deep reinforcement learning in a multi-agent centralized swarm architecture. The proposed system is tailored to perform surveillance of a specific area, searching and tracking ground targets, for security and law enforcement applications. The swarm is governed by a central swarm controller responsible for distributing different search and tracking tasks among the cooperating UAVs. Each UAV agent is then controlled by a collection of cooperative sub-agents, whose behaviors have been trained using different deep reinforcement learning models, tailored for the different task types proposed by the swarm controller. More specifically, proximal policy optimization (PPO) algorithms were used to train the agents' behavior. In addition, several metrics to assess the performance of the swarm in this application were defined. The results obtained through simulation show that our system searches the operation area effectively, acquires the targets in a reasonable time, and is capable of tracking them continuously and consistently.
AD  - Univ Politecn Madrid, Informat Proc & Telecommun Ctr, ETSI Telecomunicac, Ave Complutense 30, Madrid 28040, SpainC3  - Universidad Politecnica de MadridC3  - Centro de I+D+I en Procesado de la Informacion Telecomunicaciones (IPT)FU  - The authors acknowledge the initial work performed in the problem by Enrique Parro and Cesar Alberte, which allowed them to develop preliminary versions of the swarming training and demonstration infrastructure used for this research
FX  - The authors acknowledge the initial work performed in the problem by Enrique Parro and Cesar Alberte, which allowed them to develop preliminary versions of the swarming training and demonstration infrastructure used for this research
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 1424-8220
J9  - SENSORS-BASEL
JI  - Sensors
DA  - NOV
PY  - 2023
VL  - 23
IS  - 21
C7  - 8766
DO  - 10.3390/s23218766
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001099494000001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  78
ER  -

TY  - JOUR
AU  - Yang, YG
AU  - Bevan, MA
AU  - Li, B
TI  - Hierarchical Planning with Deep Reinforcement Learning for 3D Navigation of Microrobots in Blood Vessels
T2  - ADVANCED INTELLIGENT SYSTEMS
LA  - English
KW  - artificial intelligence
KW  - autonomous navigation
KW  - deep reinforcement learning
KW  - microrobot
KW  - LEVEL
KW  - CELLS
AB  - Designing intelligent microrobots that can autonomously navigate and perform instructed routines in blood vessels, a crowded environment with complexities including Brownian disturbance, concentrated cells, confinement, different flow patterns, and diverse vascular geometries, can offer enormous opportunities and challenges in biomedical applications. Herein, a biological-agent mimicking a hierarchical control scheme that enables a microrobot to efficiently navigate and execute customizable routines in simplified blood vessel environments is reported. The control scheme consists of two decoupled components: a high-level controller decomposing complex navigation tasks into short-ranged, simpler subtasks and a low-level deep reinforcement learning (DRL) controller responsible for maneuvering microrobots to accomplish subtasks. The proposed DRL controller utilizes 3D convolutional neural networks and is capable of learning control policies directly from raw 3D sensory data. It is shown that such a control scheme achieves effective and robust decision-making within unseen, diverse complicated environments and offers flexibility for customizable task routines. This study provides a proof of principle for designing intelligent control systems for autonomous navigation in vascular networks for microrobots.
AD  - Tsinghua Univ, Dept Engn Mech, Appl Mech Lab, Inst Biomech & Med Engn, Beijing 100084, Peoples R ChinaAD  - Johns Hopkins Univ, Chem & Biomol Engn, Baltimore, MD 21218 USAC3  - Tsinghua UniversityC3  - Johns Hopkins UniversityFU  - National Natural Science Foundation of China [11961131005, 11922207, 11921002]
FX  - This work was supported by the National Natural Science Foundation of China (Grant Nos. 11961131005, 11922207, and 11921002).
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN  - 2640-4567
J9  - ADV INTELL SYST-GER
JI  - Adv. Intell. Syst.
DA  - NOV
PY  - 2022
VL  - 4
IS  - 11
DO  - 10.1002/aisy.202200168
C6  - SEP 2022
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000859251800001
N1  - Times Cited in Web of Science Core Collection:  7
Total Times Cited:  8
Cited Reference Count:  48
ER  -

TY  - JOUR
AU  - Lee, CS
AU  - Wang, MH
AU  - Tsai, YL
AU  - Chang, WS
AU  - Reformat, M
AU  - Acampora, G
AU  - Kubota, N
TI  - FML-Based Reinforcement Learning Agent with Fuzzy Ontology for Human-Robot Cooperative Edutainment
T2  - INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS
LA  - English
KW  - Fuzzy markup language
KW  - reinforcement learning
KW  - agent
KW  - human-robot co-learning
KW  - fuzzy ontology
KW  - MARKUP LANGUAGE
KW  - SYSTEM
KW  - IMPACT
AB  - The currently observed developments in Artificial Intelligence (AI) and its influence on different types of industries mean that human-robot cooperation is of special importance. Various types of robots have been applied to the so-called field of Edutainment, i.e., the field that combines education with entertainment. This paper introduces a novel fuzzy-based system for a human-robot cooperative Edutainment. This co-learning system includes a brain-computer interface (BCI) ontology model and a Fuzzy Markup Language (FML)-based Reinforcement Learning Agent (FRL-Agent). The proposed FRL-Agent is composed of (1) a human learning agent, (2) a robotic teaching agent, (3) a Bayesian estimation agent, (4) a robotic BCI agent, (5) a fuzzy machine learning agent, and (6) a fuzzy BCI ontology. In order to verify the effectiveness of the proposed system, the FRL-Agent is used as a robot teacher in a number of elementary schools, junior high schools, and at a university to allow robot teachers and students to learn together in the classroom. The participated students use handheld devices to indirectly or directly interact with the robot teachers to learn English. Additionally, a number of university students wear a commercial EEG device with eight electrode channels to learn English and listen to music. In the experiments, the robotic BCI agent analyzes the collected signals from the EEG device and transforms them into five physiological indices when the students are learning or listening. The Bayesian estimation agent and fuzzy machine learning agent optimize the parameters of the FRL agent and store them in the fuzzy BCI ontology. The experimental results show that the robot teachers motivate students to learn and stimulate their progress. The fuzzy machine learning agent is able to predict the five physiological indices based on the eight-channel EEG data and the trained model. In addition, we also train the model to predict the other students' feelings based on the analyzed physiological indices and labeled feelings. The FRL agent is able to provide personalized learning content based on the developed human and robot cooperative edutainment approaches. To our knowledge, the FRL agent has not applied to the teaching fields such as elementary schools before and it opens up a promising new line of research in human and robot co-learning. In the future, we hope the FRL agent will solve such an existing problem in the classroom that the high-performing students feel the learning contents are too simple to motivate their learning or the low-performing students are unable to keep up with the learning progress to choose to give up learning.
AD  - Natl Univ Tainan, Dept Comp Sci & Informat Engn, Tainan, TaiwanAD  - Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB, CanadaAD  - Univ Social Sci, Informat Technol Inst, Warsaw, PolandAD  - Univ Naples Federico II, Dept Phys, Naples, ItalyAD  - Tokyo Metropolitan Univ, Grad Sch Syst Design, Tokyo, JapanC3  - National University TainanC3  - University of AlbertaC3  - University of Social SciencesC3  - University of Naples Federico IIC3  - Tokyo Metropolitan UniversityFU  - Ministry of Science Technology [MOST 106-3114-E-024-001, 107-2218-E-024-001, 108-2218-E-024001]
FX  - The authors would like to thank the financial support from the Ministry of Science Technology (MOST 106-3114-E-024-001, 107-2218-E-024-001, and 108-2218-E-024001). Additionally, the authors would like to thank Pei-Yu Lee, Cathy Chang, Chi- Hsiung Liu, the involved staff of KWS research center, OASE Lab. members, Kubota Lab. members, Yamaguchi Lab. members, and students and teachers from Rende Junior High School, Gueinan Elementary School, Rende Elementary School, and Hsinda Elementary School. Finally, we would like to thank Li-Wei Ko and Bo-Yu Tsai for their technical support with the commercial EEG device. The implemented experiments related to human research ethics have been approved by the Human Research Ethics Committee at National Cheng Kung University (NCKU HREC) in Oct. 2019 and its approval No. is NCKU HREC-F-108-326-2.
PU  - WORLD SCIENTIFIC PUBL CO PTE LTD
PI  - SINGAPORE
PA  - 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN  - 0218-4885
SN  - 1793-6411
J9  - INT J UNCERTAIN FUZZ
JI  - Int. J. Uncertainty Fuzziness Knowl.-Based Syst.
DA  - DEC
PY  - 2020
VL  - 28
IS  - 6
SP  - 1023
EP  - 1060
DO  - 10.1142/S0218488520500440
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000599916700007
N1  - Times Cited in Web of Science Core Collection:  7
Total Times Cited:  7
Cited Reference Count:  41
ER  -

TY  - JOUR
AU  - Bagaa, M
AU  - Taleb, T
AU  - Riekki, J
AU  - Song, JS
TI  - Collaborative Cross System AI: Toward 5G System and Beyond
T2  - IEEE NETWORK
LA  - English
KW  - Artificial intelligence
KW  - Task analysis
KW  - Collaboration
KW  - Supervised learning
KW  - Reinforcement learning
KW  - Automation
KW  - 5G mobile communication
AB  - The emerging industrial verticals set new challenges for 5G and beyond systems. Indeed, the heterogeneity of the underlying technologies and the challenging and conflicting requirements of the verticals make the orchestration and management of networks complicated and challenging. Recent advances in network automation and artificial intelligence (AI) create enthusiasm from industries and academia toward applying these concepts and techniques to tackle these challenges. With these techniques, the network can be autonomously optimized and configured. This article suggests a collaborative cross-system AI that leverages diverse data from different segments involved in the end-to-end communication of a service, diverse AI techniques, and diverse network automation tools to create a self-optimized and self-orchestrated network that can adapt according to the network state. We align the proposed framework with the ongoing network standardization.
AD  - Aalto Univ, Espoo, FinlandAD  - Aalto Univ, Sch Elect Engn, Espoo, FinlandAD  - Oulu Univ, Oulu, FinlandAD  - Sejong Univ, Seoul, South KoreaAD  - Sejong Univ, Dept Comp & Informat Secur & Convergence Engn Int, Leading Software Engn & Secur Lab SESLab, Seoul, South KoreaC3  - Aalto UniversityC3  - Aalto UniversityC3  - University of OuluC3  - Sejong UniversityC3  - Sejong UniversityFU  - European Union [101016509]; Academy of Finland 6Genesis project [318927]; Academy of Finland CSN project [311654]; Institute of Information & Communications Technology Planning & Evaluation (IITP) [2020-0-00959]; CSN project
FX  - The research leading to these results received funding from the European Union's Horizon 2020 research and innovation program under grant agreement no. 101016509 (project CHARITY). The article reflects only the authors' views. The Commission is not responsible for any use that may be made of the information it contains. The research work is also partially funded by the Academy of Finland 6Genesis project under Grant No. 318927, and by the Academy of Finland CSN project under Grant No. 311654. Prof. Song was supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) (No. 2020-0-00959). Dr. Miloud Bagaa work was supported by the CSN project.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 0890-8044
SN  - 1558-156X
J9  - IEEE NETWORK
JI  - IEEE Netw.
DA  - JUL-AUG
PY  - 2021
VL  - 35
IS  - 4
SP  - 286
EP  - 294
DO  - 10.1109/MNET.011.2000607
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000688521300051
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  6
Cited Reference Count:  15
ER  -

TY  - CPAPER
AU  - Jaensch, F
AU  - Klingel, L
AU  - Verl, A
A1  - IEEE
TI  - Virtual Commissioning Simulation as OpenAI Gym - A Reinforcement Learning Environment for Control Systems
T2  - 2022 5TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE FOR INDUSTRIES, AI4I
LA  - English
CP  - 5th IEEE International Conference on Artificial Intelligence for Industries (AI4I)
KW  - reinforcement learning
KW  - virtual commissioning
KW  - simulation
KW  - digital twin
AB  - Manual development of control systems' software is time-consuming and error-prone. Thus, high costs are already incurred in this phase of mechatronic system development. Virtual prototypes have so far only been used for testing purposes, such as virtual commissioning, but not for the automated creation of the control. A good test environment can also be extended to a learning environment with appropriate trial and error based algorithms. This work shows an approach to extend an industrial software tool for virtual commissioning as a standardized OpenAI gym environment. Thereby, established reinforcement learning algorithms can be used more easily and a step towards an industrial application of self-learning control systems can be made. The goal of this work is to provide industry and research with a platform for easy entry into the field of reinforcement learning.
AD  - Univ Stuttgart, ISW, Stuttgart, GermanyC3  - University of StuttgartFU  - German Federal Ministry of Education and Research (BMBF) [01IS21027 A C]
FX  - This research and development project is funded by the German Federal Ministry of Education and Research (BMBF) within the 01IS21027 A C and managed by the German Aerospace Center (DLR) Projekttrager. The author is responsible for the contents of this publication."
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2770-470X
SN  - 978-1-6654-5961-7
J9  - AI for Industries
PY  - 2022
SP  - 64
EP  - 67
DO  - 10.1109/AI4I54798.2022.00023
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001022029600017
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  14
ER  -

TY  - CPAPER
AU  - Behzadan, V
AU  - Minton, J
AU  - Munir, A
A1  - Assoc Comp Machinery
TI  - TrolleyMod v1.0: An Open-Source Simulation and Data Collection Platform for Ethical Decision-Making in Autonomous Vehicles
T2  - AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY
LA  - English
CP  - 2nd AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - Ethical Decision-Making
KW  - Autonomous Vehicles
KW  - Social Choice
KW  - Artificial Intelligence
KW  - Simulation
AB  - This paper presents TrolleyMod v1.0, an open-source platform based on the CARLA simulator for the collection of ethical decision making data for autonomous vehicles. This platform is designed to facilitate experiments aiming to observe and record human decisions and actions in high-fidelity simulations of ethical dilemmas that occur in the context of driving. Targeting experiments in the class of trolley problems, TrolleyMod provides a seamless approach to creating new experimental settings and environments with the realistic physics-engine and the high-quality graphical capabilities of CARLA and the Unreal Engine. Also, TrolleyMod provides a straightforward interface between the CARLA environment and Python to enable the implementation of custom controllers, such as deep reinforcement learning agents. The results of such experiments can be used for sociological analyses, as well as the training and tuning of value-aligned autonomous vehicles based on social values that are inferred from observations.
AD  - Kansas State Univ, Manhattan, KS 66506 USAC3  - Kansas State UniversityPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
SN  - 978-1-4503-6324-2
PY  - 2019
SP  - 391
EP  - 395
DO  - 10.1145/3306618.3314239
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000556121100054
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  18
ER  -

TY  - JOUR
AU  - Crespi, M
AU  - Ferigo, A
AU  - Custode, LL
AU  - Iacca, G
TI  - A population-based approach for multi-agent interpretable reinforcement learning
T2  - APPLIED SOFT COMPUTING
LA  - English
KW  - Multi-Agent Reinforcement Learning
KW  - Evolutionary algorithm
KW  - Explainable artificial intelligence
AB  - Multi-Agent Reinforcement Learning (MARL) made significant progress in the last decade, mainly thanks to the major developments in the field of Deep Neural Networks (DNNs). However, DNNs suffer from a fundamental issue: their lack of interpretability. While this is true for most applications of DNNs, this is exacerbated in their applications in MARL. In fact, the mutual interactions between agents and environment, as well as across agents, make it particularly difficult to understand learned strategies in these settings. One possible way to achieve explainability in MARL is through the use of interpretable models, such as decision trees, that allow for a direct inspection and understanding of their inner workings. In this work, we make a step forward in this direction, proposing a population based algorithm that combines evolutionary principles with RL for training interpretable models in multi-agent systems. We evaluate the proposed approach in a highly dynamic task where two teams of agents compete with each other. We test different variants of the proposed method in different settings, namely with/without coevolution and with/without initialization from a handcrafted policy. We find that, in most settings, our method is able to find fairly effective policies. Moreover, we show that the learned policies are easy to inspect and, possibly, interpreted based on domain knowledge.
AD  - Univ Trento, Dept Informat Engn & Comp Sci, Via Sommar 9, I-38123 Trento, ItalyC3  - University of TrentoFU  - European Union [101071179]; Horizon Europe - Pillar III [101071179] Funding Source: Horizon Europe - Pillar III
FX  - Funded by the European Union (project no. 101071179) . Views and opinions expressed are however those of the author (s) only and do not necessarily reflect those of the Euro- pean Union or EISMEA. Neither the European Union nor the granting authority can be held responsible for them.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 1568-4946
SN  - 1872-9681
J9  - APPL SOFT COMPUT
JI  - Appl. Soft. Comput.
DA  - NOV
PY  - 2023
VL  - 147
C7  - 110758
DO  - 10.1016/j.asoc.2023.110758
C6  - AUG 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001066938500001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  40
ER  -

TY  - JOUR
AU  - Koch, W
AU  - Mancuso, R
AU  - West, R
AU  - Bestavros, A
TI  - Reinforcement Learning for UAV Attitude Control
T2  - ACM TRANSACTIONS ON CYBER-PHYSICAL SYSTEMS
LA  - English
KW  - Attitude control
KW  - UAV
KW  - reinforcement learning
KW  - quadcopter
KW  - autopilot
KW  - machine learning
KW  - PID
KW  - intelligent control
KW  - adaptive control
KW  - QUADROTOR
AB  - Autopilot systems are typically composed of an "inner loop" providing stability and control, whereas an "outer loop" is responsible for mission-level objectives, such as way-point navigation. Autopilot systems for unmanned aerial vehicles are predominately implemented using Proportional-Integral-Derivative (PID) control systems, which have demonstrated exceptional performance in stable environments. However, more sophisticated control is required to operate in unpredictable and harsh environments. Intelligent flight control systems is an active area of research addressing limitations of PID control most recently through the use of reinforcement learning (RL), which has had success in other applications, such as robotics. Yet previous work has focused primarily on using RL at the mission-level controller. In this work, we investigate the performance and accuracy of the inner control loop providing attitude control when using intelligent flight control systems trained with state-of-the-art RL algorithms-Deep Deterministic Policy Gradient, Trust Region Policy Optimization, and Proximal Policy Optimization. To investigate these unknowns, we first developed an open source high-fidelity simulation environment to train a flight controller attitude control of a quadrotor through RL. We then used our environment to compare their performance to that of a PID controller to identify if using RL is appropriate in high-precision, time-critical flight control.
AD  - Boston Univ, Dept Comp Sci, 111 Cummington Mall, Boston, MA 02215 USAC3  - Boston UniversityFU  - National Science Foundation [1430145, 1414119, 1718135]; Direct For Computer & Info Scie & Enginr; Division Of Computer and Network Systems [1718135, 1414119] Funding Source: National Science Foundation; Directorate For Engineering; Div Of Industrial Innovation & Partnersh [1430145] Funding Source: National Science Foundation
FX  - This work was partially supported by a grant from the National Science Foundation under awards #1430145, #1414119, and #1718135.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN  - 2378-962X
SN  - 2378-9638
J9  - ACM TRANS CYBER-PHYS
JI  - ACM Trans. Cyber-Phys. Syst.
DA  - MAR
PY  - 2019
VL  - 3
IS  - 2
C7  - 22
DO  - 10.1145/3301273
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000465436900010
N1  - Times Cited in Web of Science Core Collection:  177
Total Times Cited:  201
Cited Reference Count:  40
ER  -

TY  - CPAPER
AU  - Zhu, MH
AU  - Xia, W
AU  - Liu, WW
AU  - Liu, YF
AU  - Tang, RM
AU  - Zhang, WN
A1  - ACM
TI  - Integrated Ranking for News Feed with Reinforcement Learning
T2  - COMPANION OF THE WORLD WIDE WEB CONFERENCE, WWW 2023
LA  - English
CP  - 32nd World Wide Web Conference (WWW)
KW  - Recommender system
KW  - integrated ranking
KW  - reinforcement learning
KW  - SEARCH
AB  - With the development of recommender systems, it becomes an increasingly common need to mix multiple item sequences from different sources. Therefore, the integrated ranking stage is proposed to be responsible for this task with re-ranking models. However, existing methods ignore the relation between the sequences, thus resulting in local optimum over the interaction session. To resolve this challenge, in this paper, we propose a new model named NFI-Rank (News Feed Integrated Ranking with reinforcement learning) and formulate the whole interaction session as a MDP (Markov Decision Process). Sufcient ofine experiments are provided to verify the efectiveness of our model. In addition, we deployed our model on Huawei Browser and gained 1.58% improvements in CTR compared with the baseline in online A/B test. Code will be available at https://gitee.com/mindspore/models/tree/master/research/ recommend/NFIRank.
AD  - Shanghai Jiao Tong Univ, Shanghai, Peoples R ChinaAD  - Huawei Noahs Ark Lab, Shenzhen, Peoples R ChinaC3  - Shanghai Jiao Tong UniversityC3  - Huawei TechnologiesPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-9416-1
PY  - 2023
SP  - 480
EP  - 484
DO  - 10.1145/3543873.3584651
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001124276300105
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  13
ER  -

TY  - JOUR
AU  - Felizardo, LK
AU  - Paiva, FCL
AU  - Graves, CD
AU  - Matsumoto, EY
AU  - Costa, AHR
AU  - Del-Moral-Hernandez, E
AU  - Brandimarte, P
TI  - Outperforming algorithmic trading reinforcement learning systems: A supervised approach to the cryptocurrency market
T2  - EXPERT SYSTEMS WITH APPLICATIONS
LA  - English
KW  - Deep neural network
KW  - Reinforcement learning
KW  - Stock trading
KW  - Time series classification
KW  - Cryptocurrencies
KW  - NEURAL-NETWORK
KW  - PERFORMANCE
KW  - PREDICTION
AB  - The interdisciplinary relationship between machine learning and financial markets has long been a theme of great interest among both research communities. Recently, reinforcement learning and deep learning methods gained prominence in the active asset trading task, aiming to achieve outstanding performances compared with classical benchmarks, such as the Buy and Hold strategy. This paper explores both the supervised learning and reinforcement learning approaches applied to active asset trading, drawing attention to the benefits of both approaches. This work extends the comparison between the supervised approach and reinforcement learning by using state-of-the-art strategies with both techniques. We propose adopting the ResNet architecture, one of the best deep learning approaches for time series classification, into the ResNet-LSTM actor (RSLSTM-A). We compare RSLSTM-A against classical and recent reinforcement learning techniques, such as recurrent reinforcement learning, deep Q-network, and advantage actor-critic. We simulated a currency exchange market environment with the price time series of the Bitcoin, Litecoin, Ethereum, Monero, Nxt, and Dash cryptocurrencies to run our tests. We show that our approach achieves better overall performance, confirming that supervised learning can outperform reinforcement learning for trading. We also present a graphic representation of the features extracted from the ResNet neural network to identify which type of characteristics each residual block generates.
AD  - Univ Sao Paulo, Escola Politecn, Sao Paulo, BrazilAD  - Fdn Getulio Vargas, Escola Econ Sao Paulo, Sao Paulo, BrazilAD  - Politecn Torino, Dept Math Sci, Turin, ItalyC3  - Universidade de Sao PauloC3  - Polytechnic University of TurinFU  - Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES -Coordination for the Improvement of Higher Education Personnel), Brazil [001, 88882.333380/2019-01]; Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPq -Brazilian National Council for Scientific and Technological Development) [100085/2020-9, 310085/2020-9]; Itau Unibanco S.A. through the Programa de Bolsas Itau (PBI) of the Centro de Ciencia de Dados (C2D, EP-USP)
FX  - This research was financed in part by the Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES -Coordination for the Improvement of Higher Education Personnel, Finance Code 001, grant 88882.333380/2019-01), Brazil, and Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPq -Brazilian National Council for Scientific and Technological Development, grants 100085/2020-9 and 310085/2020-9), and by Itau Unibanco S.A. through the Programa de Bolsas Itau (PBI) of the Centro de Ciencia de Dados (C2D, EP-USP).
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0957-4174
SN  - 1873-6793
J9  - EXPERT SYST APPL
JI  - Expert Syst. Appl.
DA  - SEP 15
PY  - 2022
VL  - 202
C7  - 117259
DO  - 10.1016/j.eswa.2022.117259
C6  - APR 2022
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000879924500003
N1  - Times Cited in Web of Science Core Collection:  8
Total Times Cited:  8
Cited Reference Count:  53
ER  -

TY  - JOUR
AU  - Hashmi, A
AU  - Barukab, O
TI  - Dementia Classification Using Deep Reinforcement Learning for Early Diagnosis
T2  - APPLIED SCIENCES-BASEL
LA  - English
KW  - deep learning
KW  - reinforcement learning
KW  - dementia
KW  - Alzheimer
KW  - classification
KW  - magnetic Imaging resonance
KW  - DISEASE
AB  - Neurodegeneration and impaired neuronal transmission in the brain are at the root of Alzheimer's disease (AD) and dementia. As of yet, no successful treatments for dementia or Alzheimer's disease have indeed been found. Therefore, preventative measures such as early diagnosis are essential. This research aimed to evaluate the accuracy of the Open Access Series of Imaging Studies (OASIS) database for the purpose of identifying biomarkers of dementia using effective machine learning methods. In most parts of the world, AD is responsible for dementia. When the challenge level is high, it is nearly impossible to get anything done without assistance. This is increasing due to population growth and the diagnostic period. Two current approaches are the medical history and testing. The main challenge for dementia research is the imbalance of datasets and their impact on accuracy. A proposed system based on reinforcement learning and neural networks could generate and segment imbalanced classes. Making a precise diagnosis and taking into account dementia in all four stages will result in high-resolution sickness probability maps. It employs deep reinforcement learning to generate accurate and understandable representations of a person's dementia sickness risk. To avoid an imbalance, classes should be evenly represented in the samples. There is a significant class imbalance in the MRI image. The Deep Reinforcement System improved trial accuracy by 6%, precision by 9%, recall by 13%, and F-score by 9-10%. The diagnosis efficiency has improved as well.
AD  - King Abdulaziz Univ, Fac Comp & Informat Technol Rabigh FCITR, Dept Informat Syst, Jeddah 21911, Saudi ArabiaAD  - King Abdulaziz Univ, Fac Comp & Informat Technol Rabigh FCITR, Dept Informat Technol, Jeddah 21911, Saudi ArabiaC3  - King Abdulaziz UniversityC3  - King Abdulaziz UniversityFU  - Deanship of Scientific Research (DSR) at King Abdulaziz University, Jeddah [G: 434-830-1443]; DSR
FX  - This project was funded by the Deanship of Scientific Research (DSR) at King Abdulaziz University, Jeddah, under grant no (G: 434-830-1443). The authors, therefore, acknowledge with thanks DSR for technical and financial support.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2076-3417
J9  - APPL SCI-BASEL
JI  - Appl. Sci.-Basel
DA  - FEB
PY  - 2023
VL  - 13
IS  - 3
C7  - 1464
DO  - 10.3390/app13031464
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000933789000001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  39
ER  -

TY  - JOUR
AU  - Fehér, A
AU  - Aradi, S
AU  - Bécsi, T
TI  - Online Trajectory Planning with Reinforcement Learning for Pedestrian Avoidance
T2  - ELECTRONICS
LA  - English
KW  - advanced driver assistance systems
KW  - machine learning
KW  - motion planning
KW  - reinforcement learning
KW  - vehicle dynamics
KW  - COLLISION-AVOIDANCE
KW  - OBSTACLE AVOIDANCE
KW  - MODEL
KW  - TRACKING
AB  - Planning the optimal trajectory of emergency avoidance maneuvers for highly automated vehicles is a complex task with many challenges. The algorithm needs to decrease accident risk by reducing the severity and keeping the car in a controllable state. Optimal trajectory generation considering all aspects of vehicle and environment dynamics is numerically complex, especially if the object to be avoided is moving. This paper presents a hierarchical method for the avoidance of moving objects in an autonomous vehicle, where a reinforcement learning agent is responsible for local planning, while longitudinal and lateral control is performed by the low-level model-predictive controller and Stanley controllers. In the developed architecture, the agent is responsible for the optimization. It is trained in various scenarios to provide the necessary parameters for a polynomial-based path and a velocity profile in a neural network output. The vehicle performs only the first step of the trajectory, which is redesigned repeatedly by the planner based on the new state. In the training phase, the vehicle executes the entire trajectory via low-level controllers to determine the reward value, which realizes a prediction for the future. The agent receives feedback and can further improve its performance. Finally, the proposed framework was tested in a simulation environment and was also compared to human drivers' abilities.
AD  - Budapest Univ Technol & Econ, Fac Transportat Engn & Vehicle Engn, Dept Control Transportat & Vehicle Syst, Muegyet Rkp 3, H-1111 Budapest, HungaryC3  - Budapest University of Technology & EconomicsFU  - European Union [RRF-2.3.1-21-2022-00002]; Ministry of Innovation and Technology of Hungary from the National Research, Development and Innovation Fund [BME-NVA-02]
FX  - The research was supported by the European Union within the framework of the National Laboratory for Autonomous Systems (RRF-2.3.1-21-2022-00002). The research reported in this paper is part of project No. BME-NVA-02, implemented with the support provided by the Ministry of Innovation and Technology of Hungary from the National Research, Development and Innovation Fund, financed under the TKP2021 funding scheme.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2079-9292
J9  - ELECTRONICS-SWITZ
JI  - Electronics
DA  - AUG
PY  - 2022
VL  - 11
IS  - 15
C7  - 2346
DO  - 10.3390/electronics11152346
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000840165400001
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  27
ER  -

TY  - JOUR
AU  - Tsur, EE
AU  - Elkana, O
TI  - Intelligent Robotics in Pediatric Cooperative Neurorehabilitation: A Review
T2  - ROBOTICS
LA  - English
KW  - intelligent robotics
KW  - neurorehabilitation
KW  - artificial intelligence (AI)
KW  - pediatric neurorehabilitation
KW  - assistive robotics
KW  - personalized rehabilitation
KW  - cognitive training
KW  - social robotics
KW  - adaptive behavior
KW  - responsible AI
KW  - DEEP LEARNING FRAMEWORK
KW  - CEREBRAL-PALSY
KW  - REHABILITATION
KW  - CHILDREN
KW  - MOBILITY
KW  - AUTISM
KW  - CLASSIFICATION
KW  - EXOSKELETON
KW  - CHALLENGES
KW  - NETWORKS
AB  - The landscape of neurorehabilitation is undergoing a profound transformation with the integration of artificial intelligence (AI)-driven robotics. This review addresses the pressing need for advancements in pediatric neurorehabilitation and underscores the pivotal role of AI-driven robotics in addressing existing gaps. By leveraging AI technologies, robotic systems can transcend the limitations of preprogrammed guidelines and adapt to individual patient needs, thereby fostering patient-centric care. This review explores recent strides in social and diagnostic robotics, physical therapy, assistive robotics, smart interfaces, and cognitive training within the context of pediatric neurorehabilitation. Furthermore, it examines the impact of emerging AI techniques, including artificial emotional intelligence, interactive reinforcement learning, and natural language processing, on enhancing cooperative neurorehabilitation outcomes. Importantly, the review underscores the imperative of responsible AI deployment and emphasizes the significance of unbiased, explainable, and interpretable models in fostering adaptability and effectiveness in pediatric neurorehabilitation settings. In conclusion, this review provides a comprehensive overview of the evolving landscape of AI-driven robotics in pediatric neurorehabilitation and offers valuable insights for clinicians, researchers, and policymakers.
AD  - Open Univ Israel, Dept Math & Comp Sci, Neurobiomorph Engn Lab, IL-4353701 Raanana, IsraelAD  - Acad Coll Tel Aviv Yafo, Sch Behav Sci, IL-6818211 Tel Aviv Yaffo, IsraelC3  - Open University IsraelFU  - Open University of Israel
FX  - The authors would like to thank the students of the Neuro and Biomorphic Engineering Lab at the Open University of Israel for the fruitful discussions.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2218-6581
J9  - ROBOTICS
JI  - Robotics
DA  - MAR
PY  - 2024
VL  - 13
IS  - 3
C7  - 49
DO  - 10.3390/robotics13030049
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:001192688300001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  180
ER  -

TY  - CPAPER
AU  - Theodoropoulos, T
AU  - Makris, A
AU  - Korontanis, I
AU  - Tserpes, K
A1  - IEEE
TI  - GreenKube: Towards Greener Container Orchestration using Artificial Intelligence
T2  - 2023 IEEE INTERNATIONAL CONFERENCE ON SERVICE-ORIENTED SYSTEM ENGINEERING, SOSE
LA  - English
CP  - 17th IEEE International Congress on Intelligent and Service-Oriented Systems Engineering (CISOSE)
KW  - Green Computing
KW  - Kubernetes
KW  - Graph Neural Networks
KW  - Deep Reinforcement Learning
KW  - Edge Computing
KW  - Cloud Computing
KW  - and Orchestration
AB  - This paper introduces the GreenKube framework, which aims to reduce energy consumption while meeting Quality of Service (QoS) requirements through the use of various AI methodologies such as deep learning time-series forecasting, deep reinforcement learning, and graph neural networks. The paper also explores the limitations of contemporary container orchestration frameworks, including Kubernetes, and describes how GreenKube aims to progress beyond them. Additionally, the paper presents a prototype of the GreenKube framework that was evaluated in an extensive simulation using CloudSim Plus, and compares its performance to Kubernetes' Horizontal Pod Autoscaler. The results demonstrate that GreenKube outperforms Kubernetes in terms of latency and task execution time while requiring fewer computational resources.
AD  - Harokopio Univ Athens, Dept Informat & Telemat, Athens, GreeceC3  - Harokopio University AthensFU  - EU [101016509]
FX  - This project has received funding from the EU's Horizon 2020 program under Grant agreement No 101016509 (CHARITY). This paper reflects only the authors' view and the Commission is not responsible for any use that may be made of the information it contains.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2642-6587
SN  - 2640-8228
SN  - 979-8-3503-2239-2
J9  - IEEE INT SYMP SERV O
PY  - 2023
SP  - 135
EP  - 139
DO  - 10.1109/SOSE58276.2023.00023
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001084635000017
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  18
ER  -

TY  - JOUR
AU  - Ramadan, M
AU  - El-Kilany, A
AU  - Mokhtar, HMO
AU  - Sobh, I
TI  - RL_QOptimizer: A Reinforcement Learning Based Query Optimizer
T2  - IEEE ACCESS
LA  - English
KW  - Reinforcement learning
KW  - Costs
KW  - Query processing
KW  - Neural networks
KW  - Markov processes
KW  - Estimation
KW  - Data models
KW  - Join ordering problem
KW  - query execution plan and query optimization
AB  - With the current availability of massive datasets and scalability requirements, different systems are required to provide their users with the best performance possible in terms of speed. On the physical level, performance can be translated into queries' execution time in database management systems. Queries have to execute efficiently (i.e. in minimum time) to meet users' needs, which puts an excessive burden on the database management system (DBMS). In this paper, we mainly focus on enhancing the query optimizer, which is one of the main components in DBMS that is responsible for choosing the optimal query execution plan and consequently determines the query execution time. Inspired by recent research in reinforcement learning in different domains, this paper proposes A Deep Reinforcement Learning Based Query Optimizer (RL_QOptimizer), a new approach to find the best policy for join order in the query plan which depends solely on the reward system of reinforcement learning. The experimental results show that a notable advantage of the proposed approach against the existing query optimization model of PostgreSQL DBMS.
AD  - Cairo Univ, Fac Comp & Artificial Intelligence, Informat Syst Dept, Cairo 12613, EgyptAD  - Egypt Univ Informat, Fac Comp & Informat Sci, Cairo 11865, EgyptAD  - Valeo, Cairo 12577, EgyptC3  - Egyptian Knowledge Bank (EKB)C3  - Cairo UniversityPU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
J9  - IEEE ACCESS
JI  - IEEE Access
PY  - 2022
VL  - 10
SP  - 70502
EP  - 70515
DO  - 10.1109/ACCESS.2022.3187102
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000838420800001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  50
ER  -

TY  - JOUR
AU  - Moradi, MH
AU  - Brutsche, M
AU  - Wenig, M
AU  - Wagner, U
AU  - Koch, T
TI  - Marine route optimization using reinforcement learning approach to reduce fuel consumption and consequently minimize CO<sub>2</sub> emissions
T2  - OCEAN ENGINEERING
LA  - English
KW  - Marine
KW  - Route optimization
KW  - CO2 reduction
KW  - Reinforcement learning
KW  - Artificial intelligence
KW  - VOYAGE OPTIMIZATION
KW  - TRIM OPTIMIZATION
KW  - SPEED
KW  - SYSTEM
KW  - SHIPS
KW  - PATH
AB  - To meet the 2050 CO2 targets, the shipping industry which is responsible for about 3% of global CO2 emissions needs to be optimized in several aspects. Obviously, alternative fuels constitute the main measure in this respect. However, relatively high fuel prices in combination with increasing political and economic pressure may raise the need for more efficient ship operation. Ship route optimization can make an indispensable contribution to achieving this goal. In this sense, this paper applies an innovative approach for route optimization using Reinforcement Learning (RL). For this purpose, a generic ship model is first developed using Artificial Neural Networks (ANNs) to predict the fuel consumption of the ship. Moreover, various RL methods, namely Deep QNetwork (DQN), Deep Deterministic Policy Gradient (DDPG), and Proximal Policy Optimization (PPO) are applied. The application of RL enables continuous action space and simultaneous optimization of ship speed and heading. DDPG demonstrates the best results as an off-policy and policy gradient method which allows a continuous action space. For example, in the fuel consumption minimization scenario without time limitation, this method can achieve savings of 6.64%. For DQN as a method with discrete action space, this value is 1.07%.
AD  - Karlsruhe Inst Technol, Inst Internal Combust Engines, Rintheimer Querallee 2,Geb 70-03, D-76131 Karlsruhe, GermanyAD  - Winterthur Gas & Diesel Ltd, Winterthur, SwitzerlandC3  - Helmholtz AssociationC3  - Karlsruhe Institute of TechnologyFU  - Winterthur Gas & Diesel Ltd. (WinGD)
FX  - The authors would like to express their appreciation to Winterthur Gas & Diesel Ltd (WinGD) for their support of this project. The authors would like to acknowledge Mr. Theodor Lanzer for his valuable research work in this area. Also, the authors would like to thank Ms. Mitra Zabihigivi for her help in proofreading the paper.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0029-8018
SN  - 1873-5258
J9  - OCEAN ENG
JI  - Ocean Eng.
DA  - SEP 1
PY  - 2022
VL  - 259
C7  - 111882
DO  - 10.1016/j.oceaneng.2022.111882
C6  - JUL 2022
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000828004600003
N1  - Times Cited in Web of Science Core Collection:  11
Total Times Cited:  11
Cited Reference Count:  59
ER  -

TY  - JOUR
AU  - Ilic, N
AU  - Dasic, D
AU  - Vucetic, M
AU  - Makarov, A
AU  - Petrovic, R
TI  - Distributed web hacking by adaptive consensus-based reinforcement learning
T2  - ARTIFICIAL INTELLIGENCE
LA  - English
KW  - Distributed reinforcement learning
KW  - Multi-agent system
KW  - Adaptive consensus-based algorithm
KW  - Distributed Q-learning
KW  - Ethical web hacking
KW  - Penetration testing
KW  - Capture the flag
AB  - In this paper, we propose a novel adaptive consensus-based learning algorithm for automated and distributed web hacking. We aim to assist ethical hackers in conducting legitimate penetration testing and improving web security by identifying system vulnerabilities at an early stage. Ethical hacking is modeled as a capture-the-flag style task addressed within a distributed reinforcement learning framework. To achieve our goal, we employ interconnected intelligent agents that interact with their copies of the environment simultaneously to reach the target. They perform local information processing to optimize their policies and exchange information with neighboring agents. We propose a novel adaptive consensus scheme for inter-agent communications, which enables the agents to efficiently share network-wide information in a decentralized manner. The scheme dynamically adjusts its weights based on heuristics, involving both recency and frequency metrics of actions selected at a given state by an individual agent, similar to eligibility traces. We extensively analyze the convergence properties of our algorithm and introduce a new communication scheme design. We demonstrate that this design ensures the fastest convergence to the desired asymptotic values under the general setting of asymmetric communication topologies. Additionally, we provide a comprehensive review of the current state of the field and propose a web agent model with improved scalability compared to existing solutions. Numerical simulations are conducted to illustrate the key characteristics of our algorithm. The results demonstrate that it outperforms both non-cooperative and average consensus schemes. Moreover, our algorithm significantly reduces hacking times when compared to baseline algorithms that rely on more complex models. These findings offer valuable insights to system security administrators, enabling them to address identified shortcomings and vulnerabilities effectively.
AD  - Vlatacom Inst High Technol Ltd, Belgrade, SerbiaAD  - Coll Appl Tech Sci, Dept Informat Technol, Krusevac, SerbiaAD  - Singidunum Univ, Belgrade, SerbiaPU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0004-3702
SN  - 1872-7921
J9  - ARTIF INTELL-AMST
JI  - Artif. Intell.
DA  - JAN
PY  - 2024
VL  - 326
C7  - 104032
DO  - 10.1016/j.artint.2023.104032
C6  - NOV 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001112877400001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  87
ER  -

TY  - CPAPER
AU  - Xenou, K
AU  - Chalkiadakis, G
AU  - Afantenos, S
ED  - Slavkovik, M
TI  - Deep Reinforcement Learning in Strategic Board Game Environments
T2  - MULTI-AGENT SYSTEMS, EUMAS 2018
LA  - English
CP  - 16th European Conference on Multi-Agent Systems (EUMAS)
KW  - Deep Reinforcement Learning
KW  - Strategic board games
AB  - In this paper we propose a novel Deep Reinforcement Learning (DRL) algorithm that uses the concept of "action-dependent state features", and exploits it to approximate the Q-values locally, employing a deep neural network with parallel Long Short Term Memory (LSTM) components, each one responsible for computing an action-related Q-value. As such, all computations occur simultaneously, and there is no need to employ "target" networks and experience replay, which are techniques regularly used in the DRL literature. Moreover, our algorithm does not require previous training experiences, but trains itself online during game play. We tested our approach in the Settlers Of Catan multi-player strategic board game. Our results confirm the effectiveness of our approach, since it outperforms several competitors, including the state-of-the-art jSettler heuristic algorithm devised for this particular domain.
AD  - Tech Univ Crete, Sch Elect & Comp Engn, Khania, GreeceAD  - Univ Paul Sabatier, Inst Rech Informat Toulouse IRIT, Toulouse, FranceC3  - Technical University of CreteC3  - Universite de ToulouseC3  - Universite Federale Toulouse Midi-Pyrenees (ComUE)C3  - Universite Toulouse III - Paul SabatierC3  - Institut National Polytechnique de ToulouseC3  - Universite Toulouse 1 CapitoleC3  - Universite de Toulouse - Jean JauresC3  - Centre National de la Recherche Scientifique (CNRS)PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 2945-9133
SN  - 1611-3349
SN  - 978-3-030-14173-8
SN  - 978-3-030-14174-5
J9  - LECT NOTES ARTIF INT
PY  - 2019
VL  - 11450
SP  - 233
EP  - 248
DO  - 10.1007/978-3-030-14174-5_16
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001061428000016
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  6
Cited Reference Count:  28
ER  -

TY  - JOUR
AU  - Lv, P
AU  - Yu, QQ
AU  - Xu, BY
AU  - Li, CC
AU  - Zhou, B
AU  - Xu, ML
TI  - Emotional Contagion-Aware Deep Reinforcement Learning for Antagonistic Crowd Simulation
T2  - IEEE TRANSACTIONS ON AFFECTIVE COMPUTING
LA  - English
KW  - Crowd simulation
KW  - emotional contagion
KW  - antagonistic behavior
KW  - decision making
KW  - deep reinforcement learning
AB  - The antagonistic behavior in the crowd usually exacerbates the seriousness of the situation in sudden riots, where the antagonistic emotional contagion and behavioral decision making play very important roles. However, the complex mechanism of antagonistic emotion influencing decision making, especially in the environment of sudden confrontation, has not yet been explored very clearly. In this paper, we propose an Emotional contagion-aware Deep reinforcement learning model for Antagonistic Crowd Simulation (ACSED). First, we build a group emotional contagion module based on the improved Susceptible Infected Susceptible (SIS) infection disease model, and estimate the emotional state of the group at each time step during the simulation. Then, the tendency of crowd antagonistic action is estimated based on Deep Q Network (DQN), where the agent learns the action autonomously, and leverages the mean field theory to quickly calculate the influence of other surrounding individuals on the central one. Finally, the rationality of the predicted actions by DQN is further analyzed in combination with group emotion, and the final action of the agent is determined. The proposed method in this paper is verified through several experiments with different settings. We can conclude antagonistic emotions play a critical role in the decision making of the crowd through influencing the individual behavior in the riot scenario, where individual behaviors are primarily driven by emotions and goals, rather than common rules. The experiment results also prove that the antagonistic emotion has a vital impact on the group combat, and positive emotional states are more conducive to combat. Moreover, by comparing the simulation results with real scenes, the feasibility of our method is further confirmed, which can provide good reference to formulate battle plans and improve the win rate of righteous groups in a variety of situations.
AD  - Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450000, Henan, Peoples R ChinaC3  - Zhengzhou UniversityFU  - National Natural Science Foundation of China
FX  - No Statement Available
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1949-3045
J9  - IEEE T AFFECT COMPUT
JI  - IEEE Trans. Affect. Comput.
DA  - OCT 1
PY  - 2023
VL  - 14
IS  - 4
SP  - 2939
EP  - 2953
DO  - 10.1109/TAFFC.2022.3225037
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001124163900053
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  56
ER  -

TY  - JOUR
AU  - Mou, LC
AU  - Saha, S
AU  - Hua, YS
AU  - Bovolo, F
AU  - Bruzzone, L
AU  - Zhu, XX
TI  - Deep Reinforcement Learning for Band Selection in Hyperspectral Image Classification
T2  - IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING
LA  - English
KW  - Hyperspectral imaging
KW  - Task analysis
KW  - Reinforcement learning
KW  - Markov processes
KW  - Earth
KW  - Correlation
KW  - Feature extraction
KW  - Deep Q-network
KW  - deep reinforcement learning
KW  - hyperspectral band selection
KW  - hyperspectral image classification
KW  - neural network
KW  - unsupervised learning
KW  - CHANGE VECTOR ANALYSIS
KW  - SLOW FEATURE ANALYSIS
KW  - DIMENSIONALITY REDUCTION
KW  - NEURAL-NETWORKS
KW  - INFORMATION
KW  - FUSION
AB  - Band selection refers to the process of choosing the most relevant bands in a hyperspectral image. By selecting a limited number of optimal bands, we aim at speeding up model training, improving accuracy, or both. It reduces redundancy among spectral bands while trying to preserve the original information of the image. By now, many efforts have been made to develop unsupervised band selection approaches, of which the majorities are heuristic algorithms devised by trial and error. In this article, we are interested in training an intelligent agent that, given a hyperspectral image, is capable of automatically learning policy to select an optimal band subset without any hand-engineered reasoning. To this end, we frame the problem of unsupervised band selection as a Markov decision process, propose an effective method to parameterize it, and finally solve the problem by deep reinforcement learning. Once the agent is trained, it learns a band-selection policy that guides the agent to sequentially select bands by fully exploiting the hyperspectral image and previously picked bands. Furthermore, we propose two different reward schemes for the environment simulation of deep reinforcement learning and compare them in experiments. This, to the best of our knowledge, is the first study that explores a deep reinforcement learning model for hyperspectral image analysis, thus opening a new door for future research and showcasing the great potential of deep reinforcement learning in remote sensing applications. Extensive experiments are carried out on four hyperspectral data sets, and experimental results demonstrate the effectiveness of the proposed method. The code is publicly available.
AD  - German Aerosp Ctr DLR, Remote Sensing Technol Inst IMF, D-82234 Wessling, GermanyAD  - Tech Univ Munich TUM, Data Sci Earth Observat SiPEO, D-80333 Munich, GermanyAD  - Fdn Bruno Kessler, I-38123 Povo, ItalyAD  - Univ Trento, Dept Informat Engn & Comp Sci, I-38122 Trento, ItalyC3  - Helmholtz AssociationC3  - German Aerospace Centre (DLR)C3  - Technical University of MunichC3  - Fondazione Bruno KesslerC3  - University of TrentoFU  - European Research Council (ERC) through the European Union's Horizon 2020 Research And Innovation Programme [ERC2016-StG-714087]; Helmholtz Association through the Framework of Helmholtz AI [ZT-I-PF-5-01]; German Federal Ministry of Education and Research (BMBF) [01DD20001]
FX  - This work was supported in part by the European Research Council (ERC) through the European Union's Horizon 2020 Research And Innovation Programme under grant Agreement ERC2016-StG-714087 (So2Sat), in part by the Helmholtz Association through the Framework of Helmholtz AI under Grant ZT-I-PF-5-01 (Local Unit "Munich Unit @Aeronautics, Space and Transport (MASTr)" and Helmholtz Excellent Professorship "Data Science in Earth Observation-Big Data Fusion for Urban Research"), and in part by the German Federal Ministry of Education and Research (BMBF) in the framework of the International Future AI Lab "AI4EO-Artificial Intelligence for Earth Observation: Reasoning, Uncertainties, Ethics and Beyond" under Grant 01DD20001.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 0196-2892
SN  - 1558-0644
J9  - IEEE T GEOSCI REMOTE
JI  - IEEE Trans. Geosci. Remote Sensing
PY  - 2022
VL  - 60
C7  - 5504414
DO  - 10.1109/TGRS.2021.3067096
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000730619400031
N1  - Times Cited in Web of Science Core Collection:  42
Total Times Cited:  44
Cited Reference Count:  71
ER  -

TY  - JOUR
AU  - Daidone, M
AU  - Ferrantelli, S
AU  - Tuttolomondo, A
TI  - Machine learning applications in stroke medicine: advancements, challenges, and future prospectives
T2  - NEURAL REGENERATION RESEARCH
LA  - English
KW  - cerebrovascular disease
KW  - deep learning
KW  - machine learning
KW  - reinforcement learning
KW  - stroke
KW  - stroke therapy
KW  - supervised learning
KW  - unsupervised learning
KW  - LONG-TERM SURVIVAL
KW  - PREDICTION
KW  - SCALE
KW  - CARE
AB  - Stroke is a leading cause of disability and mortality worldwide, necessitating the development of advanced technologies to improve its diagnosis, treatment, and patient outcomes. In recent years, machine learning techniques have emerged as promising tools in stroke medicine, enabling efficient analysis of large-scale datasets and facilitating personalized and precision medicine approaches. This abstract provides a comprehensive overview of machine learning's applications, challenges, and future directions in stroke medicine. Recently introduced machine learning algorithms have been extensively employed in all the fields of stroke medicine. Machine learning models have demonstrated remarkable accuracy in imaging analysis, diagnosing stroke subtypes, risk stratifications, guiding medical treatment, and predicting patient prognosis. Despite the tremendous potential of machine learning in stroke medicine, several challenges must be addressed. These include the need for standardized and interoperable data collection, robust model validation and generalization, and the ethical considerations surrounding privacy and bias. In addition, integrating machine learning models into clinical workflows and establishing regulatory frameworks are critical for ensuring their widespread adoption and impact in routine stroke care. Machine learning promises to revolutionize stroke medicine by enabling precise diagnosis, tailored treatment selection, and improved prognostication. Continued research and collaboration among clinicians, researchers, and technologists are essential for overcoming challenges and realizing the full potential of machine learning in stroke care, ultimately leading to enhanced patient outcomes and quality of life. This review aims to summarize all the current implications of machine learning in stroke diagnosis, treatment, and prognostic evaluation. At the same time, another purpose of this paper is to explore all the future perspectives these techniques can provide in combating this disabling disease.
AD  - Univ Palermo, Internal Med & Stroke Care Ward, Dept Hlth Promot Mother & Child Care, Internal Med & Med Specialties, Palermo, ItalyAD  - Univ Palermo, Mol & Clin Med PhD Program, Palermo, ItalyC3  - University of PalermoC3  - University of PalermoPU  - WOLTERS KLUWER MEDKNOW PUBLICATIONS
PI  - MUMBAI
PA  - WOLTERS KLUWER INDIA PVT LTD , A-202, 2ND FLR, QUBE, C T S  NO 1498A-2 VILLAGE MAROL, ANDHERI EAST, MUMBAI, Maharashtra, INDIA
SN  - 1673-5374
SN  - 1876-7958
J9  - NEURAL REGEN RES
JI  - Neural Regen. Res.
DA  - APR
PY  - 2024
VL  - 19
IS  - 4
SP  - 769
EP  - 773
DO  - 10.4103/1673-5374.382228
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001064661300018
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  6
Cited Reference Count:  37
ER  -

TY  - JOUR
AU  - Zammit, M
AU  - Voulgari, I
AU  - Liapis, A
AU  - Yannakakis, GN
TI  - Learn to Machine Learn <i>via</i> Games in the Classroom
T2  - FRONTIERS IN EDUCATION
LA  - English
KW  - machine learning
KW  - artificial intelligence
KW  - serious games
KW  - educational games
KW  - game analytics
KW  - digital literacy
KW  - supervised learning
KW  - reinforcement learning
KW  - BENEFITS
AB  - Artificial Intelligence (AI) and Machine Learning (ML) algorithms are increasingly being adopted to create and filter online digital content viewed by audiences from diverse demographics. From an early age, children grow into habitual use of online services but are usually unaware of how such algorithms operate, or even of their presence. Design decisions and biases inherent in the ML algorithms or in the datasets they are trained on shape the everyday digital lives of present and future generations. It is therefore important to disseminate a general understanding of AI and ML, and the ethical concerns associated with their use. As a response, the digital game ArtBot was designed and developed to teach fundamental principles about AI and ML, and to promote critical thinking about their functionality and shortcomings in everyday digital life. The game is intended as a learning tool in primary and secondary school classrooms. To assess the effectiveness of the ArtBot game as a learning experience we collected data from over 2,000 players across different platforms focusing on the degree of usage, interface efficiency, learners' performance and user experience. The quantitative usage data collected within the game was complemented by over 160 survey responses from teachers and students during early pilots of ArtBot. The evaluation analysis performed in this paper gauges the usability and usefulness of the game, and identifies areas of the game design which need improvement.
AD  - Univ Malta, Inst Digital Games, Msida, MaltaC3  - University of MaltaPU  - FRONTIERS MEDIA SA
PI  - LAUSANNE
PA  - AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN  - 2504-284X
J9  - FRONT EDUC
JI  - Front. Educ.
DA  - JUN 17
PY  - 2022
VL  - 7
C7  - 913530
DO  - 10.3389/feduc.2022.913530
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000819956100001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  26
ER  -

TY  - CPAPER
AU  - Neufeld, EA
AU  - Bartocci, E
AU  - Ciabattoni, A
ED  - Aydogan, R
ED  - Criado, N
ED  - Lang, J
ED  - Sanchez-Anguix, V
ED  - Serramia, M
TI  - On Normative Reinforcement Learning via Safe Reinforcement Learning
T2  - PRIMA 2022: PRINCIPLES AND PRACTICE OF MULTI-AGENT SYSTEMS
LA  - English
CP  - 24th International Conference on Principles and Practice of Multi-Agent Systems (PRIMA)
AB  - Reinforcement learning (RL) has proven a successful technique for teaching autonomous agents goal-directed behaviour. As RL agents further integrate with our society, they must learn to comply with ethical, social, or legal norms. Defeasible deontic logics are natural formal frameworks to specify and reason about such norms in a transparent way. However, their effective and efficient integration in RL agents remains an open problem. On the other hand, linear temporal logic (LTL) has been successfully employed to synthesize RL policies satisfying, e.g., safety requirements. In this paper, we investigate the extent to which the established machinery for safe reinforcement learning can be leveraged for directing normative behaviour for RL agents. We analyze some of the difficulties that arise from attempting to represent norms with LTL, provide an algorithm for synthesizing LTL specifications from certain normative systems, and analyze its power and limits with a case study.
AD  - TU Wien, Vienna, AustriaC3  - Technische Universitat WienFU  - DC-RES run by the TU Wien's Faculty of Informatics; FH-Technikum Wien; WWTF [MA16-028]
FX  - This work was supported by the DC-RES run by the TU Wien's Faculty of Informatics and the FH-Technikum Wien and by the project WWTF MA16-028.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-21202-4
SN  - 978-3-031-21203-1
J9  - LECT NOTES ARTIF INT
PY  - 2023
VL  - 13753
SP  - 72
EP  - 89
DO  - 10.1007/978-3-031-21203-1_5
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000920727000005
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  32
ER  -

TY  - CPAPER
AU  - Liu, FG
AU  - Lai, CQ
AU  - Wang, LSB
ED  - Pimenidis, E
ED  - Angelov, P
ED  - Jayne, C
ED  - Papaleonidas, A
ED  - Aydin, M
TI  - Reinforcement Learning for the Pickup and Delivery Problem
T2  - ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2022, PT II
LA  - English
CP  - 31st International Conference on Artificial Neural Networks (ICANN)
KW  - Pickup and delivery problem
KW  - Reinforcement learning
KW  - Attention mechanism
KW  - ALGORITHM
AB  - The pickup and delivery problem (PDP) and its related variants are an important part in the field of urban logistics and distribution, and there are many heuristic algorithms to solve them. However, with the continuous expansion of logistics scale, these methods generally have the problem of too long calculation time. In order to solve this problem, we propose a reinforcement learning (RL) model based on the Advantage Actor-Critic, which regards PDP as a sequential decision problem. The actor based on the attention mechanism is responsible for generating routing strategies. The critic is designed to improve the solution quality during training. The model is trained using policy gradient. The experimental results show that compared with the heuristic algorithms and previous RL approach, the proposed model has obvious advantages in computational time, and it is also competitive in terms of solution quality.
AD  - South China Univ Technol, Guangzhou 510006, Peoples R ChinaAD  - Peng Cheng Lab, Shenzhen 518055, Peoples R ChinaC3  - South China University of TechnologyC3  - Peng Cheng LaboratoryFU  - Major Program of Guangdong Basic and Applied Research [2019B030302002]; Science and Technology Major Project of Guangzhou [202007030006]; Engineering and Technology Research Center of Guangdong Province for Logistics Supply Chain and Internet of Things [GDDST[2016]176]; Industrial Development Fund Project of Guangzhou [x2jsD8183470]
FX  - This work is supported by the Major Program of Guangdong Basic and Applied Research under Grant 2019B030302002, the Science and Technology Major Project of Guangzhou under Grant 202007030006, the Engineering and Technology Research Center of Guangdong Province for Logistics Supply Chain and Internet of Things under Grant GDDST[2016]176, the Industrial Development Fund Project of Guangzhou under Project x2jsD8183470.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-15931-2
SN  - 978-3-031-15930-5
J9  - LECT NOTES COMPUT SC
PY  - 2022
VL  - 13530
SP  - 87
EP  - 98
DO  - 10.1007/978-3-031-15931-2_8
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000866212300008
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  20
ER  -

TY  - CPAPER
AU  - Yu, Y
AU  - Chung, J
AU  - Yun, H
AU  - Hessel, J
AU  - Park, JS
AU  - Lu, X
AU  - Zellers, R
AU  - Ammanabrolu, P
AU  - Le Bras, R
AU  - Kim, G
AU  - Choi, Y
A1  - IEEE
TI  - Fusing Pre-trained Language Models with Multimodal Prompts through Reinforcement Learning
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
LA  - English
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
AB  - Language models are capable of commonsense reasoning: while domain-specific models can learn from explicit knowledge (e.g. commonsense graphs [6], ethical norms [25]), and larger models like GPT-3 [7] manifest broad commonsense reasoning capacity. Can their knowledge be extended to multimodal inputs such as images and audio without paired domain data? In this work, we propose ESPER (Extending Sensory PErception with Reinforcement learning) which enables text-only pretrained models to address multimodal tasks such as visual commonsense reasoning. Our key novelty is to use reinforcement learning to align multimodal inputs to language model generations without direct supervision: for example, our reward optimization relies only on cosine similarity derived from CLIP [52] and requires no additional paired (image, text) data. Experiments demonstrate that ESPER outperforms baselines and prior work on a variety of multimodal text generation tasks ranging from captioning to commonsense reasoning; these include a new benchmark we collect and release, the ESP dataset, which tasks models with generating the text of several different domains for each image. Our code and data are publicly released at https://github.com/JiwanChung/esper.
AD  - Allen Inst Artificial Intelligence, Seattle, WA USAAD  - OpenAI, Seattle, WA USAAD  - Yonsei Univ, Dept Artificial Intelligence, Seoul, South KoreaAD  - Seoul Natl Univ, Dept Comp Sci & Engn, Seoul, South KoreaAD  - Univ Washington, Paul G Allen Sch Comp Sci, Seattle, WA 98195 USAC3  - Yonsei UniversityC3  - Seoul National University (SNU)C3  - University of WashingtonC3  - University of Washington SeattleFU  - Allen Institute for AI; DARPA MCS program through NIWC Pacific [N66001-19-2-4031]; Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government(MSIT), Artificial Intelligence Graduate School Program, Yonsei University [2020-0-01361]
FX  - We express special thanks to the Mosaic team members and AI2 researchers for their constructive feedback. Also, we thank Jaekyeom Kim for the comments on RL techniques. This work was supported by the Allen Institute for AI and DARPA MCS program through NIWC Pacific (N66001-19-2-4031), the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT), Artificial Intelligence Graduate School Program, Yonsei University, under Grant 2020-0-01361, and Basic Science Research Program through the National Research Foundation of Korea (NRF) (2020R1A2B5B03095585) and Institute of Information & communications Technology Planning & Evaluation (IITP) grant (No.2019-0-01082, SW StarLab). We thank workers on MTurk for their contributions.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
J9  - PROC CVPR IEEE
PY  - 2023
SP  - 10845
EP  - 10856
DO  - 10.1109/CVPR52729.2023.01044
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001062522103015
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  82
ER  -

TY  - JOUR
AU  - Akrasi-Mensah, NK
AU  - Agbemenu, AS
AU  - Nunoo-Mensah, H
AU  - Tchao, ET
AU  - Ahmed, AR
AU  - Keelson, E
AU  - Sikora, A
AU  - Welte, D
AU  - Kponyo, JJ
TI  - Adaptive Storage Optimization Scheme for Blockchain-IIoT Applications Using Deep Reinforcement Learning
T2  - IEEE ACCESS
LA  - English
KW  - Blockchain
KW  - IIoT
KW  - reinforcement learning
KW  - scalability
KW  - storage efficiency
KW  - storage optimization. I. INTRODUCTION
KW  - INTERNET
KW  - THINGS
KW  - IOT
AB  - Blockchain-IIoT integration into industrial processes promises greater security, transparency, and traceability. However, this advancement faces significant storage and scalability issues with existing blockchain technologies. Each peer in the blockchain network maintains a full copy of the ledger which is updated through consensus. This full replication approach places a burden on the storage space of the peers and would quickly outstrip the storage capacity of resource-constrained IIoT devices. Various solutions utilizing compression, summarization or different storage schemes have been proposed in literature. The use of cloud resources for blockchain storage has been extensively studied in recent years. Nonetheless, block selection remains a substantial challenge associated with cloud resources and blockchain integration. This paper proposes a deep reinforcement learning (DRL) approach as an alternative to solving the block selection problem, which involves identifying the blocks to be transferred to the cloud. We propose a DRL approach to solve our problem by converting the multi-objective optimization of block selection into a Markov decision process (MDP). We design a simulated blockchain environment for training and testing our proposed DRL approach. We utilize two DRL algorithms, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO) to solve the block selection problem and analyze their performance gains. PPO and A2C achieve 47.8% and 42.9% storage reduction on the blockchain peer compared to the full replication approach of conventional blockchain systems. The slowest DRL algorithm, A2C, achieves a run-time 7.2 times shorter than the benchmark evolutionary algorithms used in earlier works, which validates the gains introduced by the DRL algorithms. The simulation results further show that our DRL algorithms provide an adaptive and dynamic solution to the time-sensitive blockchain-IIoT environment.
AD  - Kwame Nkrumah Univ Sci & Technol, Fac Elect & Comp Engn, Distributed IoT Platforms Privacy & Edge Intellige, Kumasi, GhanaAD  - Kwame Nkrumah Univ Sci & Technol, Dept Comp Engn, Kumasi, GhanaAD  - Kwame Nkrumah Univ Sci andTechnol, Fac Elect & Comp Engn, Responsible Artificial Intelligence Lab RAIL, Kumasi, GhanaAD  - Kwame Nkrumah Univ Sci & Technol, Dept Telecommun Engn, Kumasi, GhanaAD  - Offenburg Univ Appl Sci, Inst Reliable Embedded Syst & Commun Elect ivESK, D-77652 Offenburg, GermanyC3  - Kwame Nkrumah University Science & TechnologyC3  - Kwame Nkrumah University Science & TechnologyC3  - Kwame Nkrumah University Science & TechnologyC3  - Hochschule OffenburgFU  - German Federal Ministry of Research and Education (Bundesministerium fur Bildung und Forschung, BMBF); German Academic Exchange Service (Deutscher Akademischer Austauschdienst, DAAD); Distributed Internet of Things (IoT)-Platforms for Safe Food Production in Education, Research and Industry (DIPPER) Project; BMBF [01DG21017]; DAAD [57557211]; Responsible Artificial Intelligence Laboratory Project; International Development Research Centre (IDRC) [109832-001]; Deutsche Gesellschaft fur Internationale Zusammenarbeit (GIZ) [81280702]
FX  - This work was supported in part by the German Federal Ministry of Research and Education (Bundesministerium fur Bildung und Forschung, BMBF) and the German Academic Exchange Service (Deutscher Akademischer Austauschdienst, DAAD); in part by the Distributed Internet of Things (IoT)-Platforms for Safe Food Production in Education, Research and Industry (DIPPER) Project, which is co-financed by the BMBF (Forderkennzeichen: 01DG21017) and DAAD under Grant 57557211; and in part by the Responsible Artificial Intelligence Laboratory Project which is being co-financed by the International Development Research Centre (IDRC) under Grant 109832-001 and Deutsche Gesellschaft fur Internationale Zusammenarbeit (GIZ) under Agreement 81280702.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
J9  - IEEE ACCESS
JI  - IEEE Access
PY  - 2023
VL  - 11
SP  - 1372
EP  - 1385
DO  - 10.1109/ACCESS.2022.3233474
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000910529600001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  44
ER  -

TY  - CPAPER
AU  - Liang, SS
AU  - Yang, Z
AU  - Jin, F
AU  - Chen, Y
ED  - Lauw, HW
ED  - Wong, RCW
ED  - Ntoulas, A
ED  - Lim, EP
ED  - Ng, SK
ED  - Pan, SJ
TI  - Data Centers Job Scheduling with Deep Reinforcement Learning
T2  - ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING, PAKDD 2020, PT II
LA  - English
CP  - 24th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)
KW  - Job scheduling
KW  - Cluster scheduling
KW  - Deep reinforcement learning
KW  - Actor critic
AB  - Efficient job scheduling on data centers under heterogeneous complexity is crucial but challenging since it involves the allocation of multi-dimensional resources over time and space. To adapt the complex computing environment in data centers, we proposed an innovative Advantage Actor-Critic (A2C) deep reinforcement learning based approach called A2cScheduler for job scheduling. A2cScheduler consists of two agents, one of which, dubbed the actor, is responsible for learning the scheduling policy automatically and the other one, the critic, reduces the estimation error. Unlike previous policy gradient approaches, A2cScheduler is designed to reduce the gradient estimation variance and to update parameters efficiently. We show that the A2cScheduler can achieve competitive scheduling performance using both simulated workloads and real data collected from an academic data center.
AD  - Texas Tech Univ, Dept Comp Sci, Lubbock, TX 79409 USAC3  - Texas Tech University SystemC3  - Texas Tech UniversityFU  - National Science Foundation [CCF-1718336, CNS-1817094]
FX  - We are thankful to the anonymous reviewers for their valuable feedback. This research is supported in part by the National Science Foundation under grant CCF-1718336 and CNS-1817094.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-47436-2
SN  - 978-3-030-47435-5
J9  - LECT NOTES ARTIF INT
PY  - 2020
VL  - 12085
SP  - 906
EP  - 917
DO  - 10.1007/978-3-030-47436-2_68
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000716989100068
N1  - Times Cited in Web of Science Core Collection:  19
Total Times Cited:  19
Cited Reference Count:  18
ER  -

TY  - JOUR
AU  - Grech, L
AU  - Valentino, G
AU  - Alves, D
AU  - Hirlaender, S
TI  - Application of reinforcement learning in the LHC tune feedback
T2  - FRONTIERS IN PHYSICS
LA  - English
KW  - LHC
KW  - beam-based controller
KW  - tune feedback
KW  - reinforcement learning
KW  - cern
AB  - The Beam-Based Feedback System (BBFS) was primarily responsible for correcting the beam energy, orbit and tune in the CERN Large Hadron Collider (LHC). A major code renovation of the BBFS was planned and carried out during the LHC Long Shutdown 2 (LS2). This work consists of an explorative study to solve a beam-based control problem, the tune feedback (QFB), utilising state-of-the-art Reinforcement Learning (RL). A simulation environment was created to mimic the operation of the QFB. A series of RL agents were trained, and the best-performing agents were then subjected to a set of well-designed tests. The original feedback controller used in the QFB was reimplemented to compare the performance of the classical approach to the performance of selected RL agents in the test scenarios. Results from the simulated environment show that the RL agent performance can exceed the controller-based paradigm.
AD  - Univ Malta, Dept Commun & Comp Engn, Msida, MaltaAD  - CERN, Geneva, SwitzerlandAD  - Univ Salzburg, Dept Artificial Intelligence & Human Interfaces, Salzburg, AustriaC3  - University of MaltaC3  - European Organization for Nuclear Research (CERN)C3  - Salzburg UniversityFU  - Malta Council for Science and Technology; Foundation for Science and Technology
FX  - Project DeepREL financed by the Malta Council for Science and Technology, for and on behalf of the Foundation for Science and Technology, through the FUSION: R&I Research Excellence Programme.
PU  - FRONTIERS MEDIA SA
PI  - LAUSANNE
PA  - AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN  - 2296-424X
J9  - FRONT PHYS-LAUSANNE
JI  - Front. Physics
DA  - SEP 7
PY  - 2022
VL  - 10
C7  - 929064
DO  - 10.3389/fphy.2022.929064
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000856530000001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  23
ER  -

TY  - CPAPER
AU  - Cheang, RM
AU  - Brandao, AAF
AU  - Sichman, JS
ED  - Ajmeri, N
ED  - Martin, AM
ED  - Savarimuthu, BTR
TI  - Centralized Norm Enforcement in Mixed-Motive Multiagent Reinforcement Learning
T2  - COORDINATION, ORGANIZATIONS, INSTITUTIONS, NORMS, AND ETHICS FOR GOVERNANCE OF MULTI-AGENT SYSTEMS XV
LA  - English
CP  - International Workshop on Coordination, Organizations, Institutions, and Norms for Governance of Multi-Agent Systems (COINE)
KW  - Mixed-motive games
KW  - Centralized norm enforcement
KW  - Multiagent reinforcement learning
AB  - Mixed-motive games comprise a subset of games in which individual and collective incentives are not entirely aligned. These games are relevant because they frequently occur in real-world and artificial societies, and their outcome is often bad for the involved parties. Institutions and norms offer a good solution for governing mixed-motive systems. Still, they are usually incorporated into the system in a distributed fashion, or they are not able to dynamically adjust to the needs of the environment at run-time. We propose a way of reaching socially good outcomes in mixed-motive multiagent reinforcement learning settings by enhancing the environment with a normative system controlled by an external reinforcement learning agent. By adopting this proposal, we show it is possible to reach social welfare in a mixed-motive system of self-interested agents using only traditional reinforcement learning agent architectures.
AD  - Univ Sao Paulo, LTI, Sao Paulo, BrazilAD  - Univ Sao Paulo, C2D, Sao Paulo, BrazilC3  - Universidade de Sao PauloC3  - Universidade de Sao PauloFU  - Itau Unibanco S.A., through the scholarship program of Programa de Bolsas Itau (PBI); Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES), Brazil [001]
FX  - This research is being carried out with the support of Itau Unibanco S.A., through the scholarship program of Programa de Bolsas Itau (PBI), and it is also financed in part by the Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES), Finance Code 001, Brazil.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-20844-7
SN  - 978-3-031-20845-4
J9  - LECT NOTES ARTIF INT
PY  - 2022
VL  - 13549
SP  - 121
EP  - 133
DO  - 10.1007/978-3-031-20845-4_8
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:000896507600008
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  33
ER  -

TY  - JOUR
AU  - Ostberg, NP
AU  - Zafar, MA
AU  - Elefteriades, JA
TI  - Machine learning: principles and applications for thoracic surgery
T2  - EUROPEAN JOURNAL OF CARDIO-THORACIC SURGERY
LA  - English
KW  - Machine learning
KW  - Supervised learning
KW  - Deep learning
KW  - Predictive models
KW  - Prognostication
KW  - HEALTH-CARE
KW  - DISSECTION
AB  - OBJECTIVES: Machine learning (ML) has experienced a revolutionary decade with advances across many disciplines. We seek to understand how recent advances in ML are going to specifically influence the practice of surgery in the future with a particular focus on thoracic surgery.
   METHODS: Review of relevant literature in both technical and clinical domains.
   RESULTS: ML is a revolutionary technology that promises to change the way that surgery is practiced in the near future. Spurred by an advance in computing power and the volume of data produced in healthcare, ML has shown remarkable ability to master tasks that had once been reserved for physicians. Supervised learning, unsupervised learning and reinforcement learning are all important techniques that can be leveraged to improve care. Five key applications of ML to cardiac surgery include diagnostics, surgical skill assessment, postoperative prognostication, augmenting intraoperative performance and accelerating translational research. Some key limitations of ML include lack of interpretability, low quality and volumes of relevant clinical data, ethical limitations and difficulties with clinical implementation.
   CONCLUSIONS: In the future, the practice of cardiac surgery will be greatly augmented by ML technologies, ultimately leading to improved surgical performance and better patient outcomes.
AD  - Yale Univ, Yale New Haven Hosp, Sch Med, Aort Inst, New Haven, CT 06519 USAAD  - NYU, Grossman Sch Med, New York, NY USAC3  - Yale UniversityC3  - New York UniversityPU  - OXFORD UNIV PRESS INC
PI  - CARY
PA  - JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN  - 1010-7940
SN  - 1873-734X
J9  - EUR J CARDIO-THORAC
JI  - Eur. J. Cardio-Thorac. Surg.
DA  - AUG
PY  - 2021
VL  - 60
IS  - 2
SP  - 213
EP  - 221
DO  - 10.1093/ejcts/ezab095
C6  - MAR 2021
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000732612500002
N1  - Times Cited in Web of Science Core Collection:  19
Total Times Cited:  19
Cited Reference Count:  42
ER  -

TY  - JOUR
AU  - Cao, JY
AU  - Chen, JCY
AU  - Farghadani, S
AU  - Hull, J
AU  - Poulos, Z
AU  - Wang, ZY
AU  - Yuan, J
TI  - Gamma and vega hedging using deep distributional reinforcement learning
T2  - FRONTIERS IN ARTIFICIAL INTELLIGENCE
LA  - English
KW  - derivatives
KW  - hedging
KW  - gamma
KW  - vega
KW  - delta-neutral
KW  - reinforcement learning
KW  - D4PG
KW  - quantile regression
AB  - We show how reinforcement learning can be used in conjunction with quantile regression to develop a hedging strategy for a trader responsible for derivatives that arrive stochastically and depend on a single underlying asset. We assume that the trader makes the portfolio delta-neutral at the end of each day by taking a position in the underlying asset. We focus on how trades in options can be used to manage gamma and vega. The option trades are subject to transaction costs. We consider three different objective functions. We reach conclusions on how the optimal hedging strategy depends on the trader's objective function, the level of transaction costs, and the maturity of the options used for hedging. We also investigate the robustness of the hedging strategy to the process assumed for the underlying asset.
AD  - Univ Toronto, Joseph L Rotman Sch Management, Toronto, ON, CanadaAD  - Univ Toronto, Dept Comp Sci, Toronto, ON, CanadaC3  - University of TorontoC3  - University of TorontoFU  - Financial Innovation Hub (FinHub); TD Bank; Global Risk Institute in Financial Services; Rotman Catalyst Fund; Royal Bank of Canada; Bank of Canada; University of Toronto's Data Science Institute
FX  - This research was undertaken by the Financial Innovation Hub (FinHub), a research center at the Joseph L. Rotman School of Management, University of Toronto. FinHub was funded by the TD Bank, the Global Risk Institute in Financial Services, the Rotman Catalyst Fund, the Royal Bank of Canada, the Bank of Canada, and the University of Toronto's Data Science Institute.
PU  - FRONTIERS MEDIA SA
PI  - LAUSANNE
PA  - AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN  - 2624-8212
J9  - FRONT ARTIF INTELL
JI  - Front. Artif. Intell.
DA  - FEB 22
PY  - 2023
VL  - 6
C7  - 1129370
DO  - 10.3389/frai.2023.1129370
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000945372500001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  12
ER  -

TY  - JOUR
AU  - Munguba, CFD
AU  - Leite, GDP
AU  - Ochoa, AAV
AU  - Droguett, EL
TI  - Condition-based maintenance with reinforcement learning for refrigeration systems with selected monitored features
T2  - ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE
LA  - English
KW  - Refrigeration
KW  - Degradation
KW  - Energy
KW  - Condition-based maintenance
KW  - Reinforcement learning
AB  - Worldwide, buildings are responsible for almost 30% of energy consumption, and those buildings that intensively use refrigeration systems, such as supermarkets and grocery stores, are also among the most energy -intensive consumers. Refrigeration devices, either commercial or residential, are responsible for a significant part of net emissions. Based on careful measurements, it is possible to reduce energy consumption in these devices by up to 15% only by improving the fault detection and diagnosis techniques. Thus, improving maintenance programs has become a crucial area in energy management in recent years. Nowadays, the market has experienced a hike after smart systems and new network interfaces applied to smart buildings that have allowed previously isolated devices to become smart devices, interacting with control algorithms smartly and, to some extent, autonomously. Here, we propose a reinforcement learning framework to develop a maintenance policy for mechanical compression refrigeration devices. Firstly, a test bench is built in which each component is assigned to be individually repairable and individually degradable in parallel and interconnected processes. Then, the degradation of the components is combined to formulate the system degradation, and the optimal maintenance policy is modeled via Markov decision processes and solved by a reinforcement learning algorithm. The agent-proposed maintenance program if compared to corrective maintenance, managed to reduce energy use and emissions by around 6% while avoiding shortfalls, as well as about the preventive program, where the agent managed to accomplish the same level of energy efficiency while reducing the maintenance costs by 31% and the time under maintenance in 10%. It was found that the reinforcement learning frameworks applied to maintenance have a series of challenges but are innovative and can show promising results compared to traditional maintenance techniques, such as preventive and corrective ones.
AD  - Univ Fed Pernambuco, Mech Engn Dept PPGEM, UFPE, Recife, BrazilAD  - Fed Inst Educ Sci & Technol Pernambuco, IFPE, DACI, CACTR, Campus Recife, Recife, BrazilAD  - Univ Calif Los Angeles, Garrick Inst Risk Sci, Los Angeles, CA 90095 USAAD  - Univ Calif Los Angeles, Dept Civil & Environm Engn, Los Angeles, CA 90095 USAAD  - IFPE Recife, Recife, BrazilC3  - Universidade Federal de PernambucoC3  - Instituto Federal de Pernambuco (IFPE)C3  - University of California SystemC3  - University of California Los AngelesC3  - University of California SystemC3  - University of California Los AngelesFU  - CAPEs; IFPE [10/2019/Propesq]; CNPq [309151-2019-7, 303417/2022-6]
FX  - The first author thanks the CAPEs for the scholarship of the masters degree and the PPGEM/UFPE. The second and the third authors thank IFPE for its financial support throughout the Call 10/2019/Propesq. The third author thanks the CNPq for the scholarships of Productivity no309151-2019-7 and 303417/2022-6.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0952-1976
SN  - 1873-6769
J9  - ENG APPL ARTIF INTEL
JI  - Eng. Appl. Artif. Intell.
DA  - JUN
PY  - 2023
VL  - 122
C7  - 106067
DO  - 10.1016/j.engappai.2023.106067
C6  - MAR 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000955035300001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  104
ER  -

TY  - CPAPER
AU  - Alexopoulos, K
AU  - Nikolakis, N
AU  - Bakopoulos, E
AU  - Siatras, V
AU  - Mavrothalassitis, P
TI  - Machine Learning Agents Augmented by Digital Twinning for Smart Production Scheduling
T2  - IFAC PAPERSONLINE
LA  - English
CP  - 22nd World Congress of the International Federation of Automatic Control (IFAC)
KW  - Machine Learning
KW  - Digital Twin
KW  - Production Scheduling
KW  - Deep Learning
KW  - Asset
KW  - Administration Shell
KW  - Smart Agents
KW  - Industry 4.0
KW  - ARTIFICIAL-INTELLIGENCE
KW  - CHALLENGES
AB  - Digital manufacturing tools aim to provide intelligent solutions that can support manufacturing industry to adapt to the volatile operational environment. The successful implementation of such tools highly depends on the capabilities of the digital frameworks or platforms they are deployed upon as well as the quality of their intelligence. The objective of this work is to develop and discuss a framework for training and deploying Machine Learning (ML) agents for production scheduling with the augmentation of Digital Twin (DT) technologies. Two types of ML production scheduling agents have been developed and integrated with the DT framework: a Deep Learning agent and a Deep Reinforcement Learning agent. In order to increase interoperability, Asset Administration Shell Industry4.0 standard has been utilized for the integration and deployment of the proposed DT framework into industrial practice. The proposed framework is tested and validated upon an industrial case study from the bicycles' production industry.
AD  - Univ Patras, Dept Mech Engn & Aeronaut, Lab Mfg Syst & Automat LMS, Rion, GreeceC3  - University of PatrasFU  - European Union [957204 MAS4AI]
FX  - This project has received funding from the European Union's Horizon 2020 research and innovation program under grant agreement No 957204 MAS4AI. The dissemination of results herein reflects only the authors' view, and the Commission is not responsible for any use that may be made of the information it contains.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 2405-8963
J9  - IFAC PAPERSONLINE
JI  - IFAC PAPERSONLINE
PY  - 2023
VL  - 56
IS  - 2
SP  - 2963
EP  - 2968
DO  - 10.1016/j.ifacol.2023.10.1420
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001196708400473
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  29
ER  -

TY  - JOUR
AU  - Ugurlu, HI
AU  - Pham, XH
AU  - Kayacan, E
TI  - Sim-to-Real Deep Reinforcement Learning for Safe End-to-End Planning of Aerial Robots
T2  - ROBOTICS
LA  - English
KW  - deep reinforcement learning
KW  - obstacle avoidance
KW  - quadrotors
KW  - sim-to-real transfer
AB  - In this study, a novel end-to-end path planning algorithm based on deep reinforcement learning is proposed for aerial robots deployed in dense environments. The learning agent finds an obstacle-free way around the provided rough, global path by only depending on the observations from a forward-facing depth camera. A novel deep reinforcement learning framework is proposed to train the end-to-end policy with the capability of safely avoiding obstacles. The Webots open-source robot simulator is utilized for training the policy, introducing highly randomized environmental configurations for better generalization. The training is performed without dynamics calculations through randomized position updates to minimize the amount of data processed. The trained policy is first comprehensively evaluated in simulations involving physical dynamics and software-in-the-loop flight control. The proposed method is proven to have a 38% and 50% higher success rate compared to both deep reinforcement learning-based and artificial potential field-based baselines, respectively. The generalization capability of the method is verified in simulation-to-real transfer without further training. Real-time experiments are conducted with several trials in two different scenarios, showing a 50% higher success rate of the proposed method compared to the deep reinforcement learning-based baseline.
AD  - Aarhus Univ, Dept Elect & Comp Engn, Artificial Intelligence Robot Lab AiR Lab, DK-8000 Aarhus C, DenmarkC3  - Aarhus UniversityFU  - European Union [871449]
FX  - This research was funded by the European Union's Horizon 2020 Research and Innovation Program (OpenDR) grant number 871449. This publication reflects the authors' views only. The European Commission is not responsible for any use that may be made of the information it contains.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2218-6581
J9  - ROBOTICS
JI  - Robotics
DA  - OCT
PY  - 2022
VL  - 11
IS  - 5
C7  - 109
DO  - 10.3390/robotics11050109
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000873677300001
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  44
ER  -

TY  - JOUR
AU  - O'Reilly, C
AU  - Oruganti, SDR
AU  - Tilwani, D
AU  - Bradshaw, J
TI  - Model-Driven Analysis of ECG Using Reinforcement Learning
T2  - BIOENGINEERING-BASEL
LA  - English
KW  - ECG
KW  - modeling
KW  - reinforcement learning
KW  - lognormal
KW  - autonomic nervous system
KW  - model-driven analysis
AB  - Modeling is essential to understand better the generative mechanisms responsible for experimental observations gathered from complex systems. In this work, we are using such an approach to analyze the electrocardiogram (ECG). We present a systematic framework to decompose ECG signals into sums of overlapping lognormal components. We used reinforcement learning to train a deep neural network to estimate the modeling parameters from ECG recorded in babies of 1 to 24 months of age. We demonstrate this model-driven approach by showing how the extracted parameters vary with age. After correction for multiple tests, 10 of 24 modeling parameters showed statistical significance below the 0.01 threshold, with absolute Kendall rank correlation coefficients in the [0.27, 0.51] range. We presented a model-driven approach to the analysis of ECG. The impact of this framework on fundamental science and clinical applications is likely to be increased by further refining the modeling of the physiological mechanisms generating the ECG. By improving the physiological interpretability, this approach can provide a window into latent variables important for understanding the heart-beating process and its control by the autonomous nervous system.
AD  - Artificial Intelligence Inst South Carolina, Columbia, SC 29208 USAAD  - Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USAAD  - Univ South Carolina, Carolina Autism & Neurodev Res Ctr, Columbia, SC 29208 USAAD  - Univ South Carolina, Inst Mind & Brain, Columbia, SC 29208 USAAD  - Univ South Carolina, Dept Psychol, Columbia, SC 29208 USAC3  - University of South Carolina SystemC3  - University of South Carolina ColumbiaC3  - University of South Carolina SystemC3  - University of South Carolina ColumbiaC3  - University of South Carolina SystemC3  - University of South Carolina ColumbiaC3  - University of South Carolina SystemC3  - University of South Carolina ColumbiaFU  - Carolina Autism amp; Neurodevelopment Center at the University of South Carolina; National Institute of Mental Health grant [K23MH120476]; National Institute on Deafness and Other Communication Disorders grant [DC017252]
FX  - This work was supported by a pilot grant from the Carolina Autism & Neurodevelopment Center at the University of South Carolina (PI: C.O'R.). The ECG was collected under a National Institute of Mental Health grant (PI: J.B.; No: K23MH120476) and a National Institute on Deafness and Other Communication Disorders grant (PI: J.B., No: DC017252).
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2306-5354
J9  - BIOENGINEERING-BASEL
JI  - Bioengineering-Basel
DA  - JUN
PY  - 2023
VL  - 10
IS  - 6
C7  - 696
DO  - 10.3390/bioengineering10060696
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001016949500001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  28
ER  -

TY  - JOUR
AU  - Tortorelli, A
AU  - Imran, M
AU  - Delli Priscoli, F
AU  - Liberati, F
TI  - A Parallel Deep Reinforcement Learning Framework for Controlling Industrial Assembly Lines
T2  - ELECTRONICS
LA  - English
KW  - deep reinforcement learning
KW  - parallel training
KW  - Industry 4
KW  - 0
AB  - Decision-making in a complex, dynamic, interconnected, and data-intensive industrial environment can be improved with the assistance of machine-learning techniques. In this work, a complex instance of industrial assembly line control is formalized and a parallel deep reinforcement learning approach is presented. We consider an assembly line control problem in which a set of tasks (e.g., vehicle assembly tasks) needs to be planned and controlled during their execution, with the aim of optimizing given key performance criteria. Specifically, the aim will be that of planning the task in order to minimize the total time taken to execute all the tasks (also called cycle time). Tasks run on workstations in the assembly line. To run, tasks need specific resources. Therefore, the tackled problem is that of optimally mapping tasks and resources to workstations, and deciding the optimal execution times of the tasks. In doing so, several constraints need to be respected (e.g., precedence constraints among the tasks, constraints on needed resources to run tasks, deadlines, etc.). The proposed approach uses deep reinforcement learning to learn a tasks/resources mapping policy that is effective in minimizing the resulting cycle time. The proposed method allows us to explicitly take into account all the constraints, and, once training is complete, can be used in real time to dynamically control the execution of tasks. Another motivation for the proposed work is in the ability of the used method to also work in complex scenarios, and in the presence of uncertainties. As a matter of fact, the use of deep neural networks allows for learning the model of the assembly line problem, in contrast with, e.g., optimization-based techniques, which require explicitly writing all the equations of the model of the problem. In order to speed up the training phase, we adopt a learning scheme in which more agents are trained in parallel. Simulations show that the proposed method can provide effective real-time decision support to industrial operators for scheduling and rescheduling activities, achieving the goal of minimizing the total tasks' execution time.
AD  - Univ Roma La Sapienza, Dept Comp Control & Management Engn DIAG, Via Ariosto 25, I-00185 Rome, ItalyAD  - Consorzio Ric Automat & Nelle Telecomunicaz CRAT, Via Giovanni Nicotera 29, I-00185 Rome, ItalyC3  - Sapienza University RomeFU  - European Union [821875]; H2020 - Industrial Leadership [821875] Funding Source: H2020 - Industrial Leadership
FX  - This work has been carried out in the framework of the SESAME project, which has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 821875. The content of this paper reflects only the author's view; the EU Commission/Agency is not responsible for any use that may be made of the information it contains.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2079-9292
J9  - ELECTRONICS-SWITZ
JI  - Electronics
DA  - FEB
PY  - 2022
VL  - 11
IS  - 4
C7  - 539
DO  - 10.3390/electronics11040539
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000763417300001
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  4
Cited Reference Count:  31
ER  -

TY  - JOUR
AU  - Magee, L
AU  - Arora, V
AU  - Munn, L
TI  - Structured like a language model: Analysing AI as an automated subject
T2  - BIG DATA & SOCIETY
LA  - English
KW  - AI
KW  - psychoanalysis
KW  - automated subjects
KW  - large language models
KW  - reinforcement learning from human feedback (RLHF)
KW  - chatbot interviews
KW  - SOCIAL CONSTRUCTION
AB  - Drawing from the resources of psychoanalysis and critical media studies, in this article we develop an analysis of large language models (LLMs) as 'automated subjects'. We argue the intentional fictional projection of subjectivity onto LLMs can yield an alternate frame through which artificial intelligence (AI) behaviour, including its productions of bias and harm, can be analysed. First, we introduce language models, discuss their significance and risks, and outline our case for interpreting model design and outputs with support from psychoanalytic concepts. We trace a brief history of language models, culminating with the releases, in 2022, of systems that realise 'state-of-the-art' natural language processing performance. We engage with one such system, OpenAI's InstructGPT, as a case study, detailing the layers of its construction and conducting exploratory and semi-structured interviews with chatbots. These interviews probe the model's moral imperatives to be 'helpful', 'truthful' and 'harmless' by design. The model acts, we argue, as the condensation of often competing social desires, articulated through the internet and harvested into training data, which must then be regulated and repressed. This foundational structure can however be redirected via prompting, so that the model comes to identify with, and transfer, its commitments to the immediate human subject before it. In turn, these automated productions of language can lead to the human subject projecting agency upon the model, effecting occasionally further forms of countertransference. We conclude that critical media methods and psychoanalytic theory together offer a productive frame for grasping the powerful new capacities of AI-driven language systems.
AD  - Western Sydney Univ, Inst Culture & Soc, Sydney, AustraliaAD  - Univ Stirling, Hist Heritage & Polit, Stirling, ScotlandAD  - Univ Queensland, Digital Cultures & Soc, Brisbane, AustraliaAD  - Western Sydney Univ, Inst Culture & Soc, Locked Bag 1797, Penrith, NSW 2751, AustraliaC3  - Western Sydney UniversityC3  - University of StirlingC3  - University of QueenslandC3  - Western Sydney UniversityFU  - Australian Research Council
FX  - No Statement Available
PU  - SAGE PUBLICATIONS INC
PI  - THOUSAND OAKS
PA  - 2455 TELLER RD, THOUSAND OAKS, CA 91320 USA
SN  - 2053-9517
J9  - BIG DATA SOC
JI  - Big Data Soc.
DA  - JUL
PY  - 2023
VL  - 10
IS  - 2
C7  - 20539517231210273
DO  - 10.1177/20539517231210273
WE  - Social Science Citation Index (SSCI)AN  - WOS:001121057800001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  40
ER  -

TY  - JOUR
AU  - Akbarzadeh, S
AU  - Lobarinas, E
AU  - Kehtarnavaz, N
TI  - Online Personalization of Compression in Hearing Aids via Maximum Likelihood Inverse Reinforcement Learning
T2  - IEEE ACCESS
LA  - English
KW  - Auditory system
KW  - Hearing aids
KW  - Reinforcement learning
KW  - Training
KW  - Standards
KW  - Gain
KW  - Fitting
KW  - Personalization of compression in hearing aids
KW  - hearing aid fitting
KW  - maximum likelihood inverse reinforcement learning
KW  - SIMPLEX PROCEDURE
KW  - DYNAMIC-RANGE
KW  - RELIABILITY
KW  - SUPPRESSION
KW  - INPUT
AB  - A key function of modern hearing aids is compression or mapping of sound to the residual hearing range of those suffering from hearing loss. This paper presents a machine learning approach to personalize compression in hearing aids in an online manner. The online feature of this approach allows it to be deployed in the field. The significance of this personalized compression lies in enabling preferred hearing outcomes relative to the one-size-fits-all prescriptive compression rationales that are currently being used. This personalization approach utilizes maximum likelihood inverse reinforcement learning to establish a model of a hearing aid user's preference based on paired comparisons by the user. The results of the preference paired comparisons between the personalized and standard prescriptive settings from ten subjects indicated that personalized settings were preferred about 10 times more than the standard prescriptive settings. In addition, a word recognition comparison was conducted showing that the personalized settings had no adverse impact on speech understanding in either quiet or in competing noise conditions.
AD  - Univ Texas Dallas, Elect & Comp Engn Dept, Richardson, TX 75080 USAAD  - Univ Texas Dallas, Callier Ctr Commun Disorders, Richardson, TX 75080 USAC3  - University of Texas SystemC3  - University of Texas DallasC3  - University of Texas SystemC3  - University of Texas DallasFU  - Institutional Review Board at the University of Texas at Dallas [20-13]
FX  - This work involved human subjects or animals in its research. Approval of all ethical and experimental procedures and protocols was granted by the Institutional Review Board at the University of Texas at Dallas under IRB No. 20-13.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
J9  - IEEE ACCESS
JI  - IEEE Access
PY  - 2022
VL  - 10
SP  - 58537
EP  - 58546
DO  - 10.1109/ACCESS.2022.3178594
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000808039100001
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  34
ER  -

TY  - JOUR
AU  - Loftus, TJ
AU  - Filiberto, AC
AU  - Li, YJ
AU  - Balch, J
AU  - Cook, AC
AU  - Tighe, PJ
AU  - Efron, PA
AU  - Upchurch, GR
AU  - Rashidi, P
AU  - Li, XL
AU  - Bihorac, A
TI  - Decision analysis and reinforcement learning in surgical decision-making
T2  - SURGERY
LA  - English
KW  - CLINICAL-PRACTICE
KW  - LAPAROSCOPIC CHOLECYSTECTOMY
KW  - BILIARY DYSKINESIA
KW  - MEDICAL LITERATURE
KW  - USERS GUIDES
KW  - STROKE
KW  - THERAPY
KW  - GUIDELINES
KW  - QUALITY
KW  - PATIENT
AB  - Background: Surgical patients incur preventable harm from cognitive and judgment errors made under time constraints and uncertainty regarding patients diagnoses and predicted response to treatment. Decision analysis and techniques of reinforcement learning theoretically can mitigate these challenges but are poorly understood and rarely used clinically. This review seeks to promote an understanding of decision analysis and reinforcement learning by describing their use in the context of surgical decision-making.
   Methods: Cochrane, EMBASE, and PubMed databases were searched from their inception to June 2019. Included were 41 articles about cognitive and diagnostic errors, decision-making, decision analysis, and machine-learning. The articles were assimilated into relevant categories according to Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews guidelines.
   Results: Requirements for time-consuming manual data entry and crude representations of individual patients and clinical context compromise many traditional decision-support tools. Decision analysis methods for calculating probability thresholds can inform population-based recommendations that jointly consider risks, benefits, costs, and patient values but lack precision for individual patient-centered decisions. Reinforcement learning, a machine-learning method that mimics human learning, can use a large set of patient-specific input data to identify actions yielding the greatest probability of achieving a goal. This methodology follows a sequence of events with uncertain conditions, offering potential advantages for personalized, patient-centered decision-making. Clinical application would require secure integration of multiple data sources and attention to ethical considerations regarding liability for errors and individual patient preferences.
   Conclusion: Traditional decision-support tools are ill-equipped to accommodate time constraints and uncertainty regarding diagnoses and the predicted response to treatment, both of which often impair surgical decision-making. Decision analysis and reinforcement learning have the potential to play complementary roles in delivering high-value surgical care through sound judgment and optimal decision-making. (C) 2020 Elsevier Inc. All rights reserved.
AD  - Univ Florida Hlth, Dept Surg, Gainesville, FL USAAD  - Univ Florida, NSF Ctr Big Learning, Gainesville, FL USAAD  - Univ Calif San Francisco, Dept Med, San Francisco, CA 94143 USAAD  - Univ Florida Hlth, Dept Anesthesiol, Gainesville, FL USAAD  - Univ Florida Hlth, Dept Orthoped, Gainesville, FL USAAD  - Univ Florida Hlth, Dept Informat Syst Operat Management, Gainesville, FL USAAD  - Univ Florida, Dept Biomed Engn, Gainesville, FL USAAD  - Univ Florida, Dept Comp & Informat Sci, Gainesville, FL USAAD  - Univ Florida, Dept Engn, Gainesville, FL USAAD  - Univ Florida, Dept Elect & Comp Engn, Gainesville, FL USAAD  - Univ Florida Hlth, Dept Med, Precis & Intelligence Med, Gainesville, FL USAC3  - State University System of FloridaC3  - University of FloridaC3  - State University System of FloridaC3  - University of FloridaC3  - University of California SystemC3  - University of California San FranciscoC3  - State University System of FloridaC3  - University of FloridaC3  - State University System of FloridaC3  - University of FloridaC3  - State University System of FloridaC3  - University of FloridaC3  - State University System of FloridaC3  - University of FloridaC3  - State University System of FloridaC3  - University of FloridaC3  - State University System of FloridaC3  - University of FloridaC3  - State University System of FloridaC3  - University of FloridaC3  - State University System of FloridaC3  - University of FloridaFU  - National Institute of General Medical Sciences (NIGMS) [T32 GM008721]; NIGMS [R01GM114290, P50 GM-111152]; CAREER award from the National Science Foundation Division of Information and Intelligent Systems [NSF-IIS 1750192]; National Insitutes of Health National Institute of Biomedical Imaging and Bioengineering [R21EB027344-01];  [R01 GM110240]
FX  - T.J.L. was supported by a postgraduate training grant (T32 GM008721) in burns, trauma, and perioperative injury from the National Institute of General Medical Sciences (NIGMS). P.T.J. was supported by R01GM114290 from the NIGMS. P.R. was supported by CAREER award, NSF-IIS 1750192, from the National Science Foundation Division of Information and Intelligent Systems, and by the National Insitutes of Health National Institute of Biomedical Imaging and Bioengineering R21EB027344-01. A.B. and P.R. were supported by R01 GM110240. A.B. and P.A.E. were supported by P50 GM-111152 from the NIGMS.
PU  - MOSBY-ELSEVIER
PI  - NEW YORK
PA  - 360 PARK AVENUE SOUTH, NEW YORK, NY 10010-1710 USA
SN  - 0039-6060
J9  - SURGERY
JI  - Surgery
DA  - AUG
PY  - 2020
VL  - 168
IS  - 2
SP  - 253
EP  - 266
DO  - 10.1016/j.surg.2020.04.049
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000554926800019
N1  - Times Cited in Web of Science Core Collection:  14
Total Times Cited:  17
Cited Reference Count:  78
ER  -

TY  - JOUR
AU  - Gönül, S
AU  - Namli, T
AU  - Cosar, A
AU  - Toroslu, IH
TI  - A reinforcement learning based algorithm for personalization of digital, just-in-time, adaptive interventions
T2  - ARTIFICIAL INTELLIGENCE IN MEDICINE
LA  - English
KW  - Reinforcement learning
KW  - Intervention delivery optimization
KW  - Just-in-time adaptive interventions
KW  - Personalized health interventions
KW  - Personalized interventions
KW  - BEHAVIOR
KW  - MODELS
AB  - Suboptimal health related behaviors and habits; and resulting chronic diseases are responsible for majority of deaths globally. Studies show that providing personalized support to patients yield improved results by preventing and/or timely treatment of these problems. Digital, just-in-time and adaptive interventions are mobile phone-based notifications that are being utilized to support people wherever and whenever necessary in coping with their health problems. In this research, we propose a reinforcement learning-based mechanism to personalize interventions in terms of timing, frequency and preferred type(s). We simultaneously employ two reinforcement learning models, namely intervention-selection and opportune-moment-identification; capturing and exploiting changes in people's long-term and momentary contexts respectively. While the intervention-selection model adapts the intervention delivery with respect to type and frequency, the opportune-moment-identification model tries to find the most opportune moments to deliver interventions throughout a day. We propose two accelerator techniques over the standard reinforcement learning algorithms to boost learning performance. First, we propose a customized version of eligibility traces for rewarding past actions throughout an agent's trajectory. Second, we utilize the transfer learning method to reuse knowledge across multiple learning environments. We validate the proposed approach in a simulated experiment where we simulate four personas differing in their daily activities, preferences on specific intervention types and attitudes towards the targeted behavior. Our experiments show that the proposed approach yields better results compared to the standard reinforcement learning algorithms and successfully capture the simulated variations associated with the personas.
AD  - SRDC Corp, Silikon Blok Kat 1 16 SRDC Teknokent ODTU, Ankara, TurkeyAD  - Middle East Tech Univ, Orta Dogu Tekn Univ, Dept Comp Engn, Mah Dumlupinar Blv 1, TR-06800 Ankara, TurkeyC3  - Middle East Technical UniversityPU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0933-3657
SN  - 1873-2860
J9  - ARTIF INTELL MED
JI  - Artif. Intell. Med.
DA  - MAY
PY  - 2021
VL  - 115
C7  - 102062
DO  - 10.1016/j.artmed.2021.102062
C6  - APR 2021
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)AN  - WOS:000651122000001
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  7
Cited Reference Count:  52
ER  -

TY  - CPAPER
AU  - Hongvanthong, S
A1  - IEEE
TI  - Novel Four-Layered Software Defined 5G Architecture for AI-based Load Balancing and QoS Provisioning
T2  - 2020 5TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION SYSTEMS (ICCCS 2020)
LA  - English
CP  - IEEE 5th International Conference on Computer and Communication Systems (ICCCS)
KW  - QoS
KW  - software defined 5G network
KW  - Artificial intelligence
KW  - distributed control plane
KW  - MULTI-CONTROLLER DEPLOYMENT
AB  - Software defined 5G network (SD-5G) is an evolving networking technology. The integration of SDN and 5G brings scalability, and efficiency. However, Quality of Service (QoS) provision is still challenging in SD-5G due to improper load balancing, traffic unawareness and so on. To overwhelm these issues this paper designs a novel load balancing scheme using Artificial Intelligence (AI) techniques. Firstly, novel four-layered SD-5G network is designed with user plane, smart data plane, load balancing plane, and distributed control plane. In the context to 5G, the data transmission rate must satisfy the QoS constraints based on the traffic type such as text, audio, video etc. Thus, the data from the user plane is classified by Smart Traffic Analyzer in the data plane. For traffic analysis, Enriched Neuro-Fuzzy (ENF) classifier is proposed. In the load balancing plane, Primary Load balancer and Secondary Load Balancer are deployed. This plane is responsible for balancing the load among controllers. For controller load balancing, switch migration is presented. Overloaded controller is predicted by Entropy function. Then decision for migration is made by Fitness-based Reinforcement Learning (F-RL) algorithm. Finally, the four-layered SD-5G network is modeled in the NS-3.26. The observations shows that the proposed work improves the SD-5G network in terms of Loss Rate, Packet Delivery Rate, Delay, and round trip time.
AD  - Wuhan Univ Technol, Sch Comp Sci, Wuhan, Peoples R ChinaC3  - Wuhan University of TechnologyPU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-7281-6136-5
PY  - 2020
SP  - 859
EP  - 863
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000610526500167
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  16
ER  -

TY  - CPAPER
AU  - Trauth, R
AU  - Kaufeld, M
AU  - Geisslinger, M
AU  - Betz, J
A1  - IEEE
TI  - Learning and Adapting Behavior of Autonomous Vehicles through Inverse Reinforcement Learning
T2  - 2023 IEEE INTELLIGENT VEHICLES SYMPOSIUM, IV
LA  - English
CP  - 34th IEEE Intelligent Vehicles Symposium (IV)
AB  - The driving behavior of autonomous vehicles has a significant impact on safety for all traffic participants. Unlike current traffic participants, autonomous vehicles in the future will also need to adhere to safety standards and defined risk properties in order to achieve a high level of public acceptance. At the same time, successful autonomous vehicles must be able to interact with human drivers in mixed traffic in a way that enables traffic to flow. In this paper, we present a hybrid approach to trajectory planning that learns and adapts human driving behavior using inverse reinforcement learning. The proposed approach performs a large-scale simulation with HighD real-world scenarios to learn human driving behavior and domain-specific traffic-flow characteristics. The analysis of the work focuses on the influence of risk-taking, which provides information about driving style safety. The results show insights into the risk behavior of trajectory planning approaches compared to human risk assessment. The comparison to human trajectories is intended to ensure comparability and accurate classification of risk-taking. We recommend a hybrid method for adapting driving behavior, in order to maintain the explainability and safety of the trajectory planning algorithm.
AD  - Tech Univ Munich, Inst Automot Technol, D-85748 Garching, GermanyAD  - Tech Univ Munich, Bavarian Res Fdn BFS, Garching, GermanyAD  - Inst Eth Artificial Intelligence IEAI, Munich, GermanyAD  - Tech Univ Munich, Sch Engn & Design, D-85748 Garching, GermanyAD  - Tech Univ Munich, Professorship Autonomous Vehicle Syst, D-85748 Garching, GermanyC3  - Technical University of MunichC3  - Technical University of MunichC3  - Technical University of MunichC3  - Technical University of MunichFU  - Technical University of Munich - Bavarian Research Foundation (BFS); Institute for Ethics in Artificial Intelligence (IEAI)
FX  - They gratefully acknowledge the financial support from the Technical University of Munich - Bavarian Research Foundation (BFS), and the Institute for Ethics in Artificial Intelligence (IEAI)
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1931-0587
SN  - 979-8-3503-4691-6
J9  - IEEE INT VEH SYM
PY  - 2023
DO  - 10.1109/IV55152.2023.10186668
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001042247300128
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  33
ER  -

TY  - CPAPER
AU  - Han, G
AU  - Choi, J
AU  - Lee, H
AU  - Kim, J
A1  - IEEE
TI  - Reinforcement Learning-Based Black-Box Model Inversion Attacks
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
LA  - English
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
AB  - Model inversion attacks are a type of privacy attack that reconstructs private data used to train a machine learning model, solely by accessing the model. Recently, white-box model inversion attacks leveraging Generative Adversarial Networks (GANs) to distill knowledge from public datasets have been receiving great attention because of their excellent attack performance. On the other hand, current black-box model inversion attacks that utilize GANs suffer from issues such as being unable to guarantee the completion of the attack process within a predetermined number of query accesses or achieve the same level of performance as white-box attacks. To overcome these limitations, we propose a reinforcement learning-based black-box model inversion attack. We formulate the latent space search as a Markov Decision Process (MDP) problem and solve it with reinforcement learning. Our method utilizes the confidence scores of the generated images to provide rewards to an agent. Finally, the private data can be reconstructed using the latent vectors found by the agent trained in the MDP. The experiment results on various datasets and models demonstrate that our attack successfully recovers the private information of the target model by achieving state-of-the-art attack performance. We emphasize the importance of studies on privacy-preserving machine learning by proposing a more advanced black-box model inversion attack.
AD  - Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South KoreaC3  - Korea Advanced Institute of Science & Technology (KAIST)FU  - Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government(MSIT) [2022-0-00184]
FX  - This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT). (NO.2022-0-00184, Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics)
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
J9  - PROC CVPR IEEE
PY  - 2023
SP  - 20504
EP  - 20513
DO  - 10.1109/CVPR52729.2023.01964
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001062531304080
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  1
Cited Reference Count:  27
ER  -

TY  - JOUR
AU  - Thon, C
AU  - Boettcher, AC
AU  - Moehlen, F
AU  - Yu, MH
AU  - Kwade, A
AU  - Schilde, C
TI  - Multi-modal framework to model wet milling through numerical simulations and artificial intelligence (part 2)
T2  - CHEMICAL ENGINEERING JOURNAL
LA  - English
KW  - Wet stirred media mills
KW  - Genetic reinforcement learning
KW  - Genetic programming
KW  - CFD-DEM simulation
KW  - Predictive mill models
KW  - STIRRED MEDIA MILLS
AB  - Modelling of stirred media mills is crucial because of their broad utilisation in various industries, ranging from mechanochemistry and mining to the production of batteries and pharmaceuticals. Stirred media mills are responsible for a considerable portion of the global energy demand. However, requirements exist regarding highly specific or uniform particle sizes, process conditions, and reduced wear or abrasion. Multi-modal modelling, which is the intelligent integration of different approaches, such as experiments, simulations, and AI, benefits from respective advantages of each approach. In the first study, results of an experiment conducted via magnetic tracking of a tracer bead was compared with those of simulations, and the inner mill mechanisms were investigated. The two-way coupled computer fluid dynamics discrete element method (CFD-DEM) simulations allowed the investigation of subsequent modelling through AI methods [1]. A novel AI training technique called "genetic reinforcement learning" (hereinafter, GRL), which combines neural nets with genetic algorithms, was demonstrated for cases with limited data. Furthermore, genetic programming was applied to derive transparent mathematical equations based on the generated data. Using these methods and experimentally validated simulation data, predictive models were trained, and mathematical equations were derived. Relative velocity distributions in the entire simulation domain as well as spatial distributions via heatmaps were predicted and evaluated for independent cases. Systematic predictions for the characteristic relative velocity values were generated instantaneously for varying tip speeds and bead diameters in a parameter space, which would have required 1-10 years through simulations. Finally, a transparent equation was generated via genetic programming.
AD  - Tech Univ Carolo Wilhelmina Braunschweig, Inst Particle Technol IPAT, Volkmaroder Str 5, D-38104 Braunschweig, GermanyC3  - Braunschweig University of TechnologyFU  - Deutsche Forschungsgemeinschaft (DFG) [SCHI 1265/19-1]
FX  - We would like to thank ESSS (Florian ' opolis, Brazil) for their close research cooperation and providing the research licence for Rocky DEM. In particular, we would like to thank Jan-Phillipp Furstenau of CADFEM GmbH (Hanover, Germany) for the technical support.
FX  - We also wish to thank the Deutsche Forschungsgemeinschaft (DFG) for the Heisenberg Program SCHI 1265/19-1 "Digital methods for complex systems in process and production engineering."
PU  - ELSEVIER SCIENCE SA
PI  - LAUSANNE
PA  - PO BOX 564, 1001 LAUSANNE, SWITZERLAND
SN  - 1385-8947
SN  - 1873-3212
J9  - CHEM ENG J
JI  - Chem. Eng. J.
DA  - DEC 15
PY  - 2022
VL  - 450
C7  - 137947
DO  - 10.1016/j.cej.2022.137947
C6  - JUL 2022
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000830499200003
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  34
ER  -

TY  - JOUR
AU  - Shumskii, SA
TI  - ADAM: a Model of Artificial Psyche
T2  - AUTOMATION AND REMOTE CONTROL
LA  - English
KW  - general artificial intelligence
KW  - deep reinforcement learning
KW  - hierarchical control system
AB  - An ADAM artificial psyche model implementing a hierarchical deep reinforcement learning architecture is proposed. ADAM is able to learn increasingly complex and time-consuming behavioral skills as the number of artificial psyche control levels increases. Purposeful behavior is formed by a hierarchical learning system with a gradual increase in the number of levels, where each hierarchical level is responsible for its own time scale of behavior.
AD  - Moscow Inst Phys & Technol, Dolgoprudnyi 141701, Moscow Oblast, RussiaC3  - Moscow Institute of Physics & TechnologyFU  - Competence Center of the National Technological Initiative in the field of "Artificial Intelligence" at the Moscow Institute of Physics and Technology
FX  - This work was supported in part by the Competence Center of the National Technological Initiative in the field of "Artificial Intelligence" at the Moscow Institute of Physics and Technology.
PU  - MAIK NAUKA/INTERPERIODICA/SPRINGER
PI  - NEW YORK
PA  - 233 SPRING ST, NEW YORK, NY 10013-1578 USA
SN  - 0005-1179
SN  - 1608-3032
J9  - AUTOMAT REM CONTR+
JI  - Autom. Remote Control
DA  - JUN
PY  - 2022
VL  - 83
IS  - 6
SP  - 847
EP  - 856
DO  - 10.1134/S0005117922060030
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000821017000003
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  16
ER  -

TY  - JOUR
AU  - Martín-Guerrero, JD
AU  - Lamata, L
TI  - Quantum Machine Learning: A tutorial
T2  - NEUROCOMPUTING
LA  - English
KW  - Quantum computing
KW  - Computational speed -up
KW  - Quantum-inspired learning algorithms
KW  - Quantum clustering
KW  - Quantum reinforcement learning
KW  - Quantum autoencoders
KW  - NEURAL-NETWORKS
AB  - This tutorial provides an overview of Quantum Machine Learning (QML), a relatively novel discipline that brings together concepts from Machine Learning (ML), Quantum Computing (QC) and Quantum Information (QI). The great development experienced by QC, partly due to the involvement of giant technological companies as well as the popularity and success of ML have been responsible of making QML one of the main streams for researchers working on fuzzy borders between Physics, Mathematics and Computer Science. A possible, although arguably coarse, classification of QML methods may be based on those approaches that make use of ML in a quantum experimentation environment and those others that take advantage of QC and QI to find out alternative and enhanced solutions to problems driven by data, oftentimes offering a considerable speedup and improved performances as a result of tackling problems from a complete different standpoint. Several examples will be provided to illustrate both classes of methods. (C) 2021 The Authors. Published by Elsevier B.V.
AD  - Univ Valencia UV, ETSE UV, Dept Engn Elect, Valencia, SpainAD  - Univ Seville, Dept Fis Atom Mol & Nucl, Seville 41080, SpainC3  - University of ValenciaC3  - University of SevillaFU  -  [PGC2018-095113-B-I00];  [PID2019-104002GB-C21];  [PID2019-104002GB-C22]
FX  - We acknowledge the funding from PGC2018-095113-B-I00, PID2019-104002GB-C21, and PID2019-104002GB-C22 (MCIU/AEI/FEDER, UE).
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
J9  - NEUROCOMPUTING
JI  - Neurocomputing
DA  - JAN 22
PY  - 2022
VL  - 470
SP  - 457
EP  - 461
DO  - 10.1016/j.neucom.2021.02.102
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000761818900017
N1  - Times Cited in Web of Science Core Collection:  24
Total Times Cited:  24
Cited Reference Count:  56
ER  -

TY  - JOUR
AU  - Yang, TT
AU  - Chen, JC
AU  - Zhang, N
TI  - AI-Empowered Maritime Internet of Things: A Parallel-Network-Driven Approach
T2  - IEEE NETWORK
LA  - English
KW  - Marine vehicles
KW  - Internet of Things
KW  - Reinforcement learning
KW  - Complexity theory
KW  - Databases
KW  - CHALLENGES
AB  - As one of the key technologies for realizing a fully digitalized world, the Internet of Things (IoT) requires ubiquitous connections across both land and sea. However, due to lack of infrastructure such as optical fibers and base stations, maritime communications inevitably face a highly complex and heterogeneous environment, which greatly challenges the connection reliability and traffic steering efficiency for future service-oriented maritime IoT. With the recent burgeoning application of artificial intelligence (AI) in many fields, an AI-empowered autonomous network for maritime IoT is envisioned as a promising solution. However, AI typically involves training/learning processes, which require realistic data/environment in order to obtain valuable outcomes. To this end, this article proposes the parallel network, which can be regarded as the "digital twin" of the real network and is responsible for realizing four key functionalities: self-learning and optimizing, state inference and network cognition, event prediction and anomaly detection, and knowledge database and snapshots. We then explain how various AI methods can facilitate the operation of the parallel- network-driven maritime network. A case study is provided to demonstrate the idea. Research directions are also outlined for further studies.
AD  - Dongguan Univ Technol, Dongguan, Peoples R ChinaAD  - Peng Cheng Lab, Shenzhen, Peoples R ChinaAD  - Univ Windsor, Windsor, ON, CanadaC3  - Dongguan University of TechnologyC3  - Peng Cheng LaboratoryC3  - University of WindsorFU  - Natural Science Foundation of China [61771086]; Dalian Outstanding Young Science and Technology Talents Foundation [2017RJ06]; Liaoning Province Xingliao Talents Program [XLYC1807149]; Guangdong Province Basic and Applied Basic Research Foundation [2019B1515120084]; Verification Platform of Multi-tier Coverage Communication Network for Oceans [PCL2018KP002]
FX  - This work was supported in part by Natural Science Foundation of China under Grant 61771086, Dalian Outstanding Young Science and Technology Talents Foundation under Grant 2017RJ06, Liaoning Province Xingliao Talents Program under Grant XLYC1807149, Guangdong Province Basic and Applied Basic Research Foundation under Grant 2019B1515120084, and the Verification Platform of Multi-tier Coverage Communication Network for Oceans (PCL2018KP002).
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 0890-8044
SN  - 1558-156X
J9  - IEEE NETWORK
JI  - IEEE Netw.
DA  - SEP-OCT
PY  - 2020
VL  - 34
IS  - 5
SP  - 54
EP  - 59
DO  - 10.1109/MNET.011.2000020
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000591303900008
N1  - Times Cited in Web of Science Core Collection:  28
Total Times Cited:  28
Cited Reference Count:  15
ER  -

TY  - JOUR
AU  - Zhang, SJ
AU  - Wang, C
AU  - Zomaya, AY
TI  - Robustness Analysis and Enhancement of Deep Reinforcement Learning-Based Schedulers
T2  - IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS
LA  - English
KW  - Perturbation methods
KW  - Robustness
KW  - Task analysis
KW  - Training
KW  - Sparks
KW  - Scheduling
KW  - Computational modeling
KW  - machine learning
KW  - deep reinforcement learning
KW  - robustness
KW  - job perturbation
KW  - black-box
KW  - gradient
KW  - vulnerability
AB  - Dependency-aware jobs, such as the big data analytic workflows, are commonly executed on the cloud. They are compiled to directed acyclic graphs, with tasks linked in regarding the dependency. The cloud scheduler, which maintains a large number of resources, is responsible to execute tasks in parallel. To resolve the complex dependencies, Deep Reinforcement Learning (DRL) based schedulers are widely applied. However, we find that the DRL-based schedulers are vulnerable to the perturbations in the input jobs and may generate falsified decisions to benefit a particular job while delaying the others. By perturbation, we mean a slight adjustment to the job's node features or dependencies, while not changing its functionality. In this paper, we first explore the vulnerability of DRL-based schedulers to job perturbations without accessing the information of the DRL models used in the scheduler. We devise the black-box perturbation system, in which, a proxy model is trained to mimic the DRL-based scheduling policy. We show that the high-faith proxy model can help to craft effective perturbations. The DRL-based schedulers can be as high as 60% likely to be badly affected by the perturbations. Then, we investigate the solution to improve the robustness of DRL-based schedulers to such perturbations. We propose an adversarial training framework to force the neural model to adapt to the perturbation patterns during training so as to eliminate the potential damage during applications. Experiments show that the adversarial-trained scheduler is more robust, reducing the chance of being affected to 3-fold less and the potential bad effects halved.
AD  - Univ Sydney, Sch Comp Sci, Camperdown, NSW 2006, AustraliaAD  - CSIRO, Data61, Eveleigh, NSW 2015, AustraliaC3  - University of SydneyC3  - Commonwealth Scientific & Industrial Research Organisation (CSIRO)FU  - Australian Research Council [DP200103494]; Australian Research Council [DP200103494] Funding Source: Australian Research Council
FX  - The work of Albert Y. Zomaya was supported by the Australian Research Council Discovery under Project No. DP200103494.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 1045-9219
SN  - 1558-2183
J9  - IEEE T PARALL DISTR
JI  - IEEE Trans. Parallel Distrib. Syst.
DA  - JAN 1
PY  - 2023
VL  - 34
IS  - 1
SP  - 346
EP  - 357
DO  - 10.1109/TPDS.2022.3218649
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000892922900003
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  46
ER  -

TY  - CPAPER
AU  - Butlin, P
A1  - ASSOC COMP MACHINERY
TI  - AI Alignment and Human Reward
T2  - AIES '21: PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY
LA  - English
CP  - 4th AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - Value alignment
KW  - reward functions
KW  - human values
KW  - value learning
KW  - INTRINSIC MOTIVATION
KW  - SYSTEMS
KW  - HABITS
AB  - According to a prominent approach to AI alignment, AI agents should be built to learn and promote human values. However, humans value things in several different ways: we have desires and preferences of various kinds, and if we engage in reinforcement learning, we also have reward functions. One research project to which this approach gives rise is therefore to say which of these various classes of human values should be promoted. This paper takes on part of this project by assessing the proposal that human reward functions should be the target for AI alignment. There is some reason to believe that powerful AI agents which were aligned to values of this form would help us to lead good lives, but there is also considerable uncertainty about this claim, arising from unresolved empirical and conceptual issues in human psychology.
AD  - Kings Coll London, Dept Philosophy, London, EnglandC3  - University of LondonC3  - King's College LondonFU  - Survival and Flourishing
FX  - This research was supported by Survival and Flourishing.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-8473-5
PY  - 2021
SP  - 437
EP  - 445
DO  - 10.1145/3461702.3462570
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000767973400051
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  53
ER  -

TY  - JOUR
AU  - Shafie-Khah, M
AU  - Talari, S
AU  - Wang, F
AU  - Catalao, JPS
TI  - Decentralised demand response market model based on reinforcement learning
T2  - IET SMART GRID
LA  - English
KW  - pricing
KW  - demand side management
KW  - power markets
KW  - learning (artificial intelligence)
KW  - aggregation-based model
KW  - decentralised model
KW  - decentralised demand response market model
KW  - bi-directional communications
KW  - reinforcement learning
KW  - Q-learning
KW  - local DR market clearing price
KW  - local markets
KW  - decentralised DR market model
KW  - electricity cost
KW  - system operator
KW  - ALGORITHM
KW  - SYSTEM
AB  - A new decentralised demand response (DR) model relying on bi-directional communications is developed in this study. In this model, each user is considered as an agent that submits its bids according to the consumption urgency and a set of parameters defined by a reinforcement learning algorithm called Q-learning. The bids are sent to a local DR market, which is responsible for communicating all bids to the wholesale market and the system operator (SO), reporting to the customers after determining the local DR market clearing price. From local markets' viewpoint, the goal is to maximise social welfare. Four DR levels are considered to evaluate the effect of different DR portions in the cost of the electricity purchase. The outcomes are compared with the ones achieved from a centralised approach (aggregation-based model) as well as an uncontrolled method. Numerical studies prove that the proposed decentralised model remarkably drops the electricity cost compare to the uncontrolled method, being nearly as optimal as a centralised approach.
AD  - Univ Vaasa, Sch Technol & Innovat, Vaasa 65200, FinlandAD  - Univ Cologne, Cologne Inst Informat Syst, Cologne, GermanyAD  - North China Elect Power Univ, Baoding 071003, Peoples R ChinaAD  - Univ Porto, Fac Engn, Porto, PortugalAD  - INESC TEC, Porto, PortugalC3  - University of VaasaC3  - University of CologneC3  - North China Electric Power UniversityC3  - Universidade do PortoC3  - INESC TECFU  - FEDER funds through COMPETE 2020; Portuguese funds through FCT [POCI-01-0145-FEDER-029803 (02/SAICT/2017)]; FLEXIMAR-project (Novel marketplace for energy flexibility) from Business Finland Smart Energy Program, 2017-2021
FX  - J.P.S. CatalAo acknowledges the support by FEDER funds through COMPETE 2020 and by Portuguese funds through FCT, under POCI-01-0145-FEDER-029803 (02/SAICT/2017). Also, the work of M. Shafie-khah was supported by FLEXIMAR-project (Novel marketplace for energy flexibility), which has received funding from Business Finland Smart Energy Program, 2017-2021.
PU  - INST ENGINEERING TECHNOLOGY-IET
PI  - HERTFORD
PA  - MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND
SN  - 2515-2947
J9  - IET SMART GRID
JI  - IET Smart Grid
DA  - OCT
PY  - 2020
VL  - 3
IS  - 5
SP  - 713
EP  - 721
DO  - 10.1049/iet-stg.2019.0129
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000591264100018
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  34
ER  -

TY  - JOUR
AU  - Krening, S
TI  - Q-learning as a model of utilitarianism in a human-machine team
T2  - NEURAL COMPUTING & APPLICATIONS
LA  - English
KW  - Reinforcement learning
KW  - Utilitarianism
KW  - Ethics
KW  - Human-machine teaming
KW  - ETHICS
AB  - This paper demonstrates that Q-learning can be used to model Utilitarian decision-making. Accurately modeling ethical theories from the field of moral philosophy is an important step in the development of ethical machine learning. Modeling Utilitarian decision-making with Q-learning is a step toward ethical human-machine teaming; the human and machine contribute according to their strengths to create a more accurate Utilitarian decision than either would make individually. The Utilitarian decision is output by the Q-learning agent, as well as the ranked order of sub-optimal actions to aid in explainability. Additionally, using RL to mathematically represent Utilitarianism solves the classic drawback of Utilitarianism concerning prohibitive computation. This model can also be used for cost-benefit analysis and business decisions.
AD  - Ohio State Univ, Dept Integrated Syst Engn, 1971 Neil Ave, Columbus, OH 43210 USAC3  - University System of OhioC3  - Ohio State UniversityPU  - SPRINGER LONDON LTD
PI  - LONDON
PA  - 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN  - 0941-0643
SN  - 1433-3058
J9  - NEURAL COMPUT APPL
JI  - Neural Comput. Appl.
DA  - AUG
PY  - 2023
VL  - 35
IS  - 23
SP  - 16853
EP  - 16864
DO  - 10.1007/s00521-022-08063-x
C6  - DEC 2022
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000913290300002
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  21
ER  -

TY  - CPAPER
AU  - Saunders, J
AU  - Prenevost, L
AU  - Simsek, Ö
AU  - Hunter, A
AU  - Li, WB
A1  - IEEE
TI  - Resource-Constrained Station-Keeping for Latex Balloons using Reinforcement Learning
T2  - 2023 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, IROS
LA  - English
CP  - IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
KW  - HIGH-ALTITUDE BALLOON
KW  - OPTIMIZATION
KW  - PREDICTION
AB  - High altitude balloons have proved useful for ecological aerial surveys, atmospheric monitoring, and communication relays. However, due to weight and power constraints, there is a need to investigate alternate modes of propulsion to navigate in the stratosphere. Very recently, reinforcement learning has been proposed as a control scheme to maintain balloons in the region of a fixed location, facilitated through diverse opposing wind-fields at different altitudes. Although air-pump based station keeping has been explored, there is no research on the control problem for venting and ballasting actuated balloons, which is commonly used as a low-cost alternative. We show how reinforcement learning can be used for this type of balloon. Specifically, we use the soft actor-critic algorithm, which on average is able to station-keep within 50 km for on average 25% of the flight, consistent with state-of-the-art. Furthermore, we show that the proposed controller effectively minimises the consumption of resources, thereby supporting long duration flights. We frame the controller as a continuous control reinforcement learning problem, which allows for a more diverse range of trajectories, as opposed to current state-of-the-art work, which uses discrete action spaces. Furthermore, through continuous control, we can make use of larger ascent rates which are not possible using air-pumps. The desired ascent-rate is decoupled into desired altitude and time-factor to provide a more transparent policy, compared to low-level control commands used in previous works. Finally, by applying the equations of motion, we establish appropriate thresholds for venting and ballasting to prevent the agent from exploiting the environment. More specifically, we ensure actions are physically feasible by enforcing constraints on venting and ballasting.
AD  - Univ Bath, Dept Comp Sci, Bath, EnglandAD  - Lux Aerobot, Montreal, PQ, CanadaAD  - Univ Bath, Dept Mech Engn, Bath, EnglandC3  - University of BathC3  - University of BathFU  - UKRI Centre for Doctoral Training in Accountable, Responsible & Transparent AI (ART-AI), under UKRI [EP/S023437/1]; Lux Aerobot
FX  - This work is supported by Lux Aerobot and the UKRI Centre for Doctoral Training in Accountable, Responsible & Transparent AI (ART-AI), under UKRI grant number EP/S023437/1.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2153-0858
SN  - 978-1-6654-9190-7
J9  - IEEE INT C INT ROBOT
PY  - 2023
SP  - 1102
EP  - 1109
DO  - 10.1109/IROS55552.2023.10341711
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001133658800115
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  34
ER  -

TY  - JOUR
AU  - Zhou, MF
AU  - Yu, Y
AU  - Qu, XB
TI  - Development of an Efficient Driving Strategy for Connected and Automated Vehicles at Signalized Intersections: A Reinforcement Learning Approach
T2  - IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS
LA  - English
KW  - Oscillators
KW  - Trajectory
KW  - Reinforcement learning
KW  - Vehicles
KW  - Real-time systems
KW  - Optimization
KW  - Training
KW  - Neural network
KW  - reinforcement learning
KW  - car-following
KW  - intersection
KW  - traffic light
KW  - machine learning
KW  - deep deterministic policy gradient
KW  - traffic oscillation
KW  - ADAPTIVE CRUISE CONTROL
KW  - TRAJECTORY DESIGN
KW  - VALIDATION
KW  - MODEL
KW  - GO
KW  - PROPAGATION
KW  - ALGORITHMS
KW  - PREDICTION
KW  - FRAMEWORK
KW  - GAME
AB  - The concept of Connected and Automated Vehicles (CAVs) enables instant traffic information to be shared among vehicle networks. With this newly proposed concept, a vehicle's driving behaviour will no longer be solely based on the driver's limited and incomplete observation. By taking advantages of the shared information, driving behaviours of CAVs can be improved greatly to a more responsible, accurate and efficient level. This study proposed a reinforcement-learning-based car following model for CAVs in order to obtain an appropriate driving behaviour to improve travel efficiency, fuel consumption and safety at signalized intersections in real-time. The result shows that by specifying an effective reward function, a controller can be learned and works well under different traffic demands as well as traffic light cycles with different durations. This study reveals a great potential of emerging reinforcement learning technologies in transport research and applications.
AD  - Tencent Holdings Ltd, Shenzhen 518057, Peoples R ChinaAD  - Univ Technol Sydney, Sch Civil & Environm Engn, Sydney, NSW 2007, AustraliaAD  - Chalmers Univ Technol, Dept Architecture & Civil Engn, S-41296 Gothenburg, SwedenC3  - TencentC3  - University of Technology SydneyC3  - Chalmers University of TechnologyPU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1524-9050
SN  - 1558-0016
J9  - IEEE T INTELL TRANSP
JI  - IEEE Trans. Intell. Transp. Syst.
DA  - JAN
PY  - 2020
VL  - 21
IS  - 1
SP  - 433
EP  - 443
DO  - 10.1109/TITS.2019.2942014
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000506619900034
N1  - Times Cited in Web of Science Core Collection:  153
Total Times Cited:  164
Cited Reference Count:  63
ER  -

TY  - JOUR
AU  - Taghavifar, H
AU  - Wei, CF
AU  - Taghavifar, L
TI  - Socially Intelligent Reinforcement Learning for Optimal Automated Vehicle Control in Traffic Scenarios
T2  - IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING
LA  - English
KW  - Decision making
KW  - Safety
KW  - Autonomous vehicles
KW  - Roads
KW  - Automobiles
KW  - Ethics
KW  - Reinforcement learning
KW  - Automated cars
KW  - obstacle avoidance
KW  - reinforcement learning
KW  - path planning
AB  - In this paper, a novel approach is presented for modeling the interaction dynamics between an ego car and a bicycle in a traffic scenario using a hybrid reinforcement learning framework combined with a social value orientation (SVO) model. The proposed framework leverages the SARSA algorithm to learn the optimal policy for the ego vehicle while incorporating risk cost as the negative log-likelihood of collision. Additionally, a customized SVO model is introduced to capture the social preferences of the ego car and the bicycle, defining the SVO of each agent as a continuous variable between egoistic and cooperative orientations. Furthermore, a weight parameter is incorporated in the framework to regulate the influence of the SVO model on the learning process. We demonstrate the effectiveness of our approach through extensive simulations, showing that the ego car can balance between maximizing its reward and avoiding collisions while considering the social preferences of the agents. The obtained results are compared to other models in the literature, and it is shown that the proposed method contributes to the development of safe and efficient autonomous driving systems that interact with human-driven vehicles in a socially intelligent manner Note to Practitioners-This proposed framework is motivated by the pressing challenge of navigation for autonomous cars in complex urban driving scenarios and mixed traffic situations. With the increasing prevalence of autonomous vehicles on roads, developing intelligent navigation systems that can effectively interact with other road users has become essential. Our novel framework addresses this need by leveraging the SARSA algorithm to learn the optimal policy for the ego vehicle while incorporating risk cost as the negative log-likelihood of collision. Additionally, a customized SVO model is introduced to capture the social preferences of the ego car and the bicycle, defining the SVO of each agent as a continuous variable between egoistic and cooperative orientations. This enables autonomous vehicles to make informed decisions and navigate safely and efficiently. Our framework can enormously help the field of autonomous vehicle navigation and contribute significantly to developing safe, human-centric, and reliable transportation systems. The versatility of our approach is evident in its potential to support a network of autonomous vehicles interacting with multiple road users, thereby enhancing scalability. By leveraging the power of machine learning, our solution provides a robust and adaptable approach that can handle the diverse and ever-changing conditions of urban driving scenarios.
AD  - Concordia Univ, Dept Mech Ind & Aerosp Engn, Montreal, PQ H3G 1M8, CanadaAD  - Queens Univ Belfast, Sch Mech & Aerosp Engn, Belfast BT7 1NN, North IrelandAD  - Islamic Azad Univ IAU, Sch Elect & Elect Engn, Tehran 1151863411, IranC3  - Concordia University - CanadaC3  - Queens University BelfastC3  - Islamic Azad UniversityFU  - Natural Sciences and Engineering Research Council of Canada (NSERC)
FX  - No Statement Available
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1545-5955
SN  - 1558-3783
J9  - IEEE T AUTOM SCI ENG
JI  - IEEE Trans. Autom. Sci. Eng.
DA  - 2024 JAN 29
PY  - 2024
DO  - 10.1109/TASE.2023.3347264
C6  - JAN 2024
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001167028300002
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  34
ER  -

TY  - CPAPER
AU  - Ramachandran, GS
AU  - Brugere, I
AU  - Varshney, LR
AU  - Xiong, CM
A1  - ASSOC COMP MACHINERY
TI  - GAEA: Graph Augmentation for Equitable Access via Reinforcement Learning
T2  - AIES '21: PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY
LA  - English
CP  - 4th AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - social networks
KW  - reinforcement learning
KW  - equity
KW  - fairness
KW  - dataset
KW  - INEQUALITY
AB  - Disparate access to resources by different subpopulations is a prevalent issue in societal and sociotechnical networks. For example, urban infrastructure networks may enable certain racial groups to more easily access resources such as high-quality schools, grocery stores, and polling places. Similarly, social networks within universities and organizations may enable certain groups to more easily access people with valuable information or influence. Here we introduce a new class of problems, Graph Augmentation for Equitable Access (GAEA), to enhance equity in networked systems by editing graph edges under budget constraints. We prove such problems are NP-hard, and cannot be approximated within a factor of (1 - 1/3e). We develop a principled, sample- and time-efficient Markov Reward Process (MRP)-based mechanism design framework for GAEA. Our algorithm outperforms baselines on a diverse set of synthetic graphs. We further demonstrate the method on real-world networks, by merging public census, school, and transportation datasets for the city of Chicago and applying our algorithm to find human-interpretable edits to the bus network that enhance equitable access to high-quality schools across racial groups. Further experiments on Facebook networks of universities yield sets of new social connections that would increase equitable access to certain attributed nodes across gender groups.
AD  - Salesforce Res, Palo Alto, CA 94301 USAAD  - Univ Illinois, Chicago, IL USAAD  - Univ Illinois, Urbana, IL USAC3  - SalesforceC3  - University of Illinois SystemC3  - University of Illinois ChicagoC3  - University of Illinois Chicago HospitalC3  - University of Illinois SystemC3  - University of Illinois Urbana-ChampaignPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-8473-5
PY  - 2021
SP  - 884
EP  - 894
DO  - 10.1145/3461702.3462615
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000767973400098
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  37
ER  -

TY  - JOUR
AU  - Jazayeri, MS
AU  - Jahangiri, A
TI  - Utilizing B-Spline Curves and Neural Networks for Vehicle Trajectory Prediction in an Inverse Reinforcement Learning Framework
T2  - JOURNAL OF SENSOR AND ACTUATOR NETWORKS
LA  - English
KW  - B-spline curves
KW  - neural networks
KW  - vehicle trajectory prediction
KW  - inverse reinforcement learning
KW  - HIDDEN MARKOV-MODELS
KW  - DYNAMICS
AB  - The ability to accurately predict vehicle trajectories is essential in infrastructure-based safety systems that aim to identify critical events such as near-crash situations and traffic violations. In a connected environment, important information about these critical events can be communicated to road users or the infrastructure to avoid or mitigate potential crashes. Intersections require special attention in this context because they are hotspots for crashes and involve numerous and complex interactions between road users. In this work, we developed an advanced machine learning method for trajectory prediction using B-spline curve representations of vehicle trajectories and inverse reinforcement learning (IRL). B-spline curves were used to represent vehicle trajectories; a neural network model was trained to predict the coefficients of these curves. A conditional variational autoencoder (CVAE) was used to generate candidate trajectories from these predicted coefficients. These candidate trajectories were then ranked according to a reward function that was obtained by training an IRL model on the (spline smoothed) vehicle trajectories and the surroundings of the vehicles. In our experiments we found that the neural network model outperformed a Kalman filter baseline and the addition of the IRL ranking module further improved the performance of the overall model.
AD  - San Diego State Univ, Dept Civil Construct & Environm Engn, 5500 Campanile Dr, San Diego, CA 92182 USAC3  - California State University SystemC3  - San Diego State UniversityFU  - Safety through Disruption (Safe-D) National University Transportation Center (UTC); U.S. Department of Transportation's University Transportation Centers Program [69A3551747115]
FX  - This research was funded by the Safety through Disruption (Safe-D) National University Transportation Center (UTC), a grant from the U.S. Department of Transportation's University Transportation Centers Program (Federal Grant Number: 69A3551747115). The contents of this paper reflect the views of the authors, who are responsible for the facts and the accuracy of the information presented herein.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2224-2708
J9  - J SENS ACTUAT NETW
JI  - J. Sens. Actuat. Netw.
DA  - MAR
PY  - 2022
VL  - 11
IS  - 1
C7  - 14
DO  - 10.3390/jsan11010014
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000774891600001
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  55
ER  -

TY  - JOUR
AU  - Chandak, A
AU  - Aote, S
AU  - Menghal, A
AU  - Negi, U
AU  - Nemani, S
AU  - Jha, S
TI  - Two-stage approach to solve ethical morality problem in self-driving cars
T2  - AI & SOCIETY
LA  - English
KW  - Self-driving cars
KW  - Ethical moralities
KW  - Reinforcement learning
KW  - Deep Q learning
AB  - Ethical morality is one of the significant issues in self-driving cars. The paper provides a newer approach to solve the ethical decision problems in self-driving cars until there is no concrete ethical decision to all problems. This paper gives a two-way approach to solve a problem, with first being the mapping of problem to the solution already known or which has a fixed set of solutions and action priorities defined to a problem previously. Now, if no solution is found or mapping is unsuccessful, then the second stage activates, where the solution from Deep Q-learning model is calculated. It estimates the best Q value and returns that solution or action which maximizes the reward at that instance. The reward function is designed with decreasing priorities and acts accordingly, where the users can change or define their priorities if needed. The case study and results show that the solution that is present in the paper will lead to solving ethical morality problems in self-driving cars up to a great extent.
AD  - Shri Ramdeobaba Coll Engn & Management, Nagpur, Maharashtra, IndiaC3  - Shri Ramdeobaba College of Engineering & ManagementC3  - Rashtrasant Tukadoji Maharaj Nagpur UniversityPU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0951-5666
SN  - 1435-5655
J9  - AI SOC
JI  - AI Soc.
DA  - APR
PY  - 2024
VL  - 39
IS  - 2
SP  - 693
EP  - 703
DO  - 10.1007/s00146-022-01517-9
C6  - JUN 2022
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000817047900004
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  34
ER  -

TY  - CPAPER
AU  - Henderson, P
AU  - Chugg, B
AU  - Anderson, B
AU  - Ho, DE
A1  - ACM
TI  - Beyond Ads: Sequential Decision-Making Algorithms in Law and Public Policy
T2  - PROCEEDINGS OF THE 2022 SYMPOSIUM ON COMPUTER SCIENCE AND LAW, CSLAW 2022
LA  - English
CP  - ACM Symposium on Computer Science and Law (CSLAW)
KW  - Sequential Decision-making
KW  - Reinforcement Learning
KW  - Bandits
KW  - Active Learning
KW  - AI and Society
KW  - Law and AI
KW  - Responsible AI
KW  - BIAS
KW  - GOVERNMENT
KW  - RISK
AB  - We explore the promises and challenges of employing sequential decision-making algorithms - such as bandits, reinforcement learning, and active learning - in law and public policy. While such algorithms have well-characterized performance in the private sector (e.g., online advertising), the tendency to naively apply algorithms motivated by one domain, often online advertisements, can be called the "advertisement fallacy." Our main thesis is that law and public policy pose distinct methodological challenges that the machine learning community has not yet addressed. Machine learning will need to address these methodological problems to move "beyond ads." Public law, for instance, can pose multiple objectives, necessitate batched and delayed feedback, and require systems to learn rational, causal decision-making policies, each of which presents novel questions at the research frontier. We discuss a wide range of potential applications of sequential decision-making algorithms in regulation and governance, including public health, environmental protection, tax administration, occupational safety, and benefits adjudication. We use these examples to highlight research needed to render sequential decision making policy-compliant, adaptable, and effective in the public sector. We also note the potential risks of such deployments and describe how sequential decision systems can also facilitate the discovery of harms. We hope our work inspires more investigation of sequential decision making in law and public policy, which provide unique challenges for machine learning researchers with potential for significant social benefit.
AD  - Stanford Univ, Stanford, CA 94305 USAC3  - Stanford UniversityPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-9234-1
PY  - 2022
SP  - 87
EP  - 100
DO  - 10.1145/3511265.3550439
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:001074472400009
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  2
Cited Reference Count:  159
ER  -

TY  - CPAPER
AU  - Saunders, J
AU  - Saeedi, S
AU  - Li, WB
A1  - IEEE
TI  - Parallel Reinforcement Learning Simulation for Visual Quadrotor Navigation
T2  - 2023 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, ICRA
LA  - English
CP  - IEEE International Conference on Robotics and Automation (ICRA)
KW  - ENVIRONMENT
AB  - Reinforcement learning (RL) is an agent-based approach for teaching robots to navigate within the physical world. Gathering data for RL is known to be a laborious task, and real-world experiments can be risky. Simulators facilitate the collection of training data in a quicker and more cost-effective manner. However, RL frequently requires a significant number of simulation steps for an agent to become skilful at simple tasks. This is a prevalent issue within the field of RL-based visual quadrotor navigation where state dimensions are typically very large and dynamic models are complex. Furthermore, rendering images and obtaining physical properties of the agent can be computationally expensive. To solve this, we present a simulation framework, built on AirSim, which provides efficient parallel training. Building on this framework, Ape-X is modified to incorporate parallel training of AirSim environments to make use of numerous networked computers. Through experiments we were able to achieve a reduction in training time from 3.9 hours to 11 minutes, for a toy problem, using the aforementioned framework and a total of 74 agents and two networked computers. Further details including a github repo and videos about our project, PRL4AirSim, can be found at https://sites.google.com/view/prl4airsim/home
AD  - Univ Bath, Dept Comp Sci, Bath, Avon, EnglandAD  - Toronto Metropolitan Univ, Dept Mech & Ind Engn, Toronto, ON, CanadaC3  - University of BathC3  - Toronto Metropolitan UniversityFU  - UKRI Centre for Doctoral Training in Accountable, Responsible & Transparent AI (ART-AI), under UKRI [EP/S023437/1]
FX  - This work is supported by the UKRI Centre for Doctoral Training in Accountable, Responsible & Transparent AI (ART-AI), under UKRI grant number EP/S023437/1.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1050-4729
SN  - 2577-087X
SN  - 979-8-3503-2365-8
J9  - IEEE INT CONF ROBOT
PY  - 2023
SP  - 1357
EP  - 1363
DO  - 10.1109/ICRA48891.2023.10160675
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001036713001026
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  37
ER  -

TY  - JOUR
AU  - Dobre, GC
AU  - Gillies, M
AU  - Pan, XN
TI  - Immersive machine learning for social attitude detection in virtual reality narrative games
T2  - VIRTUAL REALITY
LA  - English
KW  - Artificial intelligence
KW  - Expressive body language
KW  - Gaming
KW  - Human-computer interaction
KW  - Virtual agents
KW  - Virtual reality
KW  - BEHAVIOR
AB  - People can understand how human interaction unfolds and can pinpoint social attitudes such as showing interest or social engagement with a conversational partner. However, summarising this with a set of rules is difficult, as our judgement is sometimes subtle and subconscious. Hence, it is challenging to program Non-Player Characters (NPCs) to react towards social signals appropriately, which is important for immersive narrative games in Virtual Reality (VR). We collaborated with two game studios to develop an immersive machine learning (ML) pipeline for detecting social engagement. We collected data from participants-NPC interaction in VR, which was then annotated in the same immersive environment. Game design is a creative process and it is vital to respect designer's creative vision and judgement. We therefore view annotation as a key part of the creative process. We trained a reinforcement learning algorithm (PPO) with imitation learning rewards using raw data (e.g. head position) and socially meaningful derived data (e.g. proxemics); we compared different ML configurations including pre-training and a temporal memory (LSTM). The pre-training and LSTM configuration using derived data performed the best (84% F1-score, 83% accuracy). The models using raw data did not generalise. Overall, this work introduces an immersive ML pipeline for detecting social engagement and demonstrates how creatives could use ML and VR to expand their ability to design more engaging experiences. Given the pipeline's results for social engagement detection, we generalise it for detecting human-defined social attitudes.
AD  - Goldsmiths Univ London, Dept Comp, London, EnglandC3  - University of LondonC3  - Goldsmiths University LondonFU  - Innovate UK [TS/S02221X/1]; UK Engineering and Physical Sciences Research Council (EPSRC) [EP/L015846/1]
FX  - All participants took part in the experiment voluntarily and signed a consent form. They were given the freedom to drop at any time. The project was approved by the University's ethics board. We worked on this project in collaboration with two game studios: Dream Reality Interactive (https://www.dreamrealityint eractive.com/) and Maze Theory (https://www.maze-theory.com/); the project being supported by Innovate UK Grant TS/S02221X/1. This work was also partly supported by Grant EP/L015846/1 for the Centre for Doctoral Training in Intelligent Games and Game Intelligence (http://www.iggi.org.uk/) from the UK Engineering and Physical Sciences Research Council (EPSRC).
PU  - SPRINGER LONDON LTD
PI  - LONDON
PA  - 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN  - 1359-4338
SN  - 1434-9957
J9  - VIRTUAL REAL-LONDON
JI  - Virtual Real.
DA  - DEC
PY  - 2022
VL  - 26
IS  - 4
SP  - 1519
EP  - 1538
DO  - 10.1007/s10055-022-00644-4
C6  - APR 2022
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000779611800001
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  45
ER  -

TY  - JOUR
AU  - Alighanbari, S
AU  - Azad, NL
TI  - Safe Adaptive Deep Reinforcement Learning for Autonomous Driving in Urban Environments. Additional Filter? How and Where?
T2  - IEEE ACCESS
LA  - English
KW  - Deep reinforcement learning
KW  - adaptive learning
KW  - safe exploration
KW  - safety filters
KW  - autonomous vehicles
AB  - Autonomous driving (AD) provides a reliable solution for safe driving by replacing human drivers responsible for the majority of accidents. The emergence of Machine Learning, specifically Deep Reinforcement Learning (DRL), and its ability to solve complex games proved its potential to address AD challenges. However, model-free methods still suffer from safety-related issues that can be resolved using safe-DRL approaches. The addition of model-based safety filters to the learning-based algorithms provides safety bounds on their performance and constraint satisfaction. In this paper, we investigate the addition of a safety filter based on Model Predictive Control and show an increase in mean testing episode reward by 110% from -75 mean episode reward during testing for 50 episodes for Deep Deterministic Policy Gradient (DDPG) to 7.758. We study the impacts of safety filters (7.758 mean reward), heuristic rules, bounded additive noises (0.49% performance increase comparing to noise-free case), and exploration (3.425 mean reward) on the learning algorithm. We compare the effects of filters in the context of simulated exploration and bounded exploration and prove that bounded exploration results in 9.86% increase in mean reward and 12.95% decrease in std comparing to the other method. Additionally, inspired by Deep Internal Learning and biological mechanisms like brain plasticity, we investigate the idea of using each sample for training only once instead of utilizing stochastic batches which increases the mean testing accumulated reward by 1.87% and leads to the best performance (7.942 mean reward and 0.048 std). Finally, the results demonstrate better automotive results for our proposed method than DDPG. Our proposed method, DDPG with safety filter in bounded exploration and adaptive learning under noisy input conditions, has a success rate of 100% under different traffic densities for the simulation environment used in this paper and our assumptions. The proposed method's automotive results are shown for a braking scenario to avoid collision with other road users.
AD  - Univ Waterloo, Dept Syst Design Engn, SHEVS Lab, Waterloo, ON N2L 3G1, CanadaAD  - Univ Waterloo, Fac Engn, Dept Syst Design Engn, Waterloo, ON N2L 3G1, CanadaC3  - University of WaterlooC3  - University of WaterlooFU  - Toyota; Ontario Centers of Excellence; Natural Sciences and Engineering Research Council of Canada
FX  - This work was supported in part by Toyota, in part by the Ontario Centers of Excellence, and in part by the Natural Sciences and Engineering Research Council of Canada.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
J9  - IEEE ACCESS
JI  - IEEE Access
PY  - 2021
VL  - 9
SP  - 141347
EP  - 141359
DO  - 10.1109/ACCESS.2021.3119915
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000711699300001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  49
ER  -

TY  - CPAPER
AU  - Byrd, J
AU  - Lipton, ZC
ED  - Chaudhuri, K
ED  - Salakhutdinov, R
TI  - What is the Effect of Importance Weighting in Deep Learning?
T2  - INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 97
LA  - English
CP  - 36th International Conference on Machine Learning (ICML)
AB  - Importance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. While the effect of importance weighting is well-characterized for low-capacity misspecified models, little is known about how it impacts over-parameterized, deep neural networks. Inspired by recent theoretical results showing that on (linearly) separable data, deep linear networks optimized by SGD learn weight-agnostic solutions, we ask, for realistic deep networks, for which many practical datasets are separable, what is the effect of importance weighting? We present the surprising finding that while importance weighting impacts deep nets early in training, so long as the nets are able to separate the training data, its effect diminishes over successive epochs. Moreover, while L2 regularization and batch normalization (but not dropout), restore some of the impact of importance weighting, they express the effect via (seemingly) the wrong abstraction: why should practitioners tweak the L2 regularization, and by how much, to produce the correct weighting effect? We experimentally confirm these findings across a range of architectures and datasets.
AD  - Carnegie Mellon Univ, Pittsburgh, PA 15213 USAC3  - Carnegie Mellon UniversityFU  - Amazon; Salesforce; Adobe; AI Ethics and Governance Fund; Center for Machine Learning in Health
FX  - We would like to thank Daniel Soudry, Ferenc Huszar, Ben Eysenbach, and the reviewers for providing valuable feedback that has helped to improve the draft. We would also like to thank Amazon, Salesforce, Adobe, the AI Ethics and Governance Fund, and the Center for Machine Learning in Health, whose generous support has helped to advance this line of research on robust machine learning.
PU  - JMLR-JOURNAL MACHINE LEARNING RESEARCH
PI  - SAN DIEGO
PA  - 1269 LAW ST, SAN DIEGO, CA, UNITED STATES
SN  - 2640-3498
J9  - PR MACH LEARN RES
PY  - 2019
VL  - 97
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000684034301001
N1  - Times Cited in Web of Science Core Collection:  38
Total Times Cited:  38
Cited Reference Count:  33
ER  -

TY  - CPAPER
AU  - Chen, JZ
AU  - Zuo, YN
AU  - Zhang, DL
AU  - Qu, ZS
AU  - Wang, CH
ED  - Yu, H
ED  - Liu, J
ED  - Liu, L
ED  - Ju, Z
ED  - Liu, Y
ED  - Zhou, D
TI  - Promoting Constructive Interaction and Moral Behaviors Using Adaptive Empathetic Learning
T2  - INTELLIGENT ROBOTICS AND APPLICATIONS, ICIRA 2019, PT I
LA  - English
CP  - 12th International Conference on Intelligent Robotics and Applications (ICIRA)
KW  - Empathy
KW  - Constructive interaction
KW  - Multi-agent system
KW  - Reinforcement learning
KW  - EVOLUTION
AB  - Moral system assists people with constructive interaction by maximizing the inner stimulus transfered from outer feelings. For this reason, building an intrinsic sense of morality is one potential way of regulating agents' behaviors. Incorporating ideas found in social neuroscience, we hardwired a theoretical model of empathy in rational reinforcement learning-based agents to enable affective state sharing between agents. Our learning algorithm accounts for the impact of social comparison and companion impression, which play an important role on the update of empathy and make it possible for agents to change between cooperation and competition adaptively. Empathetic learners' behavioral dynamics were tested and analyzed in multiple game settings. In iterated prisoner dilemma, empathetic agents showed increased cooperation in most cases except exhibiting self-protection awareness vigilantly when their partners were in the antagonistic state. Empathetic agents also showed a strong sense of fairness in the ultimatum game which resulted in an evenhanded allocation scheme on resources.
AD  - Harbin Inst Technol, Space Sci & Inertial Technol Res Ctr, Harbin 150001, Peoples R ChinaAD  - Univ Calif Los Angeles, Dept Biol Chem, Los Angeles, CA 90095 USAAD  - Univ Calif Los Angeles, Dept Neurobiol, Los Angeles, CA 90095 USAC3  - Harbin Institute of TechnologyC3  - University of California SystemC3  - University of California Los AngelesC3  - University of California SystemC3  - University of California Los AngelesPU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-27526-6
SN  - 978-3-030-27525-9
J9  - LECT NOTES ARTIF INT
PY  - 2019
VL  - 11740
SP  - 3
EP  - 14
DO  - 10.1007/978-3-030-27526-6_1
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000569253500001
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  21
ER  -

TY  - CPAPER
AU  - Amendola, J
AU  - Tannuri, EA
AU  - Cozman, FG
AU  - Costa, AHR
A1  - ASME
TI  - PORT CHANNEL NAVIGATION SUBJECTED TO ENVIRONMENTAL CONDITIONS USING REINFORCEMENT LEARNING
T2  - PROCEEDINGS OF THE ASME 38TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING, 2019, VOL 7A
LA  - English
CP  - 38th ASME International Conference on Ocean, Offshore and Arctic Engineering (OMAE 2019)
AB  - This paper proposes a machine learning agent for automatically navigating a vessel in a confined channel subject to environmental conditions. The agent is trained and tested using a Ship Maneuvering Simulator and is responsible for commanding the rudder, so as to keep the vessel inside the channel with minimum distance from the center line, and to reach the final part of the channel with a prescribed thruster rotation level. The algorithm is based on deep reinforcement learning method and uses an efficient state-space representation. The advantage of using reinforcement learning is that it does not require any expert to directly teach the agent how to behave under particular conditions. The novelty of this work is that: it does not require previous knowledge on the vessel dynamic model and the maneuvering scenario; it is robust against fluctuations of environmental forces such as wind and current; it considers discrete actions of rudder commands emulating the pilot actions in a real maneuver. The developed method is convenient for simulations in scenarios or areas that were never navigated before, in which no previous navigation data can be used to train a conventional supervised learning agent. One direct application for this work is the integration with a realistic fast-time maneuvering simulator for new ports or operations. Both training and validation experiments focused on the unsheltered approach channel of the Suape Port, in Brazil; these experiments were run in a SMH-USP maneuvering simulator (real environmental conditions measured on-site were employed in simulations).
AD  - Univ Sao Paulo, Sao Paulo, BrazilC3  - Universidade de Sao PauloFU  - Petrobras; CNPq [304784/2017-6, 308433/2014-9, 425860/2016-7, 307027/2017-1]; FAPESP [2016/18841-0, 2016/21047-3]
FX  - Thanks to Petrobras for supporting the development of the maneuvering simulation center. The second author is partially supported by CNPq grant 304784/2017-6. The third author, by CNPq grant 308433/2014-9, and FAPESP grant 2016/18841-0. The fourth author, by CNPq grants 425860/2016-7,307027/2017-1 and FAPESP grant 2016/21047-3.
PU  - AMER SOC MECHANICAL ENGINEERS
PI  - NEW YORK
PA  - THREE PARK AVENUE, NEW YORK, NY 10016-5990 USA
SN  - 2153-4772
SN  - 978-0-7918-5884-4
J9  - P ASME INT C OCEAN
PY  - 2019
C7  - UNSP V07AT06A042
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000513307600042
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  17
ER  -

TY  - CPAPER
AU  - Guo, Z
AU  - Wang, P
AU  - Huang, LF
AU  - Cho, JH
A1  - IEEE
TI  - Authentic Dialogue Generation to Improve Youth's Awareness of Cybergrooming for Online Safety
T2  - 2023 IEEE 35TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, ICTAI
LA  - English
CP  - 35th IEEE International Conference on Tools with Artificial Intelligence (ICTAI)
KW  - Cybergrooming
KW  - natural language processing
KW  - deep reinforcement learning
KW  - chatbots
AB  - This paper deals with a cybergrooming and sexual misconduct topic in artificial intelligence-based educational programs. Although cybergrooming has been recognized as a cybercrime, there is a lack of programs to protect youth from cybergrooming. We present a generative chatbot framework, SERI (Stop cybERgroomIng), that can generate fluent authentic conversations in the context of cybergrooming between a perpetrator chatbot and a potential victim chatbot. Furthermore, we propose deep-reinforcement-learning-based dialogue generation with a stage-related reward to lead the conversation to the expected stage. We also minimize potential ethical issues introduced by the perverted languages when deploying the chatbots for cybersecurity education programs. We evaluated the conversations of SERI with open-source referenced, unreferenced metrics and human evaluation. We developed SERI as a platform for deploying perpetrator chatbot to interact with youth users to observe their responses and collect reactions when they are asked for private or sensitive information by the perpetrator.
AD  - Virginia Tech, Comp Sci, Blacksburg, VA 24061 USAC3  - Virginia Polytechnic Institute & State UniversityPU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1082-3409
SN  - 979-8-3503-4273-4
J9  - PROC INT C TOOLS ART
PY  - 2023
SP  - 64
EP  - 69
DO  - 10.1109/ICTAI59109.2023.00017
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001139095400009
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  17
ER  -

TY  - CPAPER
AU  - Dragan, A
A1  - Assoc Comp Machinery
TI  - Specifying AI Objectives as a Human-AI Collaboration Problem
T2  - AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY
LA  - English
CP  - 2nd AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - reward design
KW  - inverse reinforcement learning
KW  - value aligmnent
AD  - Univ Calif Berkeley, Berkeley, CA 94720 USAC3  - University of California SystemC3  - University of California BerkeleyPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
SN  - 978-1-4503-6324-2
PY  - 2019
SP  - 329
EP  - 329
DO  - 10.1145/3306618.3314227
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000556121100045
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  1
Cited Reference Count:  0
ER  -

TY  - CPAPER
AU  - Tang, YF
AU  - Xu, Y
A1  - IEEE
TI  - Multi-Agent Deep Reinforcement Learning for Solving Large-scale Air Traffic Flow Management Problem: A Time-Step Sequential Decision Approach
T2  - 2021 IEEE/AIAA 40TH DIGITAL AVIONICS SYSTEMS CONFERENCE (DASC)
LA  - English
CP  - IEEE/AIAA 40th Digital Avionics Systems Conference (DASC)
KW  - Air Traffic Flow Management
KW  - Demand-capacity Balance
KW  - Multi-agent Reinforcement Learning
KW  - Proximal Policy Optimization
KW  - OPTIMIZATION
AB  - In this paper, we focus on the demand-capacity balancing (DCB) problem in air traffic flow management, which is considered as a fully cooperative multi-agent learning task. First, a rule-based time-step environment is designed to mimic the DCB process. In this environment, each agent 'flight' decides its action at valid time steps. Three different rules are defined, based on the remaining capacity and the number of cooperative flights in each sector, to ease the learning process. Second, a multi-agent reinforcement learning framework, built on the proximal policy optimization (MAPPO), is proposed by using the parameter sharing mechanism and the mean-field approximation method, where an inherent feature of all other agents is extracted to address the credit assignment problem. Moreover, a supervisor integrated MAPPO framework is proposed, where a supervisor is designed to generate supervised actions, in such a way to further improve the learning performance. In the experiments, two performance indices, Search Capability and Generalization Capability, are considered. Both indices are assessed with the evaluation of two toy cases and a real-world case study. Results suggest that, the supervisor integrated MAPPO with supervised actions achieves the best performance across the different cases; other proposed methods also show some promising Search Capability, but only prove an acceptable Generalization Capability in simpler cases than the training cases.
AD  - Cranfield Univ, Sch Aerosp Transport & Mfg, Bedford, EnglandC3  - Cranfield UniversityFU  - SESAR Joint Undertaking, as part of the European Unions Horizon 2020 research and innovation programme: ISOBAR (artificial Intelligence to forecaSt meteOBased DCB imbAlances for netwoRk operations planning) [891965]
FX  - This work is funded by the SESAR Joint Undertaking under grant agreement No.891965, as part of the European Unions Horizon 2020 research and innovation programme: ISOBAR (artificial Intelligence to forecaSt meteOBased DCB imbAlances for netwoRk operations planning). The opinions expressed herein reflect the authors view only. Under no circumstances shall the SESAR Joint Undertaking be responsible for any use that may be made of the information contained herein.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2155-7195
SN  - 978-1-6654-3420-1
J9  - IEEEAAIA DIGIT AVION
PY  - 2021
DO  - 10.1109/DASC52595.2021.9594329
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000739652600034
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  30
ER  -

TY  - JOUR
AU  - Chen, YT
AU  - Xu, Y
AU  - Yang, L
AU  - Hu, MH
TI  - General real-time three-dimensional multi-aircraft conflict resolution method using multi-agent reinforcement learning
T2  - TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES
LA  - English
KW  - Air traffic management
KW  - Three-dimensional multi-aircraft conflict
KW  - resolution
KW  - Multi-agent reinforcement learning
KW  - Deep q-learning network
KW  - Generalisation
KW  - Uncertainty
AB  - Reinforcement learning (RL) techniques have been studied for solving the conflict resolution (CR) problem in air traffic management, leveraging their potential for computation and ability to handle uncertainty. However, challenges remain that impede the application of RL methods to CR in practice, including three-dimensional manoeuvres, generalisation, trajectory recovery, and success rate. This paper proposes a general multi-agent reinforcement learning approach for real-time three-dimensional multi-aircraft conflict resolution, in which agents share a neural network and are deployed on each aircraft to form a distributed decision-making system. To address the challenges, several technologies are introduced, including a partial observation model based on imminent threats for generalisation, a safety separation relaxation model for multiple flight levels for three-dimensional manoeuvres, an adaptive manoeuvre strategy for trajectory recovery, and a conflict buffer model for success rate. The Rainbow Deep Q-learning Network (DQN) is used to enhance the efficiency of the RL process. A simulation environment that considers flight uncertainty (resulting from mechanical and navigation errors and wind) is constructed to train and evaluate the proposed approach. The experimental results demonstrate that the proposed method can resolve conflicts in scenarios with much higher traffic density than in today's real-world situations.
AD  - Cranfield Univ, Cranfield MK43 0AL, Bedfordshire, EnglandAD  - Nanjing Univ Aeronaut & Astronaut, Nanjing 210000, Peoples R ChinaAD  - State Key Lab Air Traff Management Syst, Nanjing 210000, Peoples R ChinaC3  - Cranfield UniversityC3  - Nanjing University of Aeronautics & AstronauticsFU  - National Key R&D Program of China [2022YFB4300905]; National Natural Science Foundation of China [61903187]
FX  - We would like to express our gratitude to EUROCONTROL for supporting this study and providing the simulator prototype. The proposed method's prototype was ranked second-best in the 2022 EUROCONTROL Innovation Master Class Competition. The training method used was inspired by a SESAR exploratory research project we participated in: Artificial Intelligence Solutions to METEO-Based DCB Imbalances for Network Operations Planning (ISOBAR). The opinions expressed in this paper reflect the authors' views only. Under no circumstances shall the SESAR Joint Undertaking or EUROCONTROL be responsible for any use that may be made of the information contained in this paper. We also would like to thank the National Key R & D Program of China (No. 2022YFB4300905) and the National Natural Science Foundation of China (No. 61903187).
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0968-090X
SN  - 1879-2359
J9  - TRANSPORT RES C-EMER
JI  - Transp. Res. Pt. C-Emerg. Technol.
DA  - DEC
PY  - 2023
VL  - 157
C7  - 104367
DO  - 10.1016/j.trc.2023.104367
C6  - OCT 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001099754200001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  42
ER  -

TY  - CPAPER
AU  - Peschl, M
A1  - ASSOC COMP MACHINERY
TI  - Training for Implicit Norms in Deep Reinforcement Learning Agents through Adversarial Multi-Objective Reward Optimization
T2  - AIES '21: PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY
LA  - English
CP  - 4th AAAI/ACM Conference on AI, Ethics, and Society (AIES)
AB  - We propose a deep reinforcement learning algorithm that employs an adversarial training strategy for adhering to implicit human norms alongside optimizing for a narrow goal objective. Previous methods which incorporate human values into reinforcement learning algorithms either scale poorly or assume hand-crafted state features. Our algorithm drops these assumptions and is able to automatically infer norms from human demonstrations, which allows for integrating it into existing agents in the form of multi-objective optimization. We benchmark our approach in a search-and-rescue grid world and show that, conditioned on respecting human norms, our agent maintains optimal performance with respect to the predefined goal.
AD  - Delft Univ Technol, Dept Intelligent Syst, Delft, NetherlandsC3  - Delft University of TechnologyPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-8473-5
PY  - 2021
SP  - 275
EP  - 276
DO  - 10.1145/3461702.3462473
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000767973400034
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  9
ER  -

TY  - JOUR
AU  - Chen, YT
AU  - Xu, Y
AU  - Hu, MH
TI  - General multi-agent reinforcement learning integrating heuristic-based delay priority strategy for demand and capacity balancing
T2  - TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES
LA  - English
KW  - Demand and capacity balancing
KW  - Air traffic flow management
KW  - Multi-agent reinforcement learning
KW  - Heuristic algorithm
KW  - Deep q-learning network
KW  - Long short-term memory
KW  - TRAFFIC FLOW MANAGEMENT
AB  - Reinforcement learning (RL) techniques have been studied for solving the demand and capacity balancing (DCB) problem in air traffic management to exploit their full computational potential. Due to the lack of generalisation and the seemingly reduced optimisation performance affected by the training scenarios, it is challenging for existing RL-based DCB methods to be effectively applied in practice. This paper proposes a general multi-agent reinforcement learning (MARL) method that integrates a heuristic-based delay priority strategy to improve the efficiency of the solution and the generalisation of the model. The delay priority strategy is used to reduce the potential learning task and thus training difficulty. This study explores what features of the delay priority strategy are better suited to the MARL method. A long short-term memory (LSTM) network is integrated into a deep q-learning network (DQN) to ensure the model compatible with arbitrary DCB instances and to facilitate agents to identify key sectors. This study is conducted as a part of a large-scale European DCB research project, where real data from French and Spanish airspace are used for experimentation. Results suggest that the proposed method has advantages in generalisation, optimisation performance and computational performance over state-of-the-art RL-based DCB methods.
AD  - Cranfield Univ, Cranfield MK43 0AL, EnglandAD  - Nanjing Univ Aeronaut & Astronaut, Nanjing 210000, Peoples R ChinaC3  - Cranfield UniversityC3  - Nanjing University of Aeronautics & AstronauticsFU  - SESAR Joint Undertaking [891965]
FX  - This work was funded by the SESAR Joint Undertaking under grant agreement No. 891965, as part of the European Unions Horizon 2020 research and innovation programme: ISOBAR (artificial Intelligence to forecaSt meteO-Based DCB imbAlances for netwoRk operations planning) . The opinions expressed herein reflect the authors' view only. Under no circumstances shall the SESAR Joint Undertaking be responsible for any use that may be made of the information contained herein.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0968-090X
SN  - 1879-2359
J9  - TRANSPORT RES C-EMER
JI  - Transp. Res. Pt. C-Emerg. Technol.
DA  - AUG
PY  - 2023
VL  - 153
C7  - 104218
DO  - 10.1016/j.trc.2023.104218
C6  - JUN 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001147240500001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  45
ER  -

TY  - CPAPER
AU  - Pyatkin, V
AU  - Hwang, JD
AU  - Srikumar, V
AU  - Lu, XM
AU  - Jiang, LW
AU  - Choi, YJ
AU  - Bhagavatula, C
ED  - Rogers, A
ED  - Boyd-Graber, J
ED  - Okazaki, N
TI  - ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations
T2  - PROCEEDINGS OF THE 61ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2023): LONG PAPERS, VOL 1
LA  - English
CP  - 61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL)
AB  - Context is everything, even in commonsense moral reasoning. Changing contexts can flip the moral judgment of an action; Lying to a friend is wrong in general, but may be morally acceptable if it is intended to protect their life. We present CLARIFYDELPHI, an interactive system that learns to ask clarification questions (e.g., "why did you lie to your friend?") in order to elicit additional salient contexts of a social or moral situation. We posit that questions whose potential answers lead to diverging moral judgments are the most informative. Thus, we propose a reinforcement learning framework with a defeasibility reward that aims to maximize the divergence between moral judgments of hypothetical answers to a question. Human evaluation demonstrates that our system generates more relevant, informative and defeasible questions compared to competitive baselines. Our work is ultimately inspired by studies in cognitive science that have investigated the flexibility in moral cognition (i.e., the diverse contexts in which moral rules can be bent), and we hope that research in this direction can assist both cognitive and computational investigations of moral judgments.
AD  - Bar Ilan Univ, Ramat Gan, IsraelAD  - Univ Utah, Salt Lake City, UT 84112 USAAD  - Univ Washington, Allen Inst Artificial Intelligence, Seattle, WA 98195 USAAD  - Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USAC3  - Bar Ilan UniversityC3  - Utah System of Higher EducationC3  - University of UtahC3  - University of WashingtonC3  - University of Washington SeattleC3  - University of WashingtonC3  - University of Washington SeattleFU  - DARPA MCS program through NIWC Pacific [N66001-19-2-4031]
FX  - We thank our colleagues on the Beaker Team at the Allen Institute for AI for helping with the compute infrastructure. This work was supported in-part by DARPA MCS program through NIWC Pacific (N66001-19-2-4031).
PU  - ASSOC COMPUTATIONAL LINGUISTICS-ACL
PI  - STROUDSBURG
PA  - 209 N EIGHTH STREET, STROUDSBURG, PA 18360 USA
SN  - 978-1-959429-72-2
PY  - 2023
SP  - 11253
EP  - 11271
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001190962502059
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  36
ER  -

TY  - JOUR
AU  - Barth, D
AU  - Cohen-Boulakia, B
AU  - Ehounou, W
TI  - Distributed Reinforcement Learning for the Management of a Smart Grid Interconnecting Independent Prosumers
T2  - ENERGIES
LA  - English
KW  - reinforcement learning
KW  - game theory
KW  - Nash equilibria
KW  - smart grid
KW  - energy management
KW  - energy optimization
KW  - DEMAND RESPONSE
KW  - NASH EQUILIBRIA
KW  - GAMES
AB  - In the context of an eco-responsible production and distribution of electrical energy at the local scale of an urban territory, we consider a smart grid as a system interconnecting different prosumers, which all retain their decision-making autonomy and defend their own interests in a comprehensive system where the rules, accepted by all, encourage virtuous behavior. In this paper, we present and analyze a model and a management method for smart grids that is shared between different kinds of independent actors, who respect their own interests, and that encourages each actor to behavior that allows, as much as possible, an energy independence of the smart grid from external energy suppliers. We consider here a game theory model, in which each actor of the smart grid is a player, and we investigate distributed machine-learning algorithms to allow decision-making, thus, leading the game to converge to stable situations, in particular to a Nash equilibrium. We propose a Linear Reward Inaction algorithm that achieves Nash equilibria most of the time, both for a single time slot and across time, allowing the smart grid to maximize its energy independence from external energy suppliers.
AD  - UVSQ Univ Paris Saclay, DAVID Lab, 45 Ave Etats Unis, F-78035 Versailles, FranceAD  - CESI, LINEACT, F-92000 Nanterre, FranceAD  - Univ Nangui Abrogoua, Lab Math Informat, 02 BP V 102, Abidjan, Cote IvoireC3  - Universite Paris SaclayC3  - Universite Nangui AbrogouaFU  - European Regional Development Fund [IF0011058]
FX  - FundingThis research was part of GPS (Grid Power Sustainability) project funded by European Regional Development Fund grant number IF0011058.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 1996-1073
J9  - ENERGIES
JI  - Energies
DA  - FEB
PY  - 2022
VL  - 15
IS  - 4
C7  - 1440
DO  - 10.3390/en15041440
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000763339700001
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  39
ER  -

TY  - JOUR
AU  - Karabacak, M
AU  - Margetis, K
TI  - Embracing Large Language Models for Medical Applications: Opportunities and Challenges
T2  - CUREUS JOURNAL OF MEDICAL SCIENCE
LA  - English
KW  - data privacy
KW  - ethical considerations
KW  - generative ai
KW  - chatgpt
KW  - multimodal learning
KW  - domain adaptation
KW  - reinforcement learning
KW  - transfer learning
KW  - artificial intelligence
KW  - large language models
AB  - Large language models (LLMs) have the potential to revolutionize the field of medicine by, among other applications, improving diagnostic accuracy and supporting clinical decision-making. However, the successful integration of LLMs in medicine requires addressing challenges and considerations specific to the medical domain. This viewpoint article provides a comprehensive overview of key aspects for the successful implementation of LLMs in medicine, including transfer learning, domain-specific fine-tuning, domain adaptation, reinforcement learning with expert input, dynamic training, interdisciplinary collaboration, education and training, evaluation metrics, clinical validation, ethical considerations, data privacy, and regulatory frameworks. By adopting a multifaceted approach and fostering interdisciplinary collaboration, LLMs can be developed, validated, and integrated into medical practice responsibly, effectively, and ethically, addressing the needs of various medical disciplines and diverse patient populations. Ultimately, this approach will ensure that LLMs enhance patient care and improve overall health outcomes for all.
AD  - Mt Sinai Hlth Syst, Neurol Surg, New York, NY 10019 USAC3  - Icahn School of Medicine at Mount SinaiPU  - SPRINGERNATURE
PI  - LONDON
PA  - CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN  - 2168-8184
J9  - CUREUS J MED SCIENCE
JI  - Cureus J Med Sci
DA  - MAY 21
PY  - 2023
VL  - 15
IS  - 5
C7  - e39305
DO  - 10.7759/cureus.39305
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:001021631400031
N1  - Times Cited in Web of Science Core Collection:  17
Total Times Cited:  17
Cited Reference Count:  23
ER  -

TY  - CPAPER
AU  - Myhre, JN
AU  - Launonen, IK
AU  - Wei, SS
AU  - Godtliebsen, F
ED  - Pustelnik, N
ED  - Ma, Z
ED  - Tan, ZH
ED  - Larsen, J
TI  - CONTROLLING BLOOD GLUCOSE LEVELS IN PATIENTS WITH TYPE 1 DIABETES USING FITTED Q-ITERATIONS AND FUNCTIONAL FEATURES
T2  - 2018 IEEE 28TH INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING (MLSP)
LA  - English
CP  - IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP)
KW  - Reinforcement learning
KW  - nonparametric regression
KW  - biomedical engineering
KW  - diabetes
KW  - ARTIFICIAL PANCREAS
AB  - Type 1 Diabetes is characterized by the lack of insulin-producing beta cells in the pancreas. The artificial pancreas promises to alleviate the burdens of self-management. While the physical components of the system - the continuous glucose monitor and insulin pump - have experienced rapid advances, a technological bottleneck remains in the control algorithm, which is responsible for translating data from the former into instructions for the latter. In this work, we propose to bring machine learning techniques to bear upon the challenges of blood glucose control. Specifically, we employ reinforcement learning to learn an optimal insulin policy. Learning is generalized using nonparametric regression with functional features, exploiting information contained in the shape of the glucose curve. Our algorithm is model-free, data-driven and personalized. In-silico simulations with T1D models demonstrate the potential of the proposed algorithm.
AD  - Arctic Univ Norway, Dept Math & Stat, Tromso, NorwayAD  - Univ Melbourne, Sch Math & Stat, Melbourne, Vic, AustraliaC3  - UiT The Arctic University of TromsoC3  - University of MelbourneC3  - Melbourne Genomics Health AlliancePU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2161-0363
SN  - 978-1-5386-5477-4
J9  - IEEE INT WORKS MACH
PY  - 2018
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000450651000015
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  25
ER  -

TY  - CPAPER
AU  - Boucharas, DG
AU  - Androutsos, C
AU  - Tachos, NS
AU  - Tripoliti, EE
AU  - Manousos, D
AU  - Skaramagkas, V
AU  - Ktistakis, E
AU  - Marias, K
AU  - Tsiknakis, M
AU  - Fotiadis, DI
A1  - IEEE
TI  - AI Methods for Personalized Suggestions on Smart Glasses Based on Human Activity Recognition
T2  - 2022 IEEE-EMBS INTERNATIONAL CONFERENCE ON BIOMEDICAL AND HEALTH INFORMATICS (BHI) JOINTLY ORGANISED WITH THE IEEE-EMBS INTERNATIONAL CONFERENCE ON WEARABLE AND IMPLANTABLE BODY SENSOR NETWORKS (BSN'22)
LA  - English
CP  - 4th IEEE-EMBS International Conference on Wearable and Implantable Body Sensor Networks (BSN) / 18th IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)
KW  - Smart glasses
KW  - Sensor systems
KW  - Bayes methods
KW  - Q-learning
AB  - Smart wearables are becoming an irreplaceable part of daily living by supporting their users to maintain or adopt healthier lifestyles and monitor their current status. While the trend is increasing, little has been accomplished in the field of personalized solutions. In the present study, two models derived from distinct conceptual themes were developed, and the performance was evaluated utilizing a wearable prototype in the form of smart glasses. A statistical and a reinforcement learning approach were adopted to construct a personalization layer in terms of a predefined system reaction upon specific user behavior. The settings of the present study involve the user behavior derived from Artificial Intelligence (AI) based human activity recognition, among others, and the system reaction being a supportive Augmented Reality (AR) based functionality. Each approach yielding different benefits and drawbacks, imminently leads to a comparative analysis based on the efficiency offered by assessing the inference, update, and trend handling time. Both models are built upon the user's previous data, resulting in a data driven approach that is entirely different for each user and tailored to the user preferences. The results derived from the comparative analysis indicate that both approaches offer the personalization seeked, with the reinforcement learning approach to adapt faster.
AD  - FORTH, Biomed Res Inst, Ioannina, GreeceAD  - Univ Ioannina, Dept Mat Sci & Engn, Unit Med Technol & Intelligent Informat Syst, Ioannina, GreeceAD  - FORTH, Inst Comp Sci, Iraklion, Crete, GreeceAD  - Hellen Mediterranean Univ, Dept Elect & Comp Engn, Iraklion, Crete, GreeceAD  - FORTH, Dept Biomed Res, Ioannina, GreeceC3  - Foundation for Research & Technology - Hellas (FORTH)C3  - University of IoanninaC3  - Foundation for Research & Technology - Hellas (FORTH)C3  - Hellenic Mediterranean UniversityC3  - Foundation for Research & Technology - Hellas (FORTH)FU  - See Far project; European Union [GA 826429]
FX  - Research supported by the See Far project, has received funding from the European Union's H2020 research and innovation program under GA 826429. This article reflects only the authors' views. The European Commission is not responsible for any use that may be made for the information it contains.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-8791-7
PY  - 2022
DO  - 10.1109/BHI56158.2022.9926869
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000895865900054
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  5
ER  -

TY  - CPAPER
AU  - Chen, YT
AU  - Xu, Y
AU  - Hu, MH
AU  - Yang, L
A1  - IEEE
TI  - Demand and Capacity Balancing Technology Based on Multi-agent Reinforcement Learning
T2  - 2021 IEEE/AIAA 40TH DIGITAL AVIONICS SYSTEMS CONFERENCE (DASC)
LA  - English
CP  - IEEE/AIAA 40th Digital Avionics Systems Conference (DASC)
KW  - demand and capacity balancing
KW  - ground delay program
KW  - multi-agent reinforcement learning
KW  - double Q-learning network
KW  - experience replay
KW  - adaptive epsilon-greedy strategy
KW  - decentralized training with decentralized execution
KW  - FLOW
AB  - To effectively solve Demand and Capacity Balancing (DCB) in large-scale and high-density scenarios through the Ground Delay Program (GDP) in the pre-tactical stage, a sequential decision-making framework based on a time window is proposed. On this basis, the problem is transformed into Markov Decision Process (MDP) based on local observation, and then Multi-Agent Reinforcement Learning (MARL) method is adopted. Each flight is regarded as an independent agent to decide whether to implement GDP according to its local state observation. By designing the reward function in multiple combinations, a Mixed Competition and Cooperation (MCC) mode considering fairness is formed among agents. To improve the efficiency of MARL, we use the double Q-Learning Network (DQN), experience replay technology, adaptive epsilon-greedy strategy and Decentralized Training with Decentralized Execution (DTDE) framework. The experimental results show that the training process of the MARL method is convergent, efficient and stable. Compared with the Computer-Assisted Slot Allocation (CASA) method used in the actual operation, the number of flight delays and the average delay time is reduced by 33.7 % and 36.7 % respectively.
AD  - Cranfield Univ, Sch Aerosp Transport & Mfg, Bedford, EnglandAD  - Nanjing Univ Aeronaut & Astronaut, Coll Civil Aviat, Nanjing, Peoples R ChinaC3  - Cranfield UniversityC3  - Nanjing University of Aeronautics & AstronauticsFU  - SESAR Joint Undertaking, as part of the European Unions Horizon 2020 research and innovation programme: ISOBAR [891965]; China Scholarship Council; National Natural Science Foundation of China [61903187]; Natural Science Foundation of Jiangsu Province [BK20190414]; Postgraduate Research Innovation Program of Jiangsu [KYCX20 0213]; Interdisciplinary Innovation Foundation for Postgraduates of NUAA [KXKCXJJ202001]
FX  - This work is partially funded by the SESAR Joint Undertaking under grant agreement No.891965, as part of the European Unions Horizon 2020 research and innovation programme: ISOBAR (artificial Intelligence to forecaSt meteOBased DCB imbAlances for netwoRk operations planning). The opinions expressed herein reflect the authors view only. Under no circumstances shall the SESAR Joint Undertaking be responsible for any use that may be made of the information contained herein. This work is also partially funded by China Scholarship Council, National Natural Science Foundation of China (No.61903187), Natural Science Foundation of Jiangsu Province (BK20190414), Postgraduate Research Innovation Program of Jiangsu (KYCX20 0213) and Interdisciplinary Innovation Foundation for Postgraduates of NUAA (KXKCXJJ202001).
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2155-7195
SN  - 978-1-6654-3420-1
J9  - IEEEAAIA DIGIT AVION
PY  - 2021
DO  - 10.1109/DASC52595.2021.9594343
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000739652600047
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  18
ER  -

TY  - JOUR
AU  - McGuire, S
AU  - Michael Furlong, P
AU  - Heckman, C
AU  - Julier, S
AU  - Ahmed, N
TI  - Human-Aware Reinforcement Learning for Fault Recovery Using Contextual Gaussian Processes
T2  - JOURNAL OF AEROSPACE INFORMATION SYSTEMS
LA  - English
KW  - REAL-TIME
KW  - ROBOT
KW  - ASSIGNMENT
KW  - OPERATIONS
AB  - This work addresses the iterated nonstationary assistant selection problem, in which over the course of repeated interactions on a mission, an autonomous robot experiencing a fault must select a single human from among a group of assistants to restore it to operation. The assistants in our problem have a level of performance that changes as a function of their experience solving the problem. Our approach uses reinforcement learning via a multi-arm bandit formulation to learn about the capabilities of each potential human assistant and decide which human to task. This study, which is built on our past work, evaluates the potential for a Gaussian-process-based machine learning method to effectively model the complex dynamics associated with human learning and forgetting. Application of our method in simulation shows that our method is capable of tracking performance of human-like dynamics for learning and forgetting. Using a novel selection policy called the proficiency window, it is shown that our technique can outperform baseline selection strategies while providing guarantees on human use. Our work offers an effective potential alternative to dedicated human supervisors, with application to any human-robot system where a set of humans is responsible for overseeing autonomous robot operations.
AD  - Univ Calif Santa Cruz, Elect & Comp Engn, Santa Cruz, CA 95064 USAAD  - Univ Waterloo, Ctr Theoret Neurosci, Waterloo, ON N2L 3G1, CanadaAD  - Univ Colorado, Comp Sci, Boulder, CO 80309 USAAD  - UCL, Comp Sci, London WC1E 6BT, EnglandAD  - Univ Colorado, Ann & HJ Smead Dept Aerosp Engn Sci, Boulder, CO 80303 USAC3  - University of California SystemC3  - University of California Santa CruzC3  - University of WaterlooC3  - University of Colorado SystemC3  - University of Colorado BoulderC3  - University of LondonC3  - University College LondonC3  - University of Colorado SystemC3  - University of Colorado BoulderFU  - NASA [NNX15AQ14H]; Defense Advanced Research Projects Agency [HR0011-18-2-0043]; NASA [799200, NNX15AQ14H] Funding Source: Federal RePORTER
FX  - S. McGuire was supported by NASA under grant NNX15AQ14H. C. Heckman and S. McGuire receive current support from the Defense Advanced Research Projects Agency under grant HR0011-18-2-0043.
PU  - AMER INST AERONAUTICS  ASTRONAUTICS
PI  - RESTON
PA  - 1801 ALEXANDER BELL DRIVE, STE 500, RESTON, VA 22091-4344 USA
SN  - 2327-3097
J9  - J AEROSP INFORM SYST
JI  - J. Aerosp. Inf. Syst.
DA  - JUL
PY  - 2021
VL  - 18
IS  - 7
SP  - 429
EP  - 441
DO  - 10.2514/1.I010921
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000671025000004
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  44
ER  -

TY  - CPAPER
AU  - Miyashita, Y
AU  - Sugawara, T
A1  - Assoc Comp Machinery
TI  - Coordination Structures Generated by Deep Reinforcement Learning in Distributed Task Executions
T2  - AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS
LA  - English
CP  - 18th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)
KW  - Multi-agent deep reinforcement learning
KW  - Coordination
KW  - Cooperation
KW  - Divisional cooperation
AB  - We investigate the coordination structures generated by deep Q-network (DQN) in a distributed task execution. Cooperation and coordination are the crucial issues in multi-agent systems, and very sophisticated design or learning is required in order to achieve effective structures or regimes of coordination. In this paper, we show the results that agents establish the division of labor in a bottom-up manner by determining their implicit responsible area when input structure for DQN is constituted by their own observation and absolute location.
AD  - Waseda Univ, Comp Sci & Commun Engn, Tokyo, JapanC3  - Waseda UniversityFU  - KAKENHI [17KT0044]
FX  - This work is partly supported by KAKENHI (17KT0044).
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
SN  - 978-1-4503-6309-9
PY  - 2019
SP  - 2129
EP  - 2131
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000474345000328
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  9
ER  -

TY  - JOUR
AU  - Brown, NK
AU  - Deshpande, A
AU  - Garland, A
AU  - Pradeep, SA
AU  - Fadel, G
AU  - Pilla, S
AU  - Li, G
TI  - Deep reinforcement learning for the design of mechanical metamaterials with tunable deformation and hysteretic characteristics
T2  - MATERIALS & DESIGN
LA  - English
KW  - PARAMETRIC OPTIMIZATION
KW  - BEZIER CURVES
KW  - BEHAVIOR
KW  - MODEL
AB  - Mechanical metamaterials are regularly implemented in engineering applications due to their unique properties derived from their structural geometry and material composition. This study incorporates deep reinforcement learning, a subset of machine learning that teaches an agent to complete a task through interactive ex-periences, into mechanical metamaterial design. The approach creates a design environment for the reinforcement learning agent to iteratively construct meta -materials with tailorable deformation and hysteretic characteristics. Validation involved producing metamaterials with a thermoplastic polyurethane (TPU) base material that exhibited the deformation response of expanded thermoplastic polyurethane (E-TPU) while maximizing or minimizing hysteresis in cyclic compression. This alignment confirmed the feasibility of tailoring deformation and energy manipulation using mechanical metamaterials. The agent's generalizability was tested by tasking it to create various metamaterials with distinct loading deformation responses and specific hysteresis goals in a simulated setting. The agent consistently delivered metamaterials that met loading curve criteria and demonstrated favorable energy return. This work demonstrates the potential of deep reinforcement learning as a rapid and effective tool for designing mechanical metamaterials with customizable traits. It ushers in the possibility of on-demand metamaterial design solutions, opening avenues across industries like footwear, wearables, and medical equipment.
AD  - Clemson Univ, Dept Mech Engn, Clemson, SC 29634 USAAD  - Univ Delaware, Ctr Composite Mat, Newark, DE 19716 USAAD  - Univ Delaware, Dept Mech Engn, Newark, DE 19716 USAAD  - Univ Delaware, Dept Mat Sci & Engn, Newark, DE 19716 USAAD  - Univ Delaware, Dept Chem & Biomol Engn, Newark, DE 19716 USAAD  - Sandia Natl Labs, Albuquerque, NM 87123 USAAD  - 216 South Palmetto Blvd, Clemson, SC 29634 USAC3  - Clemson UniversityC3  - University of DelawareC3  - University of DelawareC3  - University of DelawareC3  - University of DelawareC3  - United States Department of Energy (DOE)C3  - Sandia National LaboratoriesFU  - AIM for Composite; Energy Frontier Research Center by the U.S. Department of Energy (DOE) Office of Science, Basic Energy Sciences (BES) [DE-SC0023389]; U.S. Department of Energy's National Nuclear Security Administration (DOE/NNSA) [DE-NA0003525]
FX  - Research is supported as part of the AIM for Composite, an Energy Frontier Research Center funded by the U.S. Department of Energy (DOE) Office of Science, Basic Energy Sciences (BES), under award #DE-SC0023389 (experimental tests and studies). "Sandia National Labora-tories is a multi-mission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC (NTESS), a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy's National Nuclear Security Administration (DOE/NNSA) under contract DE-NA0003525. This written work is authored by an employee of NTESS. The employee, not NTESS, owns the right, title and interest in and to the written work and is responsible for its contents. Any subjective views or opinions that might be expressed in the written work do not necessarily represent the views of the U.S. Government. The publisher acknowledges that the U.S. Government retains a non-exclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this written work or allow others to do so, for U.S. Government purposes. The DOE will provide public access to results of federally sponsored research in accordance with the DOE Public Access Plan. "The authors give special thanks to Sam Biemann and Tim Pruitt from Clemson University's Machining and Technical Services for their assistance with the additive manufacturing of the metamaterial samples. Additionally, we would like to thank Greg Sanders from BASF for supplying the Infinergy (R) samples used in this study.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0264-1275
SN  - 1873-4197
J9  - MATER DESIGN
JI  - Mater. Des.
DA  - NOV
PY  - 2023
VL  - 235
C7  - 112428
DO  - 10.1016/j.matdes.2023.112428
C6  - NOV 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001109333400001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  71
ER  -

TY  - JOUR
AU  - Abdallah, W
AU  - Kanzari, D
AU  - Sallami, D
AU  - Madani, K
AU  - Ghedira, K
TI  - A deep reinforcement learning based decision-making approach for avoiding crowd situation within the case of Covid'19 pandemic
T2  - COMPUTATIONAL INTELLIGENCE
LA  - English
KW  - COVID'19
KW  - crowd situation
KW  - decision making
KW  - deep reinforcement learning
KW  - multiagent reinforcement learning
KW  - SIR model
AB  - Individuals' flow's fluidifcation in the same way as the thinning of the population's concentration remains among major concerns within the context of the pandemic crisis situations. The recent COVID-19 pandemic crisis is a typical example of the aforementioned where on despite of the containment phases that radically isolate the population but are not applicable persistently, people have to adapt their behavior to new daily-life situations tempering Individuals' stream, avoiding tides, and watering down population's concentration. Crowd evacuation is one of the well-known research domains that can play a pertinent role to face the challenge of the COVID-19 pandemic. In fact, considering the population's concentration thinning within the slant of the "crowd evacuation" paradigm allows managing the flow of the population, and consequently, decreasing the probable number of infected cases. In other words, crowd evacuation modeling and simulation with the aim of better-exploiting individuals' flow allow the study and analysis of different possible outcomes for designing population's concentration thinning strategies. In this article, a new decision-making approach is proposed in order to cope with the aforesaid challenges, which relies on an independent Deep Q Network with an improved SIR model (IDQN-I-SIR). The machine-learning component (i.e., IDQN) is in charge of the agent's movements control and I-SIR (improved "susceptible-infected-recovered" individuals) model is responsible to control the virus spread. We demonstrate the effectiveness of IDQN-I-SIR through a case-study of individuals' flow's management with infected cases' avoidance in an emergency department (often overcrowded in context of a pandemic crisis).
AD  - Univ Manouba, Natl Sch Comp Sci ENSI, LARIA Lab, Manouba, TunisiaAD  - Univ Sousse, Higher Inst Appl Sci & Technol, Sousse, TunisiaAD  - Paris East Univ UPEC, Senart Inst Technol, LISSI EA Lab 3956, Lieusaint, FranceAD  - Cent Univ Tunis, Tunis, TunisiaC3  - Universite de la ManoubaC3  - Universite de SousseC3  - Universite Paris-Est-Creteil-Val-de-Marne (UPEC)PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN  - 0824-7935
SN  - 1467-8640
J9  - COMPUT INTELL-US
JI  - Comput. Intell.
DA  - APR
PY  - 2022
VL  - 38
IS  - 2
SP  - 416
EP  - 437
DO  - 10.1111/coin.12516
C6  - MAR 2022
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000767845300001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  29
ER  -

TY  - JOUR
AU  - Kofinas, P
AU  - Vouros, G
AU  - Dounis, AI
TI  - Energy management in solar microgrid via reinforcement learning using fuzzy reward
T2  - ADVANCES IN BUILDING ENERGY RESEARCH
LA  - English
KW  - Reinforcement learning
KW  - Q-learning
KW  - microgrid
KW  - energy management
KW  - fuzzy reward
AB  - This paper proposes a single-agent system towards solving energy management issues in solar microgrids. The proposed system consists of a photovoltaic (PV) source, a battery bank, a desalination unit (responsible for providing the demanded water) and a local consumer. The trade-offs and complexities involved in the operation of the different units, and the quality of services' demanded from energy consumer units (e.g. the desalination unit), makes the energy management a challenging task. The goal of the agent is to satisfy the energy demand in the solar microgrid, optimizing the battery usage, in conjunction to satisfying the quality of services provided. It is assumed that the solar microgrid operates in island-mode. Thus, no connection to the electrical grid is considered. The agent collects data from the elements of the system and learns the suitable policy towards optimizing system performance by using the Q-Learning algorithm. The reward function is implemented by fuzzy system Sugeno type for improving the learning efficiency. Simulation results provided show the performance of the system.
AD  - Univ Piraeus, Dept Digital Syst 80, Piraeus, GreeceAD  - Piraeus Univ Appl Sci TEI Piraeus, Dept Automat Engn 250, Athens, GreeceC3  - University of PiraeusC3  - University of West AtticaPU  - TAYLOR & FRANCIS LTD
PI  - ABINGDON
PA  - 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN  - 1751-2549
SN  - 1756-2201
J9  - ADV BUILD ENERGY RES
JI  - Adv. Build. Energy Res.
PY  - 2018
VL  - 12
IS  - 1
SP  - 97
EP  - 115
DO  - 10.1080/17512549.2017.1314832
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000437703800007
N1  - Times Cited in Web of Science Core Collection:  32
Total Times Cited:  32
Cited Reference Count:  20
ER  -

TY  - CPAPER
AU  - Futoma, J
AU  - Hughes, MC
AU  - Doshi-Velez, F
ED  - Chiappa, S
ED  - Calandra, R
TI  - POPCORN: Partially Observed Prediction Constrained Reinforcement Learning
T2  - INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108
LA  - English
CP  - 23rd International Conference on Artificial Intelligence and Statistics (AISTATS)
AB  - Many medical decision-making tasks can be framed as partially observed Markov decision processes (POMDPs). However, prevailing two-stage approaches that first learn a POMDP and then solve it often fail because the model that best fits the data may not be well suited for planning. We introduce a new optimization objective that (a) produces both high-performing policies and high-quality generative models, even when some observations are irrelevant for planning, and (b) does so in batch off-policy settings that are typical in healthcare, when only retrospective data is available. We demonstrate our approach on synthetic examples and a challenging medical decision-making problem.
AD  - Harvard SEAS, Cambridge, MA 02138 USAAD  - Tufts Univ, Dept Comp Sci, Medford, MA 02155 USAC3  - Harvard UniversityC3  - Tufts UniversityFU  - NSF [1750358, HDR-1934553]; Harvard CRCS fellowship; Harvard Embedded EthiCS fellowship; Direct For Computer & Info Scie & Enginr; Div Of Information & Intelligent Systems [1750358] Funding Source: National Science Foundation
FX  - FDV and JF acknowledge support from NSF Project 1750358. JF additionally acknowledges Oracle Labs, a Harvard CRCS fellowship, and a Harvard Embedded EthiCS fellowship. MCH acknowledges support from NSF Project HDR-1934553. The authors also thank David Sontag, Omer Gottesman, Leo Anthony Celi, Ryan Kindle, and the anonymous reviewers for thoughtful and constructive feedback.
PU  - ADDISON-WESLEY PUBL CO
PI  - BOSTON
PA  - 75 ARLINGTON ST, STE 300, BOSTON, MA 02116-3936 USA
SN  - 2640-3498
J9  - PR MACH LEARN RES
PY  - 2020
VL  - 108
SP  - 3578
EP  - 3587
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000559931301009
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  41
ER  -

TY  - JOUR
AU  - Sharma, S
AU  - Singh, B
TI  - Weighted cooperative reinforcement learning-based energy-efficient autonomous resource selection strategy for underlay D2D communication
T2  - IET COMMUNICATIONS
LA  - English
KW  - multi-agent systems
KW  - channel allocation
KW  - resource allocation
KW  - learning (artificial intelligence)
KW  - cellular radio
KW  - 5G mobile communication
KW  - telecommunication computing
KW  - cooperative communication
KW  - energy conservation
KW  - telecommunication power management
KW  - cochannel interference
KW  - 5G cellular networks
KW  - optimal channel allocation
KW  - multiagent reinforcement learning-based autonomous channel selection scheme
KW  - WCopQL-RS
KW  - learning agent
KW  - independent learning
KW  - average D2D user throughput
KW  - energy consumption
KW  - fairness value
KW  - underlay D2D communication
KW  - device-to-device communication
KW  - high spectral energy efficiency
KW  - ultra-low latency
KW  - resource pool
KW  - data rate
KW  - weighted cooperative reinforcement learning-based energy-efficient autonomous resource selection strategy
KW  - weighted cooperative Q-Learning
KW  - co-channel interference management
KW  - MANAGEMENT
KW  - ALLOCATION
KW  - NETWORKS
KW  - CHANNEL
KW  - SCHEME
AB  - Underlay Device-to-Device (D2D) communication is a key technology responsible for high data rate, ultra-low latency with high spectral and energy efficiency in 5G cellular networks. But to achieve its full potential, optimal channel allocation and effective co-channel interference management must be accomplished. To address this challenge, we propose a multi-agent reinforcement learning based autonomous channel selection scheme for D2D communication. The proposed scheme, Weighted Cooperative Q-Learning based Resource Selection (WCopQLRS), allows a D2D pair to learn to select a channel from the available resources autonomously. Learning process of each D2D transmitter involves cooperation from neighboring D2D agents by exchanging their latest Q-values. An additional parameter called cooperation range is used to determine the neighboring pairs whose Q-values can be used for learning the optimal policy. The limited prior information prevents a linear increase in the dimensions of Q-value matrix of each learning agent when the number of D2D pairs within the cell is huge. Though WCopQL-RS involves additional information exchange among agents as compared to independent learning but also provides improved system throughput and convergence speed. It is shown through simulation results that WCopQL-RS outperforms other existing schemes in terms of average D2D user throughput, energy consumption and fairness value.
AD  - NIT Kurukshetra, ECE Dept, Kurukshetra, Haryana, IndiaC3  - National Institute of Technology (NIT System)C3  - National Institute of Technology KurukshetraPU  - INST ENGINEERING TECHNOLOGY-IET
PI  - HERTFORD
PA  - MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND
SN  - 1751-8628
SN  - 1751-8636
J9  - IET COMMUN
JI  - IET Commun.
DA  - AUG 27
PY  - 2019
VL  - 13
IS  - 14
SP  - 2078
EP  - 2087
DO  - 10.1049/iet-com.2018.6028
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000481884600004
N1  - Times Cited in Web of Science Core Collection:  12
Total Times Cited:  12
Cited Reference Count:  38
ER  -

TY  - CPAPER
AU  - Tarbouriech, J
AU  - Pirotta, M
AU  - Valko, M
AU  - Lazaric, A
ED  - Ranzato, M
ED  - Beygelzimer, A
ED  - Dauphin, Y
ED  - Liang, PS
ED  - Vaughan, JW
TI  - A Provably Efficient Sample Collection Strategy for Reinforcement Learning
T2  - ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 34 (NEURIPS 2021)
LA  - English
CP  - 35th Conference on Neural Information Processing Systems (NeurIPS)
KW  - REGRET BOUNDS
KW  - EXPLORATION
AB  - One of the challenges in online reinforcement learning (RL) is that the agent needs to trade off the exploration of the environment and the exploitation of the samples to optimize its behavior. Whether we optimize for regret, sample complexity, state-space coverage or model estimation, we need to strike a different exploration-exploitation trade-off. In this paper, we propose to tackle the exploration-exploitation problem following a decoupled approach composed of: 1) An "objective-specific" algorithm that (adaptively) prescribes how many samples to collect at which states, as if it has access to a generative model (i.e., a simulator of the environment); 2) An "objective-agnostic" sample collection exploration strategy responsible for generating the prescribed samples as fast as possible. Building on recent methods for exploration in the stochastic shortest path problem, we first provide an algorithm that, given as input the number of samples b(s, a) needed in each state-action pair, requires (O) over tilde (BD + D(3/2)S(2)A) time steps to collect the B = Sigma(s,a) b(s, a) desired samples, in any unknown communicating MDP with S states, A actions and diameter D. Then we show how this general-purpose exploration algorithm can be paired with "objective-specific" strategies that prescribe the sample requirements to tackle a variety of settings- e.g., model estimation, sparse reward discovery, goal-free cost-free exploration in communicating MDPs - for which we obtain improved or novel sample complexity guarantees.
AD  - Facebook AI Res Paris, Paris, FranceAD  - Inria Lille, Lille, FranceAD  - DeepMind Paris, Paris, FrancePU  - NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI  - LA JOLLA
PA  - 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN  - 1049-5258
J9  - ADV NEUR IN
PY  - 2021
VL  - 34
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000901616402027
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  65
ER  -

TY  - CPAPER
AU  - Camacho-Gonzalez, G
AU  - D'Avella, S
AU  - Avizzano, CA
AU  - Tripicchio, P
A1  - IEEE
TI  - A Reinforcement Learning Decentralized Multi-Agent Control Approach exploiting Cognitive Cooperation on Continuous Environments
T2  - 2022 IEEE 18TH INTERNATIONAL CONFERENCE ON AUTOMATION SCIENCE AND ENGINEERING (CASE)
LA  - English
CP  - IEEE 18th International Conference on Automation Science and Engineering (IEEE CASE)
KW  - CONSENSUS
AB  - Multi-agent system control is a research topic that has broad applications ranging from multi-robot cooperation to distributed sensor networks. Reinforcement learning is shown to be promising as a control strategy in cases where the dynamics of the agents are non-linear, complex, and highly uncertain since it can learn policies from samples without using much model information. The presented manuscript proposes a multi-agent decentralized control approach based on a new multi-agent reinforcement learning setting in which two virtual agents, sharing the same environment, control a single avatar but have access to complementary details necessary to finish the task. Each of them is responsible for solving a portion of the problem, and in order to efficiently solve it, a collaboration should emerge among the virtual agents not to compete but to focus on the final goal. Each virtual agent, performing individually, is not fully autonomous since it does not have a complete vision of the scene and needs the other one to properly command the avatar. The proposed approach proved to be able to solve efficiently constrained navigation problems in two different simulated setups. An actor-critic architecture with a Proximal Policy Optimization (PPO) algorithm has been employed in continuous action and state spaces. The training and the testing have been done in a maze-like environment designed using the StarCraft II Learning Environment.
AD  - Scuola Super Sant Anna, Perceptual Robot Lab, Dept Excellence Robot & AI, Inst Mech Intelligence, Rome, ItalyC3  - Scuola Superiore Sant'AnnaPU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2161-8070
SN  - 978-1-6654-9042-9
J9  - IEEE INT CON AUTO SC
PY  - 2022
SP  - 1557
EP  - 1562
DO  - 10.1109/CASE49997.2022.9926587
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000927622400174
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  36
ER  -

TY  - CPAPER
AU  - Liu, SF
AU  - Yuan, Q
AU  - Chen, B
AU  - Luo, GY
AU  - Li, JL
ED  - Pimenidis, E
ED  - Angelov, P
ED  - Jayne, C
ED  - Papaleonidas, A
ED  - Aydin, M
TI  - Cooperative Multi-agent Reinforcement Learning with Hierachical Communication Architecture
T2  - ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2022, PT II
LA  - English
CP  - 31st International Conference on Artificial Neural Networks (ICANN)
KW  - Multi-agent cooperation
KW  - Hierarchical architecture
KW  - Communication
AB  - Communication is an essential way for multi-agent system to coordinate. By sharing local observations and intentions via communication channel, agents can better deal with dynamic environment and thus make optimal decisions. However, restricted by the limited communication channel, agents have to leverage less communication resources to transmit more informative messages. In this article, we propose a two-level hierarchical multi-agent reinforcement learning algorithm which utilizes different timescales in different levels. Communication happens only between high levels at a coarser time scale to generate sub-goals which convey the intention of agents for the low level. And the low level is responsible for implementing these sub-goals by controlling primitive actions at every tick of environment. Sub-goal is the core of this hierachical communication architecture which requires the high level to communicate efficiently and provide guidance for the low level to coordinate. This hierarchical communication architecture conveys several benefits: 1) It coarsens the collaborative granularity and reduces the requirement of communication since communication happens only in high level at a larger scale; 2) It enables the high level to focus on the coordination of goals without paying attention to implementation, thus improves the efficiency of communication; and 3) It makes better control by dividing a complex multi-agent cooperative task into multiple single-agent tasks. In experiments, we apply our approach in vehicle collision avoidance tasks and achieve better performance than baselines.
AD  - Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R ChinaC3  - Beijing University of Posts & TelecommunicationsFU  - Natural Science Foundation of China [61902035, 61876023, 62001054, 62102041]; Fundamental Research Funds for the Central Universities
FX  - This work was supported in part by the Natural Science Foundation of China under Grant 61902035, Grant 61876023, Grant 62001054 and Grant 62102041, and in part by the Fundamental Research Funds for the Central Universities.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-15931-2
SN  - 978-3-031-15930-5
J9  - LECT NOTES COMPUT SC
PY  - 2022
VL  - 13530
SP  - 14
EP  - 25
DO  - 10.1007/978-3-031-15931-2_2
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000866212300002
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  20
ER  -

TY  - JOUR
AU  - Govindarajan, P
AU  - Miret, S
AU  - Rector-Brooks, J
AU  - Phielipp, M
AU  - Rajendran, J
AU  - Chandar, S
TI  - Learning conditional policies for crystal design using offline reinforcement learning
T2  - DIGITAL DISCOVERY
LA  - English
AB  - Navigating through the exponentially large chemical space to search for desirable materials is an extremely challenging task in material discovery. Recent developments in generative and geometric deep learning have shown promising results in molecule and material discovery but often lack evaluation with high-accuracy computational methods. This work aims to design novel and stable crystalline materials conditioned on a desired band gap. To achieve conditional generation, we: (1) formulate crystal design as a sequential decision-making problem, create relevant trajectories based on high-quality materials data, and use conservative Q-learning to learn a conditional policy from these trajectories. To do so, we formulate a reward function that incorporates constraints for energetic and electronic properties obtained directly from density functional theory (DFT) calculations; (2) evaluate the generated materials from the policy using DFT calculations for both energy and band gap; (3) compare our results to relevant baselines, including behavioral cloning and unconditioned policy learning. Our experiments show that conditioned policies achieve targeted crystal design and demonstrate the capability to perform crystal discovery evaluated with accurate and computationally expensive DFT calculations.
   Conservative Q-learning for band-gap conditioned crystal design with DFT evaluations - the model is trained on trajectories constructed from crystals in the Materials Project. Results indicate promising performance for lower band gap targets.
AD  - Mila Quebec AI Inst, Polytech, Montreal, PQ, CanadaAD  - Intel Labs, Hillsboro, OR USAAD  - Univ Montreal, Mila Quebec AI Inst, Montreal, PQ, CanadaC3  - Universite de MontrealC3  - Polytechnique MontrealC3  - Intel CorporationC3  - Universite de MontrealFU  - Canadian Institute for Advanced Research; Intel; CIFAR; Canada CIFAR AI Chairs program; Canada Research Chair in Lifelong Machine Learning; NSERC Discovery Grant
FX  - We would like to thank Pierre-Paul De Breuck and Rajesh Raju for their valuable domain-related inputs. We acknowledge and thank the people responsible for the smooth functioning of the compute resources of Mila and Compute Canada. We also wish to acknowledge funding from Intel and CIFAR. Janarthanan Rajendran is supported by IVADO postdoctoral fellowship. Sarath Chandar is supported by the Canada CIFAR AI Chairs program, the Canada Research Chair in Lifelong Machine Learning, and the NSERC Discovery Grant.
PU  - ROYAL SOC CHEMISTRY
PI  - CAMBRIDGE
PA  - THOMAS GRAHAM HOUSE, SCIENCE PARK, MILTON RD, CAMBRIDGE CB4 0WF, CAMBS, ENGLAND
SN  - 2635-098X
J9  - DIGIT DISCOV
JI  - Digit. Discov.
DA  - APR 17
PY  - 2024
VL  - 3
IS  - 4
SP  - 769
EP  - 785
DO  - 10.1039/d4dd00024b
C6  - FEB 2024
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:001190252200001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  53
ER  -

TY  - CPAPER
AU  - Wang, N
AU  - Teng, YL
AU  - Hu, G
AU  - Yu, FR
A1  - IEEE
TI  - Importance-driven Data Collection for Efficient Online Learning Over the Wireless Edge
T2  - ICC 2023 - IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS
LA  - English
CP  - IEEE International Conference on Communications (IEEE ICC)
KW  - online learning
KW  - two-timescale stochastic optimization
KW  - hierarchical reinforcement learning
KW  - queue stability
AB  - Online learning has been widely applied in real-time artificial intelligence (AI) applications to learn new classes from the dynamic environment. Although the deployment of AI model training over the edge can facilitate faster processing of real-time data, the learning efficiency is plagued by the limited capacity of distributed data acquisition. In fact, not all data samples are equally important, and the random data selection strategy is not beneficial to accelerate training due to redundant data processing. In this paper, we present an importance-driven data collection framework, which leverages the usefulness of important data to improve the learning efficiency over the wireless edge. Specifically, the novel model convergence metric (MCM) is constructed to evaluate the data importance dynamically for model learning. Moreover, considering the constraint of limited network resources on learning efficiency, we establish an MCM maximization problem of joint data collecting, scheduling, and feeding in an edge computing system. A two-timescale hierarchical reinforcement learning (TTHRL) algorithm is designed to decouple the original problem into two-timescale two-level subproblems, where the top-level agent is responsible for data feeding strategy in the long term and the low-level agent learns data scheduling and collecting strategy in the short term. Simulation results show that our proposed scheme can achieve better performance improvements over the baseline schemes.
AD  - BUPT, Beijing Key Lab Space Ground Interconnect & Conve, Xitucheng Rd 10, Beijing 100876, Peoples R ChinaAD  - Carleton Univ, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, CanadaC3  - Beijing University of Posts & TelecommunicationsC3  - Carleton UniversityFU  - National Key R&D Program of China [2021YFB3300100]; National Natural Science Foundation of China [62171062]
FX  - This work was supported in part by the National Key R&D Program of China (No. 2021YFB3300100), the National Natural Science Foundation of China under Grant No. 62171062.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-5386-7462-8
J9  - INT CONF COMM
PY  - 2023
SP  - 410
EP  - 415
DO  - 10.1109/ICC45041.2023.10278679
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001094862600066
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  13
ER  -

TY  - JOUR
AU  - Tang, WX
AU  - Li, B
AU  - Li, WX
AU  - Wang, YE
AU  - Huang, JW
TI  - Reinforcement learning of non-additive joint steganographic embedding costs with attention mechanism
T2  - SCIENCE CHINA-INFORMATION SCIENCES
LA  - English
KW  - information hiding
KW  - non-additive steganography
KW  - steganalysis
KW  - cost learning
KW  - image processing
KW  - IMAGE
KW  - FRAMEWORK
KW  - STEGANALYSIS
KW  - FEATURES
KW  - NETWORK
KW  - CNN
AB  - Image steganography is the art and science of secure communication by concealing information within digital images. In recent years, the techniques of steganographic cost learning have developed rapidly. Although the existing methods can learn satisfactory additive costs, the interplay of different pixels' embedding impacts has not been considered, so the potential of learning may not be fully exploited. To overcome this limitation, in this paper, a reinforcement learning paradigm called JoPoL (joint policy learning) is proposed to extend the idea of additive cost learning to a non-additive situation. JoPoL aims to capture the interactions within pixel blocks by defining embedding policies and evaluating contributions of embedding impacts on a block level rather than a pixel level. Then, a policy network is utilized to learn optimal joint embedding policies for pixel blocks through interactions with the environment. Afterwards, these policies can be converted into joint embedding costs for practical message embedding. The structure of the policy network is designed with an effective attention mechanism and incorporated with the domain knowledge derived from traditional non-additive steganographic methods. The environment is responsible for assigning rewards according to the impacts of the sampled joint embedding actions, which are evaluated by the gradient information of a neural network-based steganalyzer. Experimental results show that the proposed non-additive method JoPoL significantly outperforms the existing additive methods against both feature-based and CNN-based steganalzyers over different payloads.
AD  - Guangzhou Univ, Inst Artificial Intelligence & Blockchain, Guangzhou 510006, Peoples R ChinaAD  - Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen Key Lab Media Secur, Shenzhen 518060, Peoples R ChinaAD  - Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518060, Peoples R ChinaAD  - Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou 510006, Peoples R ChinaC3  - Guangzhou UniversityC3  - Shenzhen UniversityC3  - Shenzhen Institute of Artificial Intelligence & Robotics for SocietyC3  - Guangzhou UniversityFU  - National Natural Science Foundation of China [62002075, 61872244, 61872099, U19B2022]; Guangdong Basic and Applied Basic Research Foundation [2019B151502001]; Shenzhen RD Program [JCYJ20200109105008228]
FX  - AcknowledgementsThis work was supported by National Natural Science Foundation of China (Grant Nos. 62002075, 61872244, 61872099, U19B2022), Guangdong Basic and Applied Basic Research Foundation (Grant No. 2019B151502001), and Shenzhen R&D Program (Grant No. JCYJ20200109105008228).
PU  - SCIENCE PRESS
PI  - BEIJING
PA  - 16 DONGHUANGCHENGGEN NORTH ST, BEIJING 100717, PEOPLES R CHINA
SN  - 1674-733X
SN  - 1869-1919
J9  - SCI CHINA INFORM SCI
JI  - Sci. China-Inf. Sci.
DA  - MAR
PY  - 2023
VL  - 66
IS  - 3
C7  - 132305
DO  - 10.1007/s11432-021-3453-5
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000937476000002
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  52
ER  -

TY  - CPAPER
AU  - Swapna, AI
AU  - Rosa, RV
AU  - Rothenberg, CE
AU  - Pasquini, R
AU  - Baliosian, J
ED  - Horner, L
ED  - Tutschku, K
ED  - DeLaOliva, A
ED  - ScottHayward, S
ED  - Tacca, M
ED  - Caltais, G
ED  - Parzyjegla, H
TI  - Policy Controlled Multi-domain cloud-network Slice Orchestration Strategy based on Reinforcement Learning
T2  - 2020 IEEE CONFERENCE ON NETWORK FUNCTION VIRTUALIZATION AND SOFTWARE DEFINED NETWORKS (NFV-SDN)
LA  - English
CP  - IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN)
AB  - The concept of network slicing plays a thriving role as 5G rolls out business models vouched by different stakeholders. The dynamic and variable characterization of endto-end cloud-network slices encompasses the composition of different slice parts laying at different administrative domains. Following a profit-maximizing Slice-as-a-Service (SaaS) model, such a multi-domain facet offers promising business opportunities in support of diverse vertical industries, rendering to network slicing marketplace members the roles of Infrastructure Provider, Slice Provider, and Tenants. The effective realization of SaaS approaches introduces a dynamic resource allocation problem, manifested as challenging run-time decisions upon on-demand slice part requests. The Orchestrator is hence responsible to perform an optimized decision on-the-fly on which elasticity requests to address based on an orchestration policy defined within the context of Network Slice architecture for the followed revenue model. This paper presents a slice management strategy for such an orchestrator can follow, based on reinforcement learning, able to efficiently orchestrate slice elasticity requests to comprehend the maximum revenue for the stakeholders of endto-end network slice lifecycle. The proposed strategy orients a Slice Orchestrator to learn which slice requests to address as per availability of the required resources at the different participating Infrastructure Providers. The experimental results show the Reinforcement Learning based Orchestrator outperforms several benchmark heuristics focused on revenue maximization.
AD  - Univ Campinas UNICAMP, Sao Paulo, BrazilAD  - Fed Univ Uberlandia UFU, Uberlandia, MG, BrazilAD  - Univ Republica, Montevideo, UruguayC3  - Universidade Estadual de CampinasC3  - Universidade Federal de UberlandiaC3  - Universidad de la Republica, UruguayFU  - H2020 4th EU-BR Collaborative Call (NECOS -Novel Enablers for Cloud Slicing) - European Commission [777067]; Brazilian Ministry of Science, Technology, Innovation, and Communication through RNP; CTIC; H2020 - Industrial Leadership [777067] Funding Source: H2020 - Industrial Leadership
FX  - This work was supported by the H2020 4th EU-BR Collaborative Call, under grant agreement 777067 (NECOS -Novel Enablers for Cloud Slicing), funded by the European Commission and the Brazilian Ministry of Science, Technology, Innovation, and Communication through RNP and CTIC.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-7281-8159-2
PY  - 2020
SP  - 167
EP  - 173
DO  - 10.1109/nfv-sdn50289.2020.9289852
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000674802100030
N1  - Times Cited in Web of Science Core Collection:  5
Total Times Cited:  5
Cited Reference Count:  23
ER  -

TY  - CPAPER
AU  - Cortés, EC
AU  - Ghosh, D
A1  - ACM
TI  - An Invitation to System-Wide Algorithmic Fairness
T2  - PROCEEDINGS OF THE 3RD AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY AIES 2020
LA  - English
CP  - 3rd AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - fairness
KW  - ethical ai
KW  - agent based modeling
KW  - recidivism
KW  - CAUSAL INFERENCE
KW  - MODELS
AB  - We propose a framework for analyzing and evaluating system-wide algorithmic fairness. The core idea is to use simulation techniques in order to extend the scope of current fairness assessments by incorporating context and feedback to a phenomenon of interest. By doing so, we expect to better understand the interaction among the social behavior giving rise to discrimination, automated decision making tools, and fairness-inspired statistical constraints. In particular, we invite the community to use agent based models as an explanatory tool for causal mechanisms of population level properties. We also propose embedding these into a reinforcement learning algorithm to find optimal actions for meaningful change. As an incentive for taking a system-wide approach, we show through a simple model of predictive policing and trials that if we limit our attention to one portion of the system, we may determine some blatantly unfair practices as fair, and be blind to overall unfairness.
AD  - Penn State Univ, State Coll, PA 16801 USAAD  - Univ Colorado, Anschutz Med Campus, Aurora, CO USAC3  - Pennsylvania Commonwealth System of Higher Education (PCSHE)C3  - Pennsylvania State UniversityC3  - University of Colorado SystemC3  - University of Colorado Anschutz Medical CampusPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-7110-0
PY  - 2020
SP  - 235
EP  - 241
DO  - 10.1145/3375627.3375860
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:001108779100045
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  40
ER  -

TY  - CPAPER
AU  - van Hasselt, H
AU  - Madjiheurem, S
AU  - Hessel, M
AU  - Silver, D
AU  - Barreto, A
AU  - Borsa, D
A1  - Assoc Advancement Artificial Intelligence
TI  - Expected Eligibility Traces
T2  - THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
LA  - English
CP  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence
AB  - The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state. In this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that some-times substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD(A). Finally, we discuss possible extensions and connections to related ideas, such as successor features.
AD  - DeepMind, London, EnglandAD  - UCL, London, EnglandC3  - University of LondonC3  - University College LondonPU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-866-4
J9  - AAAI CONF ARTIF INTE
PY  - 2021
VL  - 35
SP  - 9997
EP  - 10005
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000681269801075
N1  - Times Cited in Web of Science Core Collection:  8
Total Times Cited:  9
Cited Reference Count:  60
ER  -

TY  - CPAPER
AU  - Felizardo, LK
AU  - Matsumoto, E
AU  - Del-Moral-Hernandez, E
A1  - IEEE
TI  - Solving the optimal stopping problem with reinforcement learning: an application in financial option exercise
T2  - 2022 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
LA  - English
CP  - IEEE International Conference on Fuzzy Systems (FUZZ-IEEE) / IEEE World Congress on Computational Intelligence (IEEE WCCI) / International Joint Conference on Neural Networks (IJCNN) / IEEE Congress on Evolutionary Computation (IEEE CEC)
KW  - Reinforcement Learning
KW  - Least-squares Monte Carlo
KW  - Optimal stopping
KW  - Option pricing
KW  - VALUATION
AB  - The optimal stopping problem is a category of decision problems with a specific constrained configuration. It is relevant to various real-world applications such as finance and management. To solve the optimal stopping problem, state-of-the-art algorithms in dynamic programming, such as the least-squares Monte Carlo (LSMC), are employed. This type of algorithm relies on path simulations using only the last price of the underlying asset as a state representation. In addition, the LSMC is designed for the valuation of options where risk-neutral probabilities can be employed to explain uncertainty. However, the general optimal stopping problem goals may not fit the requirements of the LSMC showing auto-correlated prices. We employ a data-driven method that uses Monte Carlo simulation to train and test artificial neural networks (ANN) to solve the optimal stopping problem. Using ANN to solve decision problems is not entirely new. We propose a different architecture that uses convolutional neural networks (CNN) to deal with the dimensionality problem that arises when we transform the whole history of prices into a Markovian state. We present experiments that indicate that our proposed architecture improves results over the previous implementations under specific simulated time series function sets. Lastly, we employ our proposed method to compare the optimal exercise of the financial options problem with the LSMC algorithm. Our experiments show that our method can capture more accurate exercise opportunities when compared to the LSMC. We have an outstandingly higher (above 974% improvement) expected payoff from these exercise policies under the many Monte Carlo simulations that used the real-world return database on the out-of-sample (test) data.
AD  - Univ Sao Paulo, Dept Elect Syst, Escola Politecn, Sao Paulo, BrazilAD  - Fundacao Getulio Vargas, Sao Paulo Sch Econ, Sao Paulo, BrazilC3  - Universidade de Sao PauloC3  - Getulio Vargas FoundationC3  - Escola de Pos-Graduacao em Economia (EPGE)FU  - Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES - Coordination for the Improvement of Higher Education Personnel), Brazil [001, 88882.333380/2019-01]
FX  - This research was financed in part by the Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES - Coordination for the Improvement of Higher Education Personnel, Finance Code 001, grant 88882.333380/2019-01), Brazil.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2161-4393
SN  - 978-1-7281-8671-9
J9  - IEEE IJCNN
PY  - 2022
DO  - 10.1109/IJCNN55064.2022.9892333
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000867070903093
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  33
ER  -

TY  - JOUR
AU  - Dulay, J
AU  - Poltoratski, S
AU  - Hartmann, TS
AU  - Anthony, SE
AU  - Scheirer, WJ
TI  - Informing Machine Perception With Psychophysics
T2  - PROCEEDINGS OF THE IEEE
LA  - English
KW  - Training
KW  - Computer science
KW  - Supervised learning
KW  - Psychology
KW  - Reinforcement learning
KW  - Oral communication
KW  - Observers
KW  - 1ST IMPRESSIONS
KW  - LEVEL
AB  - Gustav Fechner's 1860 delineation of psychophysics, the measurement of sensation in relation to its stimulus, is widely considered to be the advent of modern psychological science. In psychophysics, a researcher parametrically varies some aspects of a stimulus and measures the resulting changes in a human subject's experience of that stimulus; doing so gives insight into the determining relationship between a sensation and the physical input that evoked it. This approach is used heavily in perceptual domains, including signal detection, threshold measurement, and ideal observer analysis. Scientific fields, such as vision science, have always leaned heavily on the methods and procedures of psychophysics, but there is now growing appreciation of them by machine learning researchers, sparked by widening overlap between biological and artificial perception [1], [2], [3], [4], [5]. Machine perception that is guided by behavioral measurements, as opposed to guidance restricted to arbitrarily assigned human labels, has significant potential to fuel further progress in artificial intelligence (AI).
AD  - Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USAAD  - Zoox Inc, Foster City, CA USAC3  - University of Notre DameFU  - U.S. National Science Foundation [BCS:1942151]
FX  - This work was supported in part by the U.S. National Science Foundation under Grant BCS:1942151. This work involved human subjects or animals in its research. Approval of all ethical and experimental procedures and protocols was granted by the University of Notre Dame IRB under Application No. 18-01-4341.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 0018-9219
SN  - 1558-2256
J9  - P IEEE
JI  - Proc. IEEE
DA  - FEB
PY  - 2024
VL  - 112
IS  - 2
SP  - 88
EP  - 96
DO  - 10.1109/JPROC.2024.3380905
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001200241200004
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  43
ER  -

TY  - CPAPER
AU  - Hussain, M
A1  - ACM
TI  - Can AlphaGo be apt subjects for Praise/Blame for "Move 37"?
T2  - PROCEEDINGS OF THE 2023 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, AIES 2023
LA  - English
CP  - AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES)
KW  - Blame and Praise
KW  - Moral Responsibility
KW  - Causal Responsibility
KW  - Artificial Moral Agency(AMA)
KW  - AlphaGo
KW  - "Move 37"
KW  - Machine morality
AB  - This paper examines whether machines (algorithms/programs/ AI systems) are apt subjects for praise or blame for some actions or performances. I consider "Move 37" of AlphaGo as a case study. DeepMind's AlphaGo is an AI algorithm developed to play the game of Go. The AlphaGo utilizes Deep Neural Networks. As AlphaGo is trained through reinforcement learning, the AI algorithm can improve itself over a period of time. Such AI models can go beyond the intended task and perform novel and unpredictable functions. There is a surprise element associated with "Move 37". "Move 37" not only surprises the Go players, the programmers, but also whoever is informed of this unpredicted move. Does someone or something deserve praise or blame for the surprise? If so, who or what deserves the praise or blame for "Move 37"? The programmer cannot be praised for "Move 37", which is either surprising or was not intended or imagined at all. At the same time, would we accept that neither the algorithm deserves praise for the unpredicted move that the algorithm allowed the program to make? From this, would we accept that since neither the programmer nor the algorithm/AI system deserves the praise, there is such a good or exciting move for which no one or nothing could be praised? Would we say this unpredictable move is a move for which no one deserves praise or blame? Wouldn't there be at least a few who were surprised by the unpredictable move? Should we say that for this pleasant surprise, no one deserves praise? Nonetheless, for us, specifically regarding the particular unpredictable move, we firmly find it counterintuitive to say that there is an exciting move for which no one deserves praise. The surprise element is the result of the property that belongs to the algorithm. It seems quite difficult for us to accept that no one deserves praise for "Move 37" or for similar moves. Therefore, someone or something deserves praise which is a matter of scrutiny.
AD  - Indian Inst Technol Dharwad, Dept Humanities & Social Sci, Dharwad, Karnataka, IndiaC3  - Indian Institute of Technology System (IIT System)C3  - Indian Institute of Technology (IIT) - DharwadFU  - Technology Innovation Hub on Autonomous Navigation and Data Acquisition Systems (TiHAN) of the Indian Institute of Technology-Hyderabad, a project under the Department of Science and Technology's National Mission on Interdisciplinary Cyber-Physical Systems
FX  - I am immensely grateful to my supervisor, Prof. Jolly Thomas (Department of Humanities and Social Sciences, Indian Institute of Technology Dharwad), and co-supervisor Prof, Don Wallace Freeman Dcruz (Department of Humanities and Social Sciences, Indian Institute of Technology Delhi), for insightful comments and recommendations for the research.
FX  - This work is supported by the Technology Innovation Hub on Autonomous Navigation and Data Acquisition Systems (TiHAN) of the Indian Institute of Technology-Hyderabad, a project under the Department of Science and Technology's National Mission on Interdisciplinary Cyber-Physical Systems.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0231-0
PY  - 2023
SP  - 977
EP  - 979
DO  - 10.1145/3600211.3604730
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:001117838100086
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  12
ER  -

TY  - CPAPER
AU  - Kowalczuk, Z
AU  - Cybulski, J
AU  - Czubenko, M
A1  - IEEE
TI  - JamesBot - an intelligent agent playing StarCraft II
T2  - 2019 24TH INTERNATIONAL CONFERENCE ON METHODS AND MODELS IN AUTOMATION AND ROBOTICS (MMAR)
LA  - English
CP  - 24th International Conference on Methods and Models in Automation and Robotics (MMAR)
KW  - machine learning
KW  - reinforcement learning
KW  - Q-Learning
KW  - StarCraft II
AB  - The most popular method for optimizing a certain strategy based on a reward is Reinforcement Learning (RL). Lately, a big challenge for this technique are computer games such as StarCraft II which is a real-time strategy game, created by Blizzard. The main idea of this game is to fight between agents and control objects on the battlefield in order to defeat the enemy. This work concerns creating an autonomous bot using reinforced learning, in particular, the Q-Learning algorithm for playing StarCraft. JamesBot consists of three parts. State Manager processes relevant information from the environment. Decision Manager consists of a table implementation of the Q-Learning algorithm, which assigns actions to states, and the epsilon-greedy strategy, which determines the behavior of the bot. In turn, Action Manager is responsible for executing commands. Testing bots involves fighting the default (simple) agent built into the game. Although JamesBot played better than the default (random) agent, it failed to gain the ability to defeat the opponent. The obtained results, however, are quite promising in terms of the possibilities of further development.
AD  - Gdansk Univ Technol, Fac Elect Telecommun & Informat, Gdansk, PolandC3  - Fahrenheit UniversitiesC3  - Gdansk University of TechnologyPU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-7281-0933-6
PY  - 2019
SP  - 105
EP  - 110
DO  - 10.1109/mmar.2019.8864611
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000556208300019
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  20
ER  -

TY  - JOUR
AU  - Dezfouli, A
AU  - Nock, R
AU  - Dayan, P
TI  - Adversarial vulnerabilities of human decision-making
T2  - PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA
LA  - English
KW  - decision-making
KW  - recurrent neural networks
KW  - reinforcement learning
KW  - TRUST
AB  - Adversarial examples are carefully crafted input patterns that are surprisingly poorly classified by artificial and/or natural neu-ral networks. Here we examine adversarial vulnerabilities in the processes responsible for learning and choice in humans. Building upon recent recurrent neural network models of choice processes, we propose a general framework for generating adversarial opponents that can shape the choices of individuals in particular decision-making tasks toward the behavioral patterns desired by the adversary. We show the efficacy of the framework through three experiments involving action selection, response inhibition, and social decision-making. We further investigate the strategy used by the adversary in order to gain insights into the vulnerabilities of human choice. The framework may find applications across behavioral sciences in helping detect and avoid flawed choice.
AD  - Commonwealth Sci & Ind Res Org CSIRO, Data61, Eveleigh, NSW 2015, AustraliaAD  - Australian Natl Univ, Canberra, ACT 0200, AustraliaAD  - Max Planck Inst Biol Cybernet, D-72076 Tubingen, GermanyAD  - Univ Tubingen, D-72074 Tubingen, GermanyC3  - Commonwealth Scientific & Industrial Research Organisation (CSIRO)C3  - Australian National UniversityC3  - Max Planck SocietyC3  - Eberhard Karls University of TubingenFU  - Max Planck Society; Humboldt Foundation; CSIRO's Machine Learning and Artificial Intelligence Future Science Platform
FX  - We are grateful to Yonatan Loewenstein for discussions. P.D. was funded by the Max Planck Society and the Humboldt Foundation. This research was funded partially by CSIRO's Machine Learning and Artificial Intelligence Future Science Platform.
PU  - NATL ACAD SCIENCES
PI  - WASHINGTON
PA  - 2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA
SN  - 0027-8424
J9  - P NATL ACAD SCI USA
JI  - Proc. Natl. Acad. Sci. U. S. A.
DA  - NOV 17
PY  - 2020
VL  - 117
IS  - 46
SP  - 29221
EP  - 29228
DO  - 10.1073/pnas.2016921117
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000591351800005
N1  - Times Cited in Web of Science Core Collection:  12
Total Times Cited:  14
Cited Reference Count:  24
ER  -

TY  - CPAPER
AU  - Zhong, HX
AU  - Wang, YZ
AU  - Tu, CC
AU  - Zhang, TY
AU  - Liu, ZY
AU  - Sun, MS
A1  - Assoc Advancement Artificial Intelligence
TI  - Iteratively Questioning and Answering for Interpretable Legal Judgment Prediction
T2  - THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
LA  - English
CP  - 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence
AB  - Legal Judgment Prediction (UP) aims to predict judgment results according to the facts of cases. In recent years, LJP has drawn increasing attention rapidly from both academia and the legal industry, as it can provide references for legal practitioners and is expected to promote judicial justice. However, the research to date usually suffers from the lack of interpretability, which may lead to ethical issues like inconsistent judgments or gender bias. In this paper, we present QAjudge, a model based on reinforcement learning to visualize the prediction process and give interpretable judgments. QAjudge follows two essential principles in legal systems across the world: Presumption of Innocence and Elemental Trial. During inference, a Question Net will select questions from the given set and an Answer Net will answer the question according to the fact description. Finally, a Predict Net will produce judgment results based on the answers. Reward functions are designed to minimize the number of questions asked. We conduct extensive experiments on several realworld datasets. Experimental results show that QAjudge can provide interpretable judgments while maintaining comparable performance with other state-of-the-art LJP models. The codes can be found from https://github.com/thunlp/QAjudge.
AD  - Tsinghua Univ, Inst Artificial Intelligence, Dept Comp Sci & Technol, Beijing, Peoples R ChinaAD  - Beijing Powerlaw Intelligent Technol Co Ltd, Beijing, Peoples R ChinaC3  - Tsinghua UniversityFU  - National Key Research and Development Program of China [2018YFC0831900]; National Natural Science Foundation of China [61572273, 61661146007]
FX  - This work is supported by the National Key Research and Development Program of China (No. 2018YFC0831900) and the National Natural Science Foundation of China (NSFC No. 61572273, 61661146007).
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-835-0
J9  - AAAI CONF ARTIF INTE
PY  - 2020
VL  - 34
SP  - 1250
EP  - 1257
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000667722801040
N1  - Times Cited in Web of Science Core Collection:  35
Total Times Cited:  36
Cited Reference Count:  41
ER  -

TY  - JOUR
AU  - Aguilera, A
AU  - Figueroa, CA
AU  - Hernandez-Ramos, R
AU  - Sarkar, U
AU  - Cemballi, A
AU  - Gomez-Pathak, L
AU  - Miramontes, J
AU  - Yom-Tov, E
AU  - Chakraborty, B
AU  - Yan, XX
AU  - Xu, J
AU  - Modiri, A
AU  - Aggarwal, J
AU  - Williams, JJ
AU  - Lyles, CR
TI  - mHealth app using machine learning to increase physical activity in diabetes and depression: clinical trial protocol for the DIAMANTE Study
T2  - BMJ OPEN
LA  - English
KW  - telemedicine
KW  - health informatics
KW  - depression & mood disorders
KW  - diabetes & endocrinology
KW  - ACTIVITY INTERVENTIONS
KW  - ADULTS
KW  - STEPS
AB  - Introduction Depression and diabetes are highly disabling diseases with a high prevalence and high rate of comorbidity, particularly in low-income ethnic minority patients. Though comorbidity increases the risk of adverse outcomes and mortality, most clinical interventions target these diseases separately. Increasing physical activity might be effective to simultaneously lower depressive symptoms and improve glycaemic control. Self-management apps are a cost-effective, scalable and easy access treatment to increase physical activity. However, cutting-edge technological applications often do not reach vulnerable populations and are not tailored to an individual's behaviour and characteristics. Tailoring of interventions using machine learning methods likely increases the effectiveness of the intervention. Methods and analysis In a three-arm randomised controlled trial, we will examine the effect of a text-messaging smartphone application to encourage physical activity in low-income ethnic minority patients with comorbid diabetes and depression. The adaptive intervention group receives messages chosen from different messaging banks by a reinforcement learning algorithm. The uniform random intervention group receives the same messages, but chosen from the messaging banks with equal probabilities. The control group receives a weekly mood message. We aim to recruit 276 adults from primary care clinics aged 18-75 years who have been diagnosed with current diabetes and show elevated depressive symptoms (Patient Health Questionnaire depression scale-8 (PHQ-8) >5). We will compare passively collected daily step counts, self-report PHQ-8 and most recent haemoglobin A1c from medical records at baseline and at intervention completion at 6-month follow-up. Ethics and dissemination The Institutional Review Board at the University of California San Francisco approved this study (IRB: 17-22608). We plan to submit manuscripts describing our user-designed methods and testing of the adaptive learning algorithm and will submit the results of the trial for publication in peer-reviewed journals and presentations at (inter)-national scientific meetings.
AD  - Univ Calif Berkeley, Sch Social Welf, Berkeley, CA 94720 USAAD  - Zuckerberg San Francisco Gen Hosp, Div Gen Internal Med San Francisco, UCSF Ctr Vulnerable Populat, San Francisco, CA USAAD  - Microsoft Res, Herzliyya, IsraelAD  - Duke Natl Univ Singapore, Med Sch, Ctr Quantitat Med, Singapore, SingaporeAD  - Natl Univ Singapore, Dept Stat & Appl Probabil, Singapore, SingaporeAD  - Duke Univ, Dept Biostat & Bioinformat, Durham, NC USAAD  - Univ Toronto, Comp Sci, Toronto, ON, CanadaC3  - University of California SystemC3  - University of California BerkeleyC3  - National University of SingaporeC3  - National University of SingaporeC3  - Duke UniversityC3  - University of TorontoFU  - Agency for Healthcare Research and Quality [1R01 HS25429-01]
FX  - This trial is funded by an R01 to Dr. Adrian Aguilera and Dr. Lyles, 1R01 HS25429-01 from the Agency for Healthcare Research and Quality.
PU  - BMJ PUBLISHING GROUP
PI  - LONDON
PA  - BRITISH MED ASSOC HOUSE, TAVISTOCK SQUARE, LONDON WC1H 9JR, ENGLAND
SN  - 2044-6055
J9  - BMJ OPEN
JI  - BMJ Open
PY  - 2020
VL  - 10
IS  - 8
C7  - e034723
DO  - 10.1136/bmjopen-2019-034723
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)AN  - WOS:000564351500072
N1  - Times Cited in Web of Science Core Collection:  45
Total Times Cited:  45
Cited Reference Count:  49
ER  -

TY  - JOUR
AU  - Trenkwalder, LM
AU  - López-Incera, A
AU  - Nautrup, HP
AU  - Flamini, F
AU  - Briegel, HJ
TI  - Automated gadget discovery in the quantum domain
T2  - MACHINE LEARNING-SCIENCE AND TECHNOLOGY
LA  - English
KW  - reinforcement learning
KW  - machine learning
KW  - sequence mining
KW  - quantum optics
KW  - quantum information
KW  - REINFORCEMENT
KW  - ENTANGLEMENT
KW  - LEVEL
KW  - GO
AB  - In recent years, reinforcement learning (RL) has become increasingly successful in its application to the quantum domain and the process of scientific discovery in general. However, while RL algorithms learn to solve increasingly complex problems, interpreting the solutions they provide becomes ever more challenging. In this work, we gain insights into an RL agent's learned behavior through a post-hoc analysis based on sequence mining and clustering. Specifically, frequent and compact subroutines, used by the agent to solve a given task, are distilled as gadgets and then grouped by various metrics. This process of gadget discovery develops in three stages: First, we use an RL agent to generate data, then, we employ a mining algorithm to extract gadgets and finally, the obtained gadgets are grouped by a density-based clustering algorithm. We demonstrate our method by applying it to two quantum-inspired RL environments. First, we consider simulated quantum optics experiments for the design of high-dimensional multipartite entangled states where the algorithm finds gadgets that correspond to modern interferometer setups. Second, we consider a circuit-based quantum computing environment where the algorithm discovers various gadgets for quantum information processing, such as quantum teleportation. This approach for analyzing the policy of a learned agent is agent and environment agnostic and can yield interesting insights into any agent's policy.
AD  - Univ Innsbruck, Inst Theoret Phys, A-6020 Innsbruck, AustriaAD  - Univ Konstanz, Dept Philosophy, D-78457 Constance, GermanyC3  - University of InnsbruckC3  - University of KonstanzFU  - Austrian Science Fund (FWF) [DK-ALM: W1259-N27]; SFB [F7102]; Volkswagen Foundation [Az:97721]; European Union [801110, 885567]; Austrian Federal Ministry of Education, Science and Research (BMBWF); European Union (ERC) [101055129]; European Research Council (ERC) [101055129] Funding Source: European Research Council (ERC); Marie Curie Actions (MSCA) [885567] Funding Source: Marie Curie Actions (MSCA)
FX  - We acknowledge support by the Austrian Science Fund (FWF) through the DK-ALM: W1259-N27 and SFB BeyondC F7102, and by the Volkswagen Foundation (Az:97721). This project has received funding from the European Union's Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement, Grant Nos. 801110 and 885567. It reflects only the author's view, the EU Agency is not responsible for any use that may be made of the information it contains. ESQ has received funding from the Austrian Federal Ministry of Education, Science and Research (BMBWF). This work was also supported by the European Union (ERC, QuantAI, Project No. 101055129). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.
PU  - IOP Publishing Ltd
PI  - BRISTOL
PA  - TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND
SN  - 2632-2153
J9  - MACH LEARN-SCI TECHN
JI  - Mach. Learn.-Sci. Technol.
DA  - SEP 1
PY  - 2023
VL  - 4
IS  - 3
C7  - 035043
DO  - 10.1088/2632-2153/acf098
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001060803500001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  65
ER  -

TY  - CPAPER
AU  - Shea-Blymyer, C
AU  - Abbas, H
A1  - ACM
TI  - Generating Deontic Obligations From Utility-Maximizing Systems
T2  - PROCEEDINGS OF THE 2022 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, AIES 2022
LA  - English
CP  - AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - Machine ethics
KW  - normative systems
KW  - deontic logic
KW  - model checking
KW  - explainability
AB  - This work gives a logical characterization of the (ethical and social) obligations of an agent trained with Reinforcement Learning (RL). An RL agent takes actions by following a utility-maximizing policy. We maintain that the choice of utility function embeds ethical and social values implicitly, and that it is necessary to make these values explicit. This work provides a basis for doing so. First, we propose a probabilistic deontic logic that is suited for formally specifying the obligations of a stochastic system, including its ethical obligations. We prove some useful validities about this logic, and how its semantics are compatible with those of Markov Decision Processes (MDPs). Second, we show that model checking allows us to prove that an agent has a given obligation to bring about some state of affairs - meaning that by acting optimally, it is seeking to reach that state of affairs. We develop a model checker for our logic against MDPs. Third, we observe that it is useful for a system designer to obtain a logical characterization of her system's obligations, which is potentially more interpretable and helpful in debugging than the expression of a utility function. Enumerating all the obligations of an agent is impractical, so we propose a Bayesian optimization routine that learns to generate a system's obligations that the system designer deems interesting. We implement the model checking and Bayesian optimization routines, and demonstrate their effectiveness with an initial pilot study. This work provides a rigorous method to characterize utility-maximizing agents in terms of the (ethical and social) obligations that they implicitly seek to satisfy.
AD  - Oregon State Univ, Corvallis, OR 97331 USAC3  - Oregon State UniversityFU  - NSF [1925652]
FX  - This work was partially supported by NSF Grant 1925652. We thank Rhian Preston and Alexandra Bacula for their advice on the pilot study; any problems with our study are our own.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-9247-1
PY  - 2022
SP  - 653
EP  - 663
DO  - 10.1145/3514094.3534163
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:001118017500064
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  22
ER  -

TY  - CPAPER
AU  - Turner, AM
AU  - Hadfield-Menell, D
AU  - Tadepalli, P
A1  - ACM
TI  - Conservative Agency via Attainable Utility Preservation
T2  - PROCEEDINGS OF THE 3RD AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY AIES 2020
LA  - English
CP  - 3rd AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - reinforcement learning
KW  - side effects
KW  - AI alignment
KW  - reward specification
AB  - Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.
AD  - Oregon State Univ, Corvallis, OR 97331 USAAD  - Univ Calif Berkeley, Berkeley, CA USAC3  - Oregon State UniversityC3  - University of California SystemC3  - University of California BerkeleyFU  - Berkeley Existential Risk Initiative
FX  - This work was supported by the Center for Human-Compatible AI and the Berkeley Existential Risk Initiative. We thank Thomas Dietterich, Alan Fern, Adam Gleave, Victoria Krakovna, Matthew Rahtz, and Cody Wild for their feedback, and are grateful for the preparatory assistance of Phillip Bindeman, Alison Bowden, and Neale Ratzlaff.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-7110-0
PY  - 2020
SP  - 385
EP  - 391
DO  - 10.1145/3375627.3375851
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:001108779100070
N1  - Times Cited in Web of Science Core Collection:  5
Total Times Cited:  7
Cited Reference Count:  27
ER  -

TY  - CPAPER
AU  - Lerer, A
AU  - Peysakhovich, A
A1  - Assoc Comp Machinery
TI  - Learning Existing Social Conventions via Observationally Augmented Self-Play
T2  - AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY
LA  - English
CP  - 2nd AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - coordination
KW  - game theory
KW  - deep reinforcement learning
KW  - social conventions
KW  - self-play
KW  - GAME
AB  - In order for artificial agents to coordinate effectively with people, they must act consistently with existing conventions (e.g. how to navigate in traffic, which language to speak, or how to coordinate with teammates). A group's conventions can be viewed as a choice of equilibrium in a coordination game. We consider the problem of an agent learning a policy for a coordination game in a simulated environment and then using this policy when it enters an existing group. When there are multiple possible conventions we show that learning a policy via multi-agent reinforcement learning (MARL) is likely to find policies which achieve high payoffs at training time but fail to coordinate with the real group into which the agent enters. We assume access to a small number of samples of behavior from the true convention and show that we can augment the MARL objective to help it find policies consistent with the real group's convention. In three environments from the literature - traffic, communication, and team coordination - we observe that augmenting MARL with a small amount of imitation learning greatly increases the probability that the strategy found by MARL fits well with the existing social convention. We show that this works even in an environment where standard training methods very rarely find the true convention of the agent's partners.
AD  - Facebook AI Res, New York, NY 10011 USAC3  - Facebook IncPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-6324-2
PY  - 2019
SP  - 107
EP  - 114
DO  - 10.1145/3306618.3314268
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000556121100016
N1  - Times Cited in Web of Science Core Collection:  8
Total Times Cited:  8
Cited Reference Count:  39
ER  -

TY  - JOUR
AU  - Fragkos, G
AU  - Minwalla, C
AU  - Tsiropoulou, EE
AU  - Plusquellic, J
TI  - Enhancing Privacy in PUF-Cash through Multiple Trusted Third Parties and Reinforcement Learning
T2  - ACM JOURNAL ON EMERGING TECHNOLOGIES IN COMPUTING SYSTEMS
LA  - English
KW  - Internet of things
KW  - security
KW  - electronic money
KW  - digital currency
KW  - networks
AB  - Electronic cash (e-Cash) is a digital alternative to physical currency such as coins and bank notes. Suitably constructed, e-Cash has the ability to offer an anonymous offline experience much akin to cash, and in direct contrast to traditional forms of payment such as credit and debit cards. Implementing security and privacy within e-Cash, i.e., preserving user anonymity while preventing counterfeiting, fraud, and double spending, is a non-trivial challenge. In this article, we propose major improvements to an e-Cash protocol, termed PUF-Cash, based on physical unclonable functions (PUFs). PUF-Cash was created as an offline-first, secure e-Cash scheme that preserved user anonymity in payments. In addition, PUF-Cash supports remote payments; an improvement over traditional currency. In this work, a novel multi-trusted-third-party exchange scheme is introduced, which is responsible for "blinding" Alice's e-Cash tokens; a feature at the heart of preserving her anonymity. The exchange operations are governed by machine learning techniques which are uniquely applied to optimize user privacy, while remaining resistant to identity-revealing attacks by adversaries and trusted authorities. Federation of the single trusted third party into multiple entities distributes the workload, thereby improving performance and resiliency within the e-Cash system architecture. Experimental results indicate that improvements to PUF-Cash enhance user privacy and scalability.
AD  - Univ New Mexico, POB 1212, Albuquerque, NM USAAD  - Bank Canada, 234 Wellington St, Ottawa, ON K1A OG9, CanadaC3  - University of New MexicoC3  - Bank of CanadaPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN  - 1550-4832
SN  - 1550-4840
J9  - ACM J EMERG TECH COM
JI  - ACM J. Emerg. Technol. Comput. Syst.
DA  - JAN
PY  - 2022
VL  - 18
IS  - 1
C7  - 7
DO  - 10.1145/3441139
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000908213900007
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  7
Cited Reference Count:  25
ER  -

TY  - JOUR
AU  - Lopez-Gazpio, I
TI  - Gaining Student Engagement Through Project-Based Learning: A Competitive 2D Game Construction Case Study
T2  - IEEE ACCESS
LA  - English
KW  - Games
KW  - Education
KW  - Learning (artificial intelligence)
KW  - Informatics
KW  - Faces
KW  - Engines
KW  - Task analysis
KW  - Action research
KW  - active learning computing education
KW  - cross-disciplinary skills
KW  - engagement
KW  - game construction
KW  - project-based learning (PBL)
KW  - reinforcement learning
KW  - DELIBERATE PRACTICE
KW  - COMPUTER-SCIENCE
KW  - EDUCATION
KW  - SIMULATION
KW  - IMPACT
AB  - In this work we consider an open artificial intelligence game as a matter of study within the lectures of artificial intelligence to combat lack of motivation and increase engagement within the classroom. During formation, students in computer science can deal with moderately complex projects, nevertheless, dealing with such problems is relegated to the Degree Final Project. In this investigation we show the procedural steps of how project-based learning combined with game construction can effectively be used to promote engagement in informatic lectures at university. For the task, we build a 2D game engine and propose students to enroll in factitious research teams with the aim of programming intelligent agents that play the game employing artificial intelligence techniques. The intended principal outcome is to show evidence of the application of project-based learning in artificial intelligence within the lectures, and how it can be combined with game construction to increase motivation in the classroom. Project-based learning has the students learn, organize, and solve challenges while students themselves remain their own responsible for the investigation and process of work. We propose to follow a series of sequential phases that conform a set of milestones that incorporate a project-based learning approach to the lectures. Through this work we show that the use of project-based learning combined with game construction provides reliable evidence that a much deeper understanding about artificial intelligence is attained by students participating in the challenge. Student evaluation questionnaires and final grade results attained by students indicate that students remained more engaged during the semester in comparison to previous semesters in which lack of motivation was reported.
AD  - Univ Deusto, Dept Jnformat Elect & Commun Technol, Donostia San Sebastian 20012, SpainC3  - University of DeustoPU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
J9  - IEEE ACCESS
JI  - IEEE Access
PY  - 2022
VL  - 10
SP  - 1881
EP  - 1892
DO  - 10.1109/ACCESS.2021.3139764
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)AN  - WOS:000739991900001
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  48
ER  -

TY  - JOUR
AU  - Wei, Y
AU  - Pan, L
AU  - Liu, SJ
AU  - Wu, L
AU  - Meng, XX
TI  - DRL-Scheduling: An Intelligent QoS-Aware Job Scheduling Framework for Applications in Clouds
T2  - IEEE ACCESS
LA  - English
KW  - Cloud computing
KW  - deep Q-Learning
KW  - job scheduling
KW  - QoS
KW  - reinforcement learning
AB  - As an increasing number of traditional applications migrated to the cloud, achieving resource management and performance optimization in such a dynamic and uncertain environment becomes a big challenge for cloud-based application providers. In particular, job scheduling is a non-trivial task, which is responsible for allocating massive job requests submitted by users to the most suitable resources and satisfying user QoS requirements as much as possible. Inspired by recent success of using deep reinforcement learning techniques to solve AI control problems, in this paper, we propose an intelligent QoS-aware job scheduling framework for application providers. A deep reinforcement learning-based job scheduler is the key component of the framework. It is able to learn to make appropriate online job-to-VM decisions for continuous job requests directly from its experiences without any prior knowledge. Experimental results using synthetic workloads and real-world NASA workload traces show that compared with other baseline solutions, our proposed job scheduling approach can efficiently reduce average job response time (e.g., reduced by 40.4% compared with the best baseline for NASA traces), guarantee the QoS at a high level (e.g., job success rate is higher than 93% for all simulated changing workload scenarios), and adapt to different workload conditions.
AD  - Shandong Univ, Sch Software, Jinan 25101, Shandong, Peoples R ChinaC3  - Shandong UniversityFU  - National Natural Science Foundation of China [61872222]; National Key Research and Development Program of China [2017YFA0700601]; Key Research and Development Program of Shandong Province [2017CXGC0605, 2017CXGC0604, 2018GGX101019]; Shandong University; Special Funds of Taishan Scholar Construction Project
FX  - This work was supported in part by the National Natural Science Foundation of China under Grant 61872222, in part by the National Key Research and Development Program of China under Grant 2017YFA0700601, in part by the Key Research and Development Program of Shandong Province under Grant 2017CXGC0605, Grant 2017CXGC0604, and Grant 2018GGX101019, in part by Shandong University through the Young Scholars Program, and in part by the Special Funds of Taishan Scholar Construction Project.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
J9  - IEEE ACCESS
JI  - IEEE Access
PY  - 2018
VL  - 6
SP  - 55112
EP  - 55125
DO  - 10.1109/ACCESS.2018.2872674
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000448110800001
N1  - Times Cited in Web of Science Core Collection:  56
Total Times Cited:  59
Cited Reference Count:  38
ER  -

TY  - JOUR
AU  - Chen, JZ
AU  - Zhang, DL
AU  - Qu, ZS
AU  - Wang, CH
TI  - Modeling adaptive empathy based on neutral assessment: a way to enhance the prosocial behaviors of socialized agents under the premise of self-security
T2  - APPLIED INTELLIGENCE
LA  - English
KW  - Adaptive empathy
KW  - Prosocial behaviors
KW  - Multi-agent reinforcement learning
KW  - Cooperation and competition
KW  - DECISION-MAKING
KW  - EVOLUTION
KW  - HAPPINESS
KW  - ALTRUISM
KW  - EMOTION
KW  - INCOME
KW  - GAME
AB  - Ethical concerns over artificial intelligence (AI) have recently drawn extensive interest in both academia and industry. According to behavioral economics and neuropsychology, empathy may be an inherent mechanism to elicit prosocial behaviors. Therefore, we establish a three-layer general framework of empathetic AI (eAI) with emotion, empathy, and decision-making. By introducing different sub-models, three learning structures based on eAI are proposed, including a gradient ascent (GA)-based structure, an adaptive learning structure, and a practical learning structure. The dynamics of the first two structures are analyzed theoretically, and the practical dynamics are tested in games. We prove that in the prisoner's dilemma (PD) environment, the GA-based eAI with neutral assessment can carry out adaptive cooperation and competition under the premise of self-security. In addition, although the modeling of empathy by extracting the emotional contagion and limited cognitive regulation is simplified and primitive, tests in the prisoner's dilemma, the ultimatum game, and a multi-agent dilemma game, show that the eAI structure successfully elicits prosocial behaviors including altruism, cooperation and fairness. Compared with other socialized algorithms, the eAI structure has a more comprehensive coverage in terms of convergence, fairness, security, adaptability, and structural expansibility. Therefore, we believe this work can provide novel methods and insights for regulating the behaviors of socialized agents, as well as artificial subjects in psychological and economic experiments.
AD  - Harbin Inst Technol, Space Sci & Inertial Technol Res Ctr, Harbin 150001, Peoples R ChinaC3  - Harbin Institute of TechnologyPU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0924-669X
SN  - 1573-7497
J9  - APPL INTELL
JI  - Appl. Intell.
DA  - APR
PY  - 2022
VL  - 52
IS  - 6
SP  - 6692
EP  - 6722
DO  - 10.1007/s10489-021-02712-9
C6  - SEP 2021
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)AN  - WOS:000695791200001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  62
ER  -

TY  - JOUR
AU  - Wei, XX
AU  - Wang, SP
AU  - Yan, HQ
TI  - Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA  - English
KW  - Adversarial examples
KW  - black-box attacks
KW  - reinforcement learning
KW  - spatial-temporal analysis
KW  - video recognition
AB  - Adversarial robustness assessment for video recognition models has raised concerns owing to their wide applications on safety-critical tasks. Compared with images, videos have much high dimension, which brings huge computational costs when generating adversarial videos. This is especially serious for the query-based black-box attacks where gradient estimation for the threat models is usually utilized, and high dimensions will lead to a large number of queries. To mitigate this issue, we propose to simultaneously eliminate the temporal and spatial redundancy within the video to achieve an effective and efficient gradient estimation on the reduced searching space, and thus query number could decrease. To implement this idea, we design the novel Adversarial spatial-temporal Focus (AstFocus) attack on videos, which performs attacks on the simultaneously focused key frames and key regions from the inter-frames and intra-frames in the video. AstFocus attack is based on the cooperative Multi-Agent Reinforcement Learning (MARL) framework. One agent is responsible for selecting key frames, and another agent is responsible for selecting key regions. These two agents are jointly trained by the common rewards received from the black-box threat models to perform a cooperative prediction. By continuously querying, the reduced searching space composed of key frames and key regions is becoming precise, and the whole query number becomes less than that on the original video. Extensive experiments on four mainstream video recognition models and three widely used action recognition datasets demonstrate that the proposed AstFocus attack outperforms the SOTA methods, which is prevenient in fooling rate, query number, time, and perturbation magnitude at the same time.
AD  - Beihang Univ, Inst Artificial Intelligence, Beijing 100191, Peoples R ChinaC3  - Beihang UniversityFU  - National Key R amp; D Program of China [2020AAA0104002]; National Natural Science Foundation of China [62076018]
FX  - This work was supported in part by National Key R & D Program of China under Grant 2020AAA0104002, and in part by National Natural Science Foundation of China under Grant 62076018. Recommended for acceptance by G. Hua.& nbsp;
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
J9  - IEEE T PATTERN ANAL
JI  - IEEE Trans. Pattern Anal. Mach. Intell.
DA  - SEPT 1
PY  - 2023
VL  - 45
IS  - 9
SP  - 10898
EP  - 10912
DO  - 10.1109/TPAMI.2023.3262592
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001045832200019
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  50
ER  -

TY  - CPAPER
AU  - Chowdhury, SR
AU  - Zhou, XY
A1  - Assoc Advancement Artificial Intelligence
TI  - Differentially Private Regret Minimization in Episodic Markov Decision Processes
T2  - THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
LA  - English
CP  - 36th AAAI Conference on Artificial Intelligence / 34th Conference on Innovative Applications of Artificial Intelligence / 12th Symposium on Educational Advances in Artificial Intelligence
KW  - ALGORITHMS
AB  - We study regret minimization in finite horizon tabular Markov decision processes (MDPs) under the constraints of differential privacy (DP). This is motivated by the widespread applications of reinforcement learning (RL) in real-world sequential decision making problems, where protecting users' sensitive and private information is becoming paramount. We consider two variants of DP - joint DP (JDP), where a centralized agent is responsible for protecting users' sensitive data and local DP (LDP), where information needs to be protected directly on the user side. We first propose two general frameworks - one for policy optimization and another for value iteration - for designing private, optimistic RL algorithms. We then instantiate these frameworks with suitable privacy mechanisms to satisfy JDP and LDP requirements, and simultaneously obtain sublinear regret guarantees. The regret bounds show that under JDP, the cost of privacy is only a lower order additive term, while for a stronger privacy protection under LDP, the cost suffered is multiplicative. Finally, the regret bounds are obtained by a unified analysis, which, we believe, can be extended beyond tabular MDPs.
AD  - Indian Inst Sci, Bangalore, Karnataka, IndiaAD  - Wayne State Univ, ECE Dept, Detroit, MI USAC3  - Indian Institute of Science (IISC) - BangaloreC3  - Wayne State UniversityPU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-876-3
J9  - AAAI CONF ARTIF INTE
PY  - 2022
SP  - 6375
EP  - 6383
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000893636206054
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  58
ER  -

TY  - JOUR
AU  - Requena, B
AU  - Muñoz-Gil, G
AU  - Lewenstein, M
AU  - Dunjko, V
AU  - Tura, J
TI  - Certificates of quantum many-body properties assisted by machine learning
T2  - PHYSICAL REVIEW RESEARCH
LA  - English
KW  - MATRIX PRODUCT STATES
KW  - PHASE-TRANSITIONS
KW  - GROUND-STATES
KW  - ENTANGLEMENT
KW  - NONLOCALITY
KW  - ALGORITHM
KW  - NETWORKS
KW  - ENERGY
KW  - OPTIMIZATION
KW  - COMPLEXITY
AB  - Computationally intractable tasks are often encountered in physics and optimization. They usually comprise a cost function to be optimized over a so-called feasible set, which is specified by a set of constraints. This may yield, in general, to difficult and nonconvex optimization tasks. A number of standard methods are used to tackle such problems: variational approaches focus on parametrizing a subclass of solutions within the feasible set. In contrast, relaxation techniques have been proposed to approximate it from outside, thus complementing the variational approach to provide ultimate bounds to the global optimal solution. In this paper, we propose a novel approach combining the power of relaxation techniques with deep reinforcement learning in order to find the best possible bounds within a limited computational budget. We illustrate the viability of the method in two paradigmatic problems in quantum physics and quantum information processing: finding the ground state energy of many-body quantum systems, and building energy-based entanglement witnesses of quantum local Hamiltonians. We benchmark our approach against other classical optimization algorithms such as breadth-first search or Monte Carlo, and we characterize the effect of transfer learning. We find the latter may be indicative of phase transitions with a completely autonomous approach. Finally, we provide tools to tackle other common applications in the field of quantum information processing with our method.
AD  - Barcelona Inst Sci & Technol, ICFO Inst Ciencies Foton, Ave Carl Friedrich Gauss 3, Castelldefels 08860, Barcelona, SpainAD  - Univ Innsbruck, Inst Theoret Phys, Technikerstr 21a, A-6020 Innsbruck, AustriaAD  - ICREA, Pg Lluis Co 23, Barcelona 08010, SpainAD  - Leiden Univ, LIACS, Niels Bohrweg 1, NL-2333 CA Leiden, NetherlandsAD  - Max Planck Inst Quantum Opt, Hans Kopfermann Str 1, D-85748 Garching, GermanyAD  - Leiden Univ, Inst Lorentz, POB 9506, NL-2300 RA Leiden, NetherlandsC3  - Barcelona Institute of Science & TechnologyC3  - Universitat Politecnica de CatalunyaC3  - Institut de Ciencies Fotoniques (ICFO)C3  - University of InnsbruckC3  - ICREAC3  - Leiden University - Excl LUMCC3  - Leiden UniversityC3  - Max Planck SocietyC3  - Leiden University - Excl LUMCC3  - Leiden UniversityFU  - ERC AdG NOQIA; Agencia Estatal de Investigacion [PGC2018-097027-B-I00, CEX2019-000910-S/10.13039/501100011033, PID2019-106901GB-I00, PCI2019-111828-2, PCI2022-132919, RTC2019-007196-7]; MICIIN; European Union [PRTR-C17.I1, 101029393, 847648, ID100010434, LCF/BQ/PI19/11690013, LCF/BQ/PI20/11760031, LCF/BQ/PR20/11770012, LCF/BQ/PR21/11840013]; Generalitat de Catalunya; Fundacio Privada Cellex; Fundacio Mir-Puig; Generalitat de Catalunya (European Social Fund FEDER); Generalitat de Catalunya (CERCA program, AGAUR Grant) [2021 SGR 01452, U16-011424]; Generalitat de Catalunya (ERDF Operational Program of Catalonia 2014-2020); Barcelona Supercomputing Center MareNostrum [FI-2022-1-0042]; EU Horizon 2020 FET-OPEN OPTOLogic [899794]; EU Horizon Europe Program [101080086]; National Science Centre, Poland [2016/20/W/ST4/00314]; ICFO Internal "QuantumGaudi" project; Austrian Science Fund (FWF) through SFB BeyondC [F7102]; Fundacio Obra Social la Caixa (LCF-ICFO grant); Alexander von Humboldt foundation; Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) [414325145, SFB F7104]; European Union's Horizon 2020 research and innovation programme [899354]; European Union's Horizon Europe program through the ERC StG FINE-TEA-SQUAD [101040729]; Quantum Delta program; 'Quantum Inspire - the Dutch Quantum Computer in the Cloud' project of the NWA research program 'Research on Routes by Consortia (ORC)' - Netherlands Organization for Scientific Research (NWO) [NWA.1292.19.194]; Dutch Research Council (NWO/OCW), as part of the Quantum Software Consortium programme [024.003.037]; Horizon Europe - Pillar II [101080086] Funding Source: Horizon Europe - Pillar II; European Research Council (ERC) [101040729] Funding Source: European Research Council (ERC)
FX  - We thank A. Acin, F. Alet, F. Baccari, M. Lubasch, and N. Pancotti for enlightening discussions. We acknowledge the contribution of Aina Guirao to the design of the figures. B.R., G.M-G., and M.L. acknowledge support from ERC AdG NOQIA, Agencia Estatal de Investigacion (PGC2018-097027-B-I00/10.13039/501100011033, CEX2019-000910-S/10.13039/501100011033, Plan National FIDEUA PID2019-106901GB-I00, FPI, QUANTERA MAQS PCI2019-111828-2, QUANTERA DYNAMITE PCI2022-132919, Proyectos de I+D+I "Retos Colaboracion" QUSPIN RTC2019-007196-7), MICIIN with funding from European Union NextGenerationEU(PRTR-C17.I1), and by Generalitat de Catalunya, Fundacio Privada Cellex, Fundacio Mir-Puig, Generalitat de Catalunya (European Social Fund FEDER and CERCA program, AGAUR Grant No. 2021 SGR 01452, QuantumCAT U16-011424, co-funded by ERDF Operational Program of Catalonia 2014-2020), Barcelona Supercomputing Center MareNostrum (FI-2022-1-0042), EU Horizon 2020 FET-OPEN OPTOLogic (Grant No. 899794), EU Horizon Europe Program (Grant Agreement 101080086 - NeQST), National Science Centre, Poland (Symfonia Grant No. 2016/20/W/ST4/00314), ICFO Internal "QuantumGaudi" project; European Union's Horizon 2020 research and innovation program under theMarie-Sklodowska-Curie Grant Agreement No. 101029393 (STREDCH) and No. 847648 ("La Caixa" Junior Leaders fellowships ID100010434: LCF/BQ/PI19/11690013, LCF/BQ/PI20/11760031, LCF/BQ/PR20/11770012, LCF/BQ/PR21/11840013). G.M.-G. acknowledges support from the Austrian Science Fund (FWF) through SFB BeyondC F7102 and from Fundacio Obra Social la Caixa (LCF-ICFO grant). J.T. thanks the Alexander von Humboldt foundation for support. This project has received funding from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project No. 414325145 in the framework of the Austrian Science Fund (FWF): SFB F7104. This project has received funding from the European Union's Horizon 2020 research and innovation programme under Grant Agreement No. 899354. This work has received support from the European Union's Horizon Europe program through the ERC StG FINE-TEA-SQUAD (Grant No. 101040729). The authors also acknowledge support from the Quantum Delta program. This publication is part of the 'Quantum Inspire - the Dutch Quantum Computer in the Cloud' project (with Project No. NWA.1292.19.194) of the NWA research program 'Research on Routes by Consortia (ORC)', which is funded by the Netherlands Organization for Scientific Research (NWO). This work was supported by the Dutch Research Council (NWO/OCW), as part of the Quantum Software Consortium programme (Project No. 024.003.037). Views and opinions expressed in this work are, however, those of the author(s) only and do not necessarily reflect those of the European Union, European Climate, Infrastructure and Environment Executive Agency (CINEA), nor any other granting authority. Neither the European Union nor any granting authority can be held responsible for them.
PU  - AMER PHYSICAL SOC
PI  - COLLEGE PK
PA  - ONE PHYSICS ELLIPSE, COLLEGE PK, MD 20740-3844 USA
SN  - 2643-1564
J9  - PHYS REV RES
JI  - Phys. Rev. Res.
DA  - FEB 10
PY  - 2023
VL  - 5
IS  - 1
C7  - 013097
DO  - 10.1103/PhysRevResearch.5.013097
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000950669300005
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  191
ER  -

TY  - JOUR
AU  - Killeen, BD
AU  - Cho, SM
AU  - Armand, M
AU  - Taylor, RH
AU  - Unberath, M
TI  - <i>In silico</i> simulation: a key enabling technology for next-generation intelligent surgical systems
T2  - PROGRESS IN BIOMEDICAL ENGINEERING
LA  - English
KW  - in silico virtual clinical trials
KW  - minimally invasive surgery
KW  - machine learning
KW  - reinforcement learning
KW  - endoscopy
KW  - x-ray
KW  - ultrasound
KW  - MINIMALLY INVASIVE SURGERY
KW  - PERCUTANEOUS VERTEBROPLASTY
KW  - ARTIFICIAL-INTELLIGENCE
KW  - RADIATION-EXPOSURE
KW  - SPINE SURGERY
KW  - CLOSED REDUCTION
KW  - IMAGING PHANTOM
KW  - SINUS SURGERY
KW  - ULTRASOUND
KW  - FIXATION
AB  - To mitigate the challenges of operating through narrow incisions under image guidance, there is a desire to develop intelligent systems that assist decision making and spatial reasoning in minimally invasive surgery (MIS). In this context, machine learning-based systems for interventional image analysis are receiving considerable attention because of their flexibility and the opportunity to provide immediate, informative feedback to clinicians. It is further believed that learning-based image analysis may eventually form the foundation for semi- or fully automated delivery of surgical treatments. A significant bottleneck in developing such systems is the availability of annotated images with sufficient variability to train generalizable models, particularly the most recently favored deep convolutional neural networks or transformer architectures. A popular alternative to acquiring and manually annotating data from the clinical practice is the simulation of these data from human-based models. Simulation has many advantages, including the avoidance of ethical issues, precisely controlled environments, and the scalability of data collection. Here, we survey recent work that relies on in silico training of learning-based MIS systems, in which data are generated via computational simulation. For each imaging modality, we review available simulation tools in terms of compute requirements, image quality, and usability, as well as their applications for training intelligent systems. We further discuss open challenges for simulation-based development of MIS systems, such as the need for integrated imaging and physical modeling for non-optical modalities, as well as generative patient models not dependent on underlying computed tomography, MRI, or other patient data. In conclusion, as the capabilities of in silico training mature, with respect to sim-to-real transfer, computational efficiency, and degree of control, they are contributing toward the next generation of intelligent surgical systems.
AD  - Johns Hopkins Univ, Lab Computat Sensing & Robot, 3400 N Charles St, Baltimore, MD 21218 USAC3  - Johns Hopkins UniversityPU  - IOP Publishing Ltd
PI  - BRISTOL
PA  - TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND
SN  - 2516-1091
J9  - PROG BIOMED ENG
JI  - Prog. Biomed. Eng.
DA  - JUL 1
PY  - 2023
VL  - 5
IS  - 3
C7  - 032001
DO  - 10.1088/2516-1091/acd28b
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000987842900001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  231
ER  -

TY  - CPAPER
AU  - Balakrishnan, S
AU  - Bi, JX
AU  - Soh, H
A1  - ACM
TI  - SCALES: From Fairness Principles to Constrained Decision-Making
T2  - PROCEEDINGS OF THE 2022 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, AIES 2022
LA  - English
CP  - AAAI/ACM Conference on AI, Ethics, and Society (AIES)
KW  - Fairness
KW  - Constrained Reinforcement Learning
AB  - This paper proposes SCALES, a general framework that translates well-established fairness principles into a common representation based on the Constraint Markov Decision Process (CMDP). With the help of causal language, our framework can place constraints on both the procedure of decision making (procedural fairness) as well as the outcomes resulting from decisions (outcome fairness). Specifically, we show that well-known fairness principles can be encoded either as a utility component, a non-causal component, or a causal component in a SCALES-CMDP. We illustrate SCALES using a set of case studies involving a simulated healthcare scenario and the real-world COMPAS dataset. Experiments demonstrate that our framework produces fair policies that embody alternative fairness principles in single-step and sequential decision-making scenarios.
AD  - Natl Univ Singapore, Singapore, SingaporeC3  - National University of SingaporeFU  - National Research Foundation Singapore under its AI Singapore Programme [AISG2-RP-2020-017]
FX  - This research is supported by the National Research Foundation Singapore under its AI Singapore Programme (Award Number: AISG2-RP-2020-017).
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-9247-1
PY  - 2022
SP  - 46
EP  - 55
DO  - 10.1145/3514094.3534190
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:001118017500010
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  48
ER  -

TY  - JOUR
AU  - Vamplew, P
AU  - Smith, BJ
AU  - Källström, J
AU  - Ramos, G
AU  - Radulescu, R
AU  - Roijers, DM
AU  - Hayes, CF
AU  - Heintz, F
AU  - Mannion, P
AU  - Libin, PJK
AU  - Dazeley, R
AU  - Foale, C
TI  - Scalar reward is not enough: a response to Silver, Singh, Precup and Sutton (2021)
T2  - AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS
LA  - English
KW  - Scalar rewards
KW  - Vector rewards
KW  - Artificial general intelligence
KW  - Reinforcement learning
KW  - Multi-objective decision making
KW  - Multi-objective reinforcement learning
KW  - Safe and ethical AI
KW  - PREFRONTAL CORTEX
KW  - REINFORCEMENT
KW  - MODULATION
KW  - SATIATION
KW  - DOPAMINE
AB  - The recent paper "Reward is Enough" by Silver, Singh, Precup and Sutton posits that the concept of reward maximisation is sufficient to underpin all intelligence, both natural and artificial, and provides a suitable basis for the creation of artificial general intelligence. We contest the underlying assumption of Silver et al. that such reward can be scalar-valued. In this paper we explain why scalar rewards are insufficient to account for some aspects of both biological and computational intelligence, and argue in favour of explicitly multi-objective models of reward maximisation. Furthermore, we contend that even if scalar reward functions can trigger intelligent behaviour in specific cases, this type of reward is insufficient for the development of human-aligned artificial general intelligence due to unacceptable risks of unsafe or unethical behaviour.
AD  - Federat Univ Australia, Ballarat, Vic, AustraliaAD  - Univ Oregon, Ctr Translat Neurosci, Eugene, OR 97403 USAAD  - Linkoping Univ, Linkoping, SwedenAD  - Univ Vale Rio dos Sinos, Sao Leopoldo, RS, BrazilAD  - Vrije Univ Brussel, AI Lab, Brussels, BelgiumAD  - Vrije Univ Brussel, Brussels, BelgiumAD  - HU Univ Appl Sci Utrecht, Utrecht, NetherlandsAD  - Natl Univ Ireland Galway, Galway, IrelandAD  - Univ Hasselt, Hasselt, BelgiumAD  - Katholieke Univ Leuven, Leuven, BelgiumAD  - Deakin Univ, Geelong, Vic, AustraliaC3  - Federation University AustraliaC3  - University of OregonC3  - Linkoping UniversityC3  - Universidade do Vale do Rio dos Sinos (Unisinos)C3  - Vrije Universiteit BrusselC3  - Vrije Universiteit BrusselC3  - Ollscoil na Gaillimhe-University of GalwayC3  - Hasselt UniversityC3  - KU LeuvenC3  - Deakin UniversityFU  - Flemish Government; National Cancer Institute of the U.S. National Institutes of Health [1R01CA240452-01A1]; Research Foundation Flanders (FWO) [1242021N]; Swedish Governmental Agency for Innovation Systems [NFFP7/2017-04885]; Wallenberg Artificial Intelligence, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation; National University of Ireland Galway Hardiman Scholarship; FAPERGS [19/2551-0001277-2]; FAPESP [2020/05165-1]
FX  - This research was supported by funding from the Flemish Government under the "Onderzoeksprogramma Artificiele Intelligentie (AI) Vlaanderen" program, and by the National Cancer Institute of the U.S. National Institutes of Health under Award Number 1R01CA240452-01A1. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health or of other funders. Pieter J.K. Libin acknowledges support from the Research Foundation Flanders (FWO, fwo.be) (postdoctoral fellowship 1242021N). Johan Kallstrom and Fredrik Heintz were partially supported by the Swedish Governmental Agency for Innovation Systems (Grant NFFP7/2017-04885), and the Wallenberg Artificial Intelligence, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. Conor F. Hayes is funded by the National University of Ireland Galway Hardiman Scholarship. Gabriel Ramos was partially supported by FAPERGS (Grant 19/2551-0001277-2) and FAPESP (Grant 2020/05165-1).
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1387-2532
SN  - 1573-7454
J9  - AUTON AGENT MULTI-AG
JI  - Auton. Agents Multi-Agent Syst.
DA  - OCT
PY  - 2022
VL  - 36
IS  - 2
C7  - 41
DO  - 10.1007/s10458-022-09575-5
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000826149200001
N1  - Times Cited in Web of Science Core Collection:  5
Total Times Cited:  5
Cited Reference Count:  83
ER  -

TY  - CPAPER
AU  - Hjelmeland, HW
AU  - Eriksen, BOH
AU  - Mengshoel, OJ
AU  - Lekkas, AM
TI  - Identification of Failure Modes in the Collision Avoidance System of an Autonomous Ferry using Adaptive Stress Testing
T2  - IFAC PAPERSONLINE
LA  - English
CP  - 14th IFAC Conference on Control Applications in Marine Systems, Robotics, and Vehicles (CAMS)
KW  - Safety
KW  - Simulation
KW  - Autonomous Vehicles
KW  - Reinforcement Learning
KW  - Adaptive Stress Testing
AB  - As complex autonomous systems emerge in the maritime sector, measures must be taken in order to ensure thorough safety assessment. Real-world testing can be costly and potentially dangerous, and therefore there is a need for suitable simulation-based methods. This paper presents an implementation of the Adaptive Stress Testing (AST) method applied to the collision avoidance (COLAV) system of a small passenger ferry. AST is a simulation-based technique which has shown promising results in safety assessment of aviation and automobile systems. Given a simulator of a system, AST uses reinforcement learning to optimize toward system failure, and returns the most likely failure scenarios. AST is here shown to successfully identify scenarios where the criteria for failure are met, which is when the ferry collides with an adversary vessel controlled by AST. However, most of the initial results exhibit failures where the COLAV system of the ferry is not responsible for the failure, making the results less valuable to system developers. To improve the relevance, augmentations are made to the optimization problem. The augmentations result in four distinct problem formulations presented in the paper. Finally, the results are clustered using an unsupervised machine learning method called Soft Dynamic Time Warping k-means clustering in order to present a general summary of the identified failure scenarios. Our results demonstrate the relevance and potential of AST for the maritime sector and illustrate how common drawbacks of AST can be circumvented by method adjustment. Copyright (C) 2022 The Authors.
AD  - Norwegian Univ Sci & Technol, N-7491 Trondheim, NorwayC3  - Norwegian University of Science & Technology (NTNU)FU  - Research Council of Norway [304843]
FX  - This research could not have been conducted without the use of the Python AST Toolbox, which is developed and maintained by Stanford Intelligent Systems Lab (SISL), with extra support from Ritchie Lee and Mark Koren. Resources and equipment were provided by the Department of Engineering Cybernetics at the Norwegian University of Science and Technology. The Research Council of Norway supported this work through the EXAIGON project, project number 304843.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 2405-8963
J9  - IFAC PAPERSONLINE
JI  - IFAC PAPERSONLINE
PY  - 2022
VL  - 55
IS  - 31
SP  - 470
EP  - 477
DO  - 10.1016/j.ifacol.2022.10.472
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000901514500074
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  12
ER  -

TY  - JOUR
AU  - De Paola, A
AU  - Gaglio, S
AU  - Giammanco, A
AU  - Lo Re, G
AU  - Morana, M
TI  - A multi-agent system for itinerary suggestion in smart environments
T2  - CAAI TRANSACTIONS ON INTELLIGENCE TECHNOLOGY
LA  - English
KW  - artificial intelligence
KW  - pattern recognition
KW  - HUMAN MOBILITY
AB  - Modern smart environments pose several challenges, among which the design of intelligent algorithms aimed to assist the users. When a variety of points of interest are available, for instance, trajectory recommendations are needed to suggest users the most suitable itineraries based on their interests and contextual constraints. Unfortunately, in many cases, these interests must be explicitly requested and their lack causes the so-called cold-start problem. Moreover, lengthy travelling distances and excessive crowdedness of specific points of interest make itinerary planning more difficult. To address these aspects, a multi-agent itinerary suggestion system that aims at assisting the users in an online and collaborative way is proposed. A profiling agent is responsible for the detection of groups of users whose movements are characterised by similar semantic, spatial and temporal features; then, a recommendation agent leverages contextual information and dynamically associates the current user with the trajectory clusters according to a Multi-Armed Bandit policy. Framing the trajectory recommendation as a reinforcement learning problem permits to provide high-quality suggestions while avoiding both cold-start and preference elicitation issues. The effectiveness of the approach is demonstrated by some deployments in real-life scenarios, such as smart campuses and theme parks.
AD  - Univ Palermo, I-90128 Palermo, ItalyAD  - CINI Consorzio Interuniv Nazl Informat, Smart Cities & Commun Natl Lab, Palermo, ItalyC3  - University of PalermoFU  - Project SmartEP, P.O. F.E.S.R.
FX  - This research is partially funded by the Project SmartEP, P.O. F.E.S.R. 2014/2020 Regione Siciliana.
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN  - 2468-6557
SN  - 2468-2322
J9  - CAAI T INTELL TECHNO
JI  - CAAI T. Intell. Technol.
DA  - DEC
PY  - 2021
VL  - 6
IS  - 4
SP  - 377
EP  - 393
DO  - 10.1049/cit2.12056
C6  - SEP 2021
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000693664400001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  66
ER  -

TY  - CPAPER
AU  - Huang, WL
AU  - Mordatch, I
AU  - Pathak, D
ED  - Daume, H
ED  - Singh, A
TI  - One Policy to Control Them All: Shared Modular Policies for Agent-Agnostic Control
T2  - INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 119
LA  - English
CP  - International Conference on Machine Learning (ICML)
AB  - Reinforcement learning is typically concerned with learning control policies tailored to a particular agent. We investigate whether there exists a single global policy that can generalize to control a wide variety of agent morphologies - ones in which even dimensionality of state and action spaces changes. We propose to express this global policy as a collection of identical modular neural networks, dubbed as Shared Modular Policies (SMP), that correspond to each of the agent's actuators. Every module is only responsible for controlling its corresponding actuator and receives information from only its local sensors. In addition, messages are passed between modules, propagating information between distant modules. We show that a single modular policy can successfully generate locomotion behaviors for several planar agents with different skeletal structures such as monopod hoppers, quadrupeds, bipeds, and generalize to variants not seen during training - a process that would normally require training and manual hyperparameter tuning for each morphology. We observe that a wide variety of drastically diverse locomotion styles across morphologies as well as centralized coordination emerges via message passing between decentralized modules purely from the reinforcement learning objective.
AD  - Univ Calif Berkeley, Berkeley, CA USAAD  - Google, Mountain View, CA 94043 USAAD  - CMU, Pittsburgh, PA 15213 USAAD  - Facebook AI Res, Menlo Pk, CA 94025 USAC3  - University of California SystemC3  - University of California BerkeleyC3  - Google IncorporatedC3  - Carnegie Mellon UniversityC3  - Facebook IncFU  - Google faculty research award
FX  - We would like to thank Alyosha Efros, Yann LeCun, Jitendra Malik, Pieter Abbeel, Hang Gao, and the members of BAIR community for fruitful discussions. This work was supported in part by the Google faculty research award.
PU  - JMLR-JOURNAL MACHINE LEARNING RESEARCH
PI  - SAN DIEGO
PA  - 1269 LAW ST, SAN DIEGO, CA, UNITED STATES
SN  - 2640-3498
J9  - PR MACH LEARN RES
PY  - 2020
VL  - 119
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000683178504051
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  41
ER  -

TY  - JOUR
AU  - Abdo, L
AU  - Ahmad, I
AU  - Abed, S
TI  - A smart admission control and cache replacement approach in content delivery networks
T2  - CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND APPLICATIONS
LA  - English
KW  - Smart caching policies
KW  - Reinforcement learning
KW  - Deep learning
KW  - Probability prediction
KW  - Cache hit ratio
AB  - Content Delivery Networks (CDNs) distribute most data traffic nowadays by caching the contents in a network of servers to provide users with the requested objects, and helping to reduce latency when delivering contents to the user. The content caching system performance depends upon many factors such as where the objects should be stored, which object to store, and when to cache them. The proposed methodology includes two main phases: an admission control phase and a cache replacement phase. The admission control phase is responsible for accepting or rejecting the incoming request based on training the Reinforcement Learning (RL) algorithm to make the best decision in the near future to maximize its reward, which, in this case, is the hit ratio. The cache replacement phase estimates the object's future popularity. This is achieved by building a predictive model based on the popularity prediction mechanism, where the Long-Short-Term Memory (LSTM) model is used to compute the object's popularity. The LSTM model's outcome can help decide which objects to cache and which objects to evict from the cache. The proposed methodology is tested on a dataset to demonstrate its effectiveness in enhancing the hit ratio compared to conventional replacement policies such as First-in-First-Out (FIFO), Least Recently Used (LRU), Least Frequently Used (LFU) and a recent machine learning-based algorithm. The experimental results on the dataset revealed that the proposed methodology outperformed the baseline algorithms by 34.7% to 97.17% with a cache size of 130.
AD  - Kuwait Univ, Coll Engn & Petr, Comp Engn Dept, Kuwait, KuwaitC3  - Kuwait UniversityPU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 1386-7857
SN  - 1573-7543
J9  - CLUSTER COMPUT
JI  - Cluster Comput.
DA  - 2023 JUL 8
PY  - 2023
DO  - 10.1007/s10586-023-04095-7
C6  - JUL 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001025715700002
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  17
ER  -

TY  - CPAPER
AU  - Yang, C
AU  - Li, XX
AU  - Baracaldo, N
AU  - Shah, N
AU  - He, CY
AU  - Lyu, LJ
AU  - Sun, LC
AU  - Avestimehr, S
A1  - ACM
TI  - The 1st International Workshop on Federated Learning with Graph Data (FedGraph)
T2  - PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022
LA  - English
CP  - 31st ACM International Conference on Information and Knowledge Management (CIKM)
AB  - The field of graph data mining, one of the most important AI research areas, has been revolutionized by graph neural networks (GNNs), which benefit from training on real-world graph data with millions to billions of nodes and links. Unfortunately, the training data and process of GNNs involving graphs beyond millions of nodes are extremely costly on a centralized server, if not impossible. Moreover, due to the increasing concerns about data privacy, emerging data from realistic applications are naturally fragmented, forming distributed private graphs of multiple "data silos", among which direct transferring of data is forbidden. The nascent field of federated learning (FL), which aims to enable individual clients to jointly train their models while keeping their local data decentralized and completely private, is a promising paradigm for large-scale distributed and private training of GNNs. FedGraph2022 aims to bring together researchers from different backgrounds with a common interest in how to extend current FL algorithms to operate with graph data models such as GNNs. FL is an extremely hot topic of large commercial interest and has been intensively explored for machine learning with visual and textual data. The exploration from graph mining researchers and industrial practitioners is timely catching up just recently. There are many unexplored challenges and opportunities, which urges the establishment of an organized and open community to collaboratively advance the science behind it. The prospective participants of this workshop will include researchers and practitioners from both graph mining and federated learning communities, whose interests include, but are not limited to: graph analysis and mining, heterogeneous network modeling, complex data mining, large-scale machine learning, distributed systems, optimization, meta-learning, reinforcement learning, privacy, robustness, explainability, fairness, ethics, and trustworthiness.
AD  - Emory Univ, Atlanta, GA 30322 USAAD  - Univ British Columbia, Vancouver, BC, CanadaAD  - IBM Res, San Jose, CA USAAD  - Snap Res, Seattle, WA USAAD  - FedML Inc, Los Angeles, CA USAAD  - Sony AI, Tokyo, JapanAD  - Lehigh Univ, Bethlehem, PA 18015 USAAD  - Univ Southern Calif, Los Angeles, CA 90007 USAC3  - Emory UniversityC3  - University of British ColumbiaC3  - International Business Machines (IBM)C3  - Lehigh UniversityC3  - University of Southern CaliforniaFU  - JZTData Technology; Tencent; FedML; USC-Amazon Center
FX  - With pride and gratitude, we thank the sponsors of JZTData Technology, Tencent, FedML, and USC-Amazon Center, who financially supported our cash prizes for best papers, honorariums to speakers and travel awards to student volunteers. We also thank the student volunteers/organizers-Han Xie (Emory University), Hejie Cui (Emory University), Zishan Gu (Columbia University) and Emir Ceyani (University of Southern California)-for their efforts in helping the organization of FedGraph2022.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-9236-5
PY  - 2022
SP  - 5179
EP  - 5180
DO  - 10.1145/3511808.3557495
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001074639605056
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  0
ER  -

TY  - CPAPER
AU  - Alkoby, S
AU  - Rath, A
AU  - Stone, P
A1  - Assoc Comp Machinery
TI  - Teaching Social Behavior through Human Reinforcement for Ad hoc Teamwork - The STAR Framework
T2  - AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS
LA  - English
CP  - 18th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)
KW  - Ad hoc
KW  - Reinforcement Learning
KW  - Social Norms
AB  - As AI technology continues to develop, more and more agents will become capable of long term autonomy alongside people. Thus, a recent line of research has studied the problem of teaching autonomous agents the concept of ethics and human social norms. Most existing work considers the case of an individual agent attempting to learn a predefined set of rules. In reality however, social norms are not always pre-defined and are very difficult to represent algorithmically. Moreover, the basic idea behind the social norms concept is ensuring that one's actions do not negatively influence others' utilities, which is inherently a multiagent concept. Thus, here we investigate a way to teach agents, as a team, how to act according to human social norms. In this research, we introduce the star framework used to teach an ad hoc team of agents to act in accordance with human social norms. Using a hybrid team (agents and people), when taking an action considered to be socially unacceptable, the agents receive negative feedback from the human teammate(s) who has(have) an awareness of the team's norms. We view star as an important step towards teaching agents to act more consistently with respect to human morality.
AD  - Univ Texas Austin, Austin, TX 78712 USAC3  - University of Texas SystemC3  - University of Texas AustinFU  - NSF [IIS-1637736, IIS-1651089, ITS-1724157]; ONR [N00014-18-2243]; DARPA; Intel; Raytheon; Lockheed Martin; FLI [RFP2-000]
FX  - This work has taken place in the Learning Agents Research Group (LARG) at UT Austin. LARG research is supported in part by NSF (IIS-1637736, IIS-1651089, ITS-1724157), ONR (N00014-18-2243), FLI (RFP2-000), DARPA, Intel, Raytheon, and Lockheed Martin. Peter Stone serves on the Board of Directors of Cogitai, Inc. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-6309-9
PY  - 2019
SP  - 1773
EP  - 1775
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000474345000209
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  22
ER  -

TY  - JOUR
AU  - Wei, CY
AU  - Wang, H
AU  - Bai, HA
AU  - Ji, Z
AU  - Liu, ZH
TI  - PPLC: Data-driven offline learning approach for excavating control of cutter suction dredgers
T2  - ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE
LA  - English
KW  - Cutter suction dredgers
KW  - Excavating operation
KW  - Prediction
KW  - Deep reinforcement learning
KW  - EXPERT-SYSTEM
KW  - DEEP
KW  - NETWORK
KW  - BIG
AB  - Cutter suction dredgers (CSDs) play a very important role in the construction of ports, waterways and navigational channels. Currently, most of CSDs are mainly manipulated by human operators, and a large amount of instrument data needs to be monitored in real time in case of unforeseen accidents. In order to reduce the heavy workload of the operators, we propose a data-driven offline learning approach, named Preprocessing-Prediction-Learning Control (PPLC), for obtaining the optimal control policy of the excavating operation of CSDs. The proposed framework consists of three modules, i.e., a data preprocessing module, a dynamics prediction module realized by a Convolutional Neural Network (CNN), and a deep reinforcement learning based control module. The first module is responsible for filtering out irrelevant variables through correlation analysis and dimensionality reduction of raw data. The second module works as a state transition function that provides the dynamics prediction of the excavating operation of a CSD. To realize the learning control, the third module employs the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm to control the swing speed during the excavating operation. The simulation results show that the proposed framework can provide an effective and reliable solution to the automated excavating control of a CSD.
AD  - Hohai Univ, Coll Mech & Elect Engn, Nanjing, Peoples R ChinaAD  - Hohai Univ, Engn Res Ctr Dredging Technol, Minist Educ, Nanjing, Peoples R ChinaAD  - Cardiff Univ, Sch Engn, Cardiff CF24 3AA, WalesC3  - Hohai UniversityC3  - Hohai UniversityC3  - Cardiff UniversityFU  - National Natural Science Foundation of China [61703138]; International Science and Technology Cooperation Research Funds of Changzhou [CZ20210027]
FX  - Acknowledgements This work was supported in part by the National Natural Science Foundation of China under Grant 61703138, and the International Science and Technology Cooperation Research Funds of Changzhou under Grant CZ20210027.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0952-1976
SN  - 1873-6769
J9  - ENG APPL ARTIF INTEL
JI  - Eng. Appl. Artif. Intell.
DA  - OCT
PY  - 2023
VL  - 125
C7  - 106708
DO  - 10.1016/j.engappai.2023.106708
C6  - JUL 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001034116900001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  31
ER  -

TY  - JOUR
AU  - Kontes, GD
AU  - Giannakis, GI
AU  - Sanchez, V
AU  - de Agustin-Camacho, P
AU  - Romero-Amorrortu, A
AU  - Panagiotidou, N
AU  - Rovas, DV
AU  - Steiger, S
AU  - Mutschler, C
AU  - Gruen, G
TI  - Simulation-Based Evaluation and Optimization of Control Strategies in Buildings
T2  - ENERGIES
LA  - English
KW  - model predictive control in buildings
KW  - reinforcement learning
KW  - data-driven control
KW  - simulation model
KW  - multi-criteria decision analysis
KW  - energyplus
KW  - MODEL-PREDICTIVE CONTROL
KW  - INDOOR THERMAL COMFORT
KW  - ENERGY
KW  - FRAMEWORK
KW  - SYSTEMS
KW  - GENERATION
KW  - MANAGEMENT
KW  - DESIGN
KW  - MPC
KW  - IDENTIFICATION
AB  - Over the last several years, a great amount of research work has been focused on the development of model predictive control techniques for the indoor climate control of buildings, but, despite the promising results, this technology is still not adopted by the industry. One of the main reasons for this is the increased cost associated with the development and calibration (or identification) of mathematical models of special structure used for predicting future states of the building. We propose a methodology to overcome this obstacle by replacing these hand-engineered mathematical models with a thermal simulation model of the building developed using detailed thermal simulation engines such as EnergyPlus. As designing better controllers requires interacting with the simulation model, a central part of our methodology is the control improvement (or optimisation) module, facilitating two simulation-based control improvement methodologies: one based in multi-criteria decision analysis methods and the other based on state-space identification of dynamical systems using Gaussian process models and reinforcement learning. We evaluate the proposed methodology in a set of simulation-based experiments using the thermal simulation model of a real building located in Portugal. Our results indicate that the proposed methodology could be a viable alternative to model predictive control-based supervisory control in buildings.
AD  - Fraunhofer Inst Integrated Circuits IIS, Precise Positioning & Analyt Dept, Machine Learning & Informat Fus Grp, Nordostpk 84, D-90411 Nurnberg, GermanyAD  - Tech Hsch Nurnberg Georg Simon Ohm, Dept Mech Engn & Bldg Serv Engn, D-90489 Nurnberg, GermanyAD  - Tech Univ Crete, Sch Prod Engn & Management, Khania 73100, GreeceAD  - Tecnalia Res & Innovat, Sustainable Construct Div, Parque Tecnol Bizkaia,Edificio 700, Derio 48160, SpainAD  - UCL, Fac Built Environm, Bartlett Sch Environm Energy & Resources, London WC1E 6BT, EnglandAD  - Fraunhofer Inst Bldg Phys, Dept Energy Efficiency & Indoor Climate, Nuremberg Branch, Tech Bldg Syst Grp, D-90429 Nurnberg, GermanyAD  - Friedrich Alexander Univ Erlangen Nuremberg, Machine Learning & Data Analyt Lab, Carl Thiersch Str 2b, D-91052 Erlangen, GermanyAD  - Fraunhofer Inst Bldg Phys, Dept Energy Efficiency & Indoor Climate, Fraunhoferstr 10, D-83626 Valley, GermanyC3  - Fraunhofer GesellschaftC3  - Technical University of CreteC3  - University of LondonC3  - University College LondonC3  - Fraunhofer GesellschaftC3  - University of Erlangen NurembergC3  - Fraunhofer GesellschaftFU  - Modelling Optimization of Energy Efficiency in Buildings for Urban Sustainability (MOEEBIUS) project; European Union's Horizon 2020 research and innovation programme [680517]; European Commission H2020-EeB5-2015 project "Optimised Energy Efficient Design Platform for Refurbishment at District Level" [680676]; Federal Ministry of Education and Research of Germany in the framework of Machine Learning Forum [01IS17071]; MOEEBIUS project; H2020 Societal Challenges Programme [680517] Funding Source: H2020 Societal Challenges Programme
FX  - Research leading to these results has been partially supported by the Modelling Optimization of Energy Efficiency in Buildings for Urban Sustainability (MOEEBIUS) project. This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No. 680517. Georgios Giannakis and Dimitrios Rovas gratefully acknowledge financial support from the European Commission H2020-EeB5-2015 project "Optimised Energy Efficient Design Platform for Refurbishment at District Level" under Contract #680676 (OptEEmAL). Georgios Kontes and Christopher Mutschler gratefully acknowledge financial support from the Federal Ministry of Education and Research of Germany in the framework of Machine Learning Forum (grant number 01IS17071). Georgios Kontes, Natalia Panagiotidou, Simone Steiger and Gunnar Gruen gratefully acknowledge use of the services and facilities of the Energie Campus Nurnberg. The APC was funded by MOEEBIUS project. This paper reflects only the authors' views and the Commission is not responsible for any use that may be made of the information contained therein.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 1996-1073
J9  - ENERGIES
JI  - Energies
DA  - DEC
PY  - 2018
VL  - 11
IS  - 12
C7  - 3376
DO  - 10.3390/en11123376
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000455358300137
N1  - Times Cited in Web of Science Core Collection:  28
Total Times Cited:  30
Cited Reference Count:  105
ER  -

TY  - JOUR
AU  - Rabe, M
AU  - Ammouriova, M
AU  - Schmitt, D
AU  - Dross, F
TI  - Simheuristics Approaches for Efficient Decision-Making Support in Materials Trading Networks
T2  - ALGORITHMS
LA  - English
KW  - simulation
KW  - optimization
KW  - machine learning
KW  - logistics
KW  - distribution networks
KW  - SIMULATION-OPTIMIZATION
KW  - EVOLUTIONARY ALGORITHM
KW  - METAHEURISTICS
KW  - MANAGEMENT
AB  - The distribution process in business-to-business materials trading is among the most complex and in transparent ones within logistics. The highly volatile environment requires continuous adaptations by the responsible decision-makers, who face a substantial number of potential improvement actions with conflicting goals, such as simultaneously maintaining a high service level and low costs. Simulation-optimisation approaches have been proposed in this context, for example based on evolutionary algorithms. But, on real-world system dimensions, they face impractically long computation times. This paper addresses this challenge in two principal streams. On the one hand, reinforcement learning is investigated to reduce the response time of the system in a concrete decision situation. On the other hand, domain-specific information and defining equivalent solutions are exploited to support a metaheuristic algorithm. For these approaches, we have developed suitable implementations and evaluated them with subsets of real-world data. The results demonstrate that reinforcement learning exploits the idle time between decision situations to learn which decisions might be most promising, thus adding computation time but significantly reducing the response time. Using domain-specific information reduces the number of required simulation runs and guides the search for promising actions. In our experimentation, defining equivalent solutions decreased the number of required simulation runs up to 15%.
AD  - TU Dortmund, IT Prod & Logist, Fac Mech Engn, D-44227 Dortmund, GermanyC3  - Dortmund University of TechnologyFU  - German University of Jordan; Graduate School of Logistics in Dortmund (Germany); thyssenkrupp Materials International GmbH
FX  - This research was partially funded by the German University of Jordan, the Graduate School of Logistics in Dortmund (Germany) and by thyssenkrupp Materials International GmbH.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 1999-4893
J9  - ALGORITHMS
JI  - Algorithms
DA  - JAN
PY  - 2021
VL  - 14
IS  - 1
C7  - 23
DO  - 10.3390/a14010023
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000609638500001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  89
ER  -

TY  - JOUR
AU  - Arnold, T
AU  - Scheutz, M
TI  - Understanding the spirit of a norm: Challenges for norm-learning agents
T2  - AI MAGAZINE
LA  - English
KW  - ROBOTS
AB  - Social and moral norms are a fabric for holding human societies together and helping them to function. As such they will also become a means of evaluating the performance of future human-machine systems. While machine ethics has offered various approaches to endowing machines with normative competence, from the more logic-based to the more data-based, none of the proposals so far have considered the challenge of capturing the "spirit of a norm," which often eludes rigid interpretation and complicates doing the right thing. We present some paradigmatic scenarios across contexts to illustrate why the spirit of a norm can be critical to make explicit and why it exposes the inadequacies of mere data-driven "value alignment" techniques such as reinforcement learning RL for interactive, real-time human-robot interaction. Instead, we argue that norm learning, in particular, learning to capture the spirit of a norm, requires combining common-sense inference-based and data-driving approaches.
AD  - Tufts Univ, Human Robot Interact Lab, Medford, MA USAAD  - Tufts Univ, Human Robot Interact Lab, 177 Coll Ave, Medford, MA 02155 USAC3  - Tufts UniversityC3  - Tufts UniversityFU  - This work was in part funded by AFOSR Grant #FA9550-23-1-0425. [FA9550-23-1-0425]; AFOSR Grant
FX  - This work was in part funded by AFOSR Grant #FA9550-23-1-0425.
PU  - AMER ASSOC ARTIFICIAL INTELL
PI  - MENLO PK
PA  - 445 BURGESS DRIVE, MENLO PK, CA 94025-3496 USA
SN  - 0738-4602
SN  - 2371-9621
J9  - AI MAG
JI  - AI Mag.
DA  - DEC
PY  - 2023
VL  - 44
IS  - 4
SP  - 524
EP  - 536
DO  - 10.1002/aaai.12138
C6  - OCT 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001090404600001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  47
ER  -

TY  - CPAPER
AU  - Luo, CA
AU  - Zhao, P
AU  - Chen, C
AU  - Qiao, B
AU  - Du, C
AU  - Zhang, HY
AU  - Wu, W
AU  - Cai, SW
AU  - He, B
AU  - Rajmohan, S
AU  - Lin, QW
A1  - Assoc Advancement Artificial Intelligence
TI  - <i>PULNS</i>: Positive-Unlabeled Learning with Effective Negative Sample Selector
T2  - THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
LA  - English
CP  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence
AB  - Positive-unlabeled learning (PU learning) is an important case of binary classification where the training data only contains positive and unlabeled samples. The current state-of-the-art approach for PU learning is the cost-sensitive approach, which casts PU learning as a cost-sensitive classification problem and relies on unbiased risk estimator for correcting the bias introduced by the unlabeled samples. However, this approach requires the knowledge of class prior and is subject to the potential label noise. In this paper, we propose a novel PU learning approach dubbed PULNS, equipped with an effective negative sample selector, which is optimized by reinforcement learning. Our PULNS approach employs an effective negative sample selector as the agent responsible for selecting negative samples from the unlabeled data. While the selected, likely negative samples can be used to improve the classifier, the performance of classifier is also used as the reward to improve the selector through the REINFORCE algorithm. By alternating the updates of the selector and the classifier, the performance of both is improved. Extensive experimental studies on 7 real-world application benchmarks demonstrate that PULNS consistently outperforms the current state-of-the-art methods in PU learning, and our experimental results also confirm the effectiveness of the negative sample selector underlying PULNS.
AD  - Microsoft Res, Beijing, Peoples R ChinaAD  - Microsoft 365, Redmond, WA USAAD  - Univ Newcastle, Newcastle, NSW, AustraliaAD  - Leibniz Univ Hannover, L3S Res Ctr, Hannover, GermanyAD  - Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R ChinaAD  - Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing, Peoples R ChinaC3  - MicrosoftC3  - University of NewcastleC3  - Leibniz University HannoverC3  - Chinese Academy of SciencesC3  - Institute of Software, CASC3  - Chinese Academy of SciencesC3  - University of Chinese Academy of Sciences, CASPU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-866-4
J9  - AAAI CONF ARTIF INTE
PY  - 2021
VL  - 35
SP  - 8784
EP  - 8792
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000681269800042
N1  - Times Cited in Web of Science Core Collection:  9
Total Times Cited:  11
Cited Reference Count:  30
ER  -

TY  - CPAPER
AU  - Chen, PH
AU  - Lu, Y
AU  - Peng, Y
AU  - Liu, JF
AU  - Xu, Q
ED  - Bittencourt, II
ED  - Cukurova, M
ED  - Muldner, K
ED  - Luckin, R
ED  - Millan, E
TI  - Identification of Students' Need Deficiency Through a Dialogue System
T2  - ARTIFICIAL INTELLIGENCE IN EDUCATION (AIED 2020), PT II
LA  - English
CP  - 21st International Conference on Artificial Intelligence in Education (AIED)
KW  - Need deficiency
KW  - Problem behavior
KW  - Dialogue system
AB  - In the domain of moral education, students' need deficiency refers to the unsatisfied need that would result in problem behaviors. Timely and accurate identification of students' need deficiency is crucial to moral education and the students themselves. Previous psychology research focusing on distinct factors only provides scattered guidelines to identify such need deficiencies and meanwhile few teachers and parents have the related expertise, which makes the identification task difficult to accomplish. To address these issues, we develop a task-oriented dialogue system to help teachers and parents identify students' need deficiency through multi-turn dialogues. Specifically, relevant factors of need deficiency are summarized based on psychology theories, which provides a theoretical foundation for the newly proposed system. In addition, reinforcement learning methodology is adopted to learn dialogue policy to serve the designed dialogue system. Experimental results demonstrate that the developed dialogue system achieves its design objectives.
AD  - Beijing Normal Univ, Adv Innovat Ctr Future Educ, Fac Educ, Beijing 100875, Peoples R ChinaC3  - Beijing Normal UniversityFU  - National Natural Science Foundation of China [61807003]; Fundamental Research Funds for the Central Universities; CCF-Tencent Open Fund
FX  - This research is partially supported by the National Natural Science Foundation of China (No. 61807003), the Fundamental Research Funds for the Central Universities, and sponsored by CCF-Tencent Open Fund.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-52240-7
SN  - 978-3-030-52239-1
J9  - LECT NOTES ARTIF INT
PY  - 2020
VL  - 12164
SP  - 59
EP  - 63
DO  - 10.1007/978-3-030-52240-7_11
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:000885076200011
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  13
ER  -

TY  - CPAPER
AU  - Coronato, A
AU  - Di Napoli, C
AU  - Paragliola, G
AU  - Serino, L
A1  - IEEE
TI  - Intelligent Planning of Onshore Touristic Itineraries for Cruise Passengers in a Smart City
T2  - 2021 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT ENVIRONMENTS (IE)
LA  - English
CP  - 17th International Conference on Intelligent Environments (IE)
KW  - Intelligent Transportation Systems
KW  - Smart Cities
KW  - Deep Reinforcement Learning
KW  - Optimal Planning
KW  - ALGORITHM
AB  - Typical cruise-line itineraries are planned more than a year in advance, so it is possible to know how many passengers arrive in advance months ahead, as well as when they leave and for how long they stay. Nevertheless, cruise passengers are often responsible of what is called "tourism crowding" that has a beneficial economic outcome for a destination, but often at the cost of increment of city traffic, overloading of the public transportation means and discomfort generated for residents. So, cruise tourism requires planning and management that takes into account also port city destinations factors.
   In this paper, we present a Deep Reinforcement Learning based planner for the onshore touristic itineraries and the intelligent distribution of cruise passengers in a city. The aim is to maximise the number of touristic attraction locations visited during a tour by avoiding the overcrowding of the touristic attraction locations. The planner is able to compose onshore touristic itineraries meeting the constraint given within the time window available for the cruise passengers, by taking into account the number of attraction locations available in the city together with dynamic parameters as their reception capacity and the time needed to go from one attraction to another.
AD  - Natl Res Council Italy, Inst High Performance Comp & Networking, Naples, ItalyC3  - Consiglio Nazionale delle Ricerche (CNR)C3  - Istituto di Calcolo e Reti ad Alte Prestazioni (ICAR-CNR)FU  - project ASMARA -Applicazioni pilota post Direttiva - MIUR P.O.N. Smart Cities and Communities and Social Innovation [2010/65, SCN 00529]
FX  - This research has been partly funded by the project ASMARA -Applicazioni pilota post Direttiva 2010/65 in realt`a portuali italiane della Suite MIELE a supporto delle Authority per ottimizzazione della inteRoperabilit`a nell'intermodalitA' dei flussi citt`a porto -SCN 00529 -MIUR P.O.N. Smart Cities and Communities and Social Innovation
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2469-8792
SN  - 978-1-6654-0346-7
J9  - INT CONF INTEL ENVIR
PY  - 2021
DO  - 10.1109/IE51775.2021.9486648
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000815053000021
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  17
ER  -

TY  - CPAPER
AU  - Afsar, MM
A1  - Assoc Comp Machinery
TI  - Intelligent Multi-Purpose Healthcare Bot Facilitating Shared Decision Making
T2  - AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS
LA  - English
CP  - 18th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)
KW  - Shared-decision making
KW  - Patient decision aids
KW  - Multi-agent systems
KW  - Chatbots
KW  - Reinforcement learning
AB  - Patient decision aids (PtDAs) have been promoted to facilitate personalized information retrieval and decision support; nonetheless, although promoted for more than 20 years, they have generally failed to gain a foothold in the general delivery of healthcare. Intelligent interactive agent technologies could address the design features necessary to facilitate support and shared-decision making. In this thesis, we develop and build a PtDA for Prostate cancer using intelligent agent technology. The proposed system, called ALAN, has a multi-layered architecture with three layers. While the first layer (User-Interface) is responsible to effectively interact with users (patients and physicians), the bottom layer (Data) handles requests regarding storing and retrieving the data. Unlike most existing bots, our core objective is to enable ALAN with learning abilities, which can evolve in the course of time and improve its behaviour with minimum distraction of the user. To this end, reinforcement learning and deep learning algorithms are employed in the main layer, i.e., Analytical Decision Making. This research is expected to have impact on delivery of personalized healthcare.
AD  - Univ Calgary, Calgary, AB, CanadaC3  - University of CalgaryPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
SN  - 978-1-4503-6309-9
PY  - 2019
SP  - 2393
EP  - 2395
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000474345000417
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  8
ER  -

TY  - JOUR
AU  - Nikpour, B
AU  - Armanfard, N
TI  - Spatio-temporal hard attention learning for skeleton-based activity recognition
T2  - PATTERN RECOGNITION
LA  - English
KW  - Temporal attention
KW  - Spatial attention
KW  - Spatio-temporal attention
KW  - Activity recognition
KW  - Skeleton data
KW  - Deep reinforcement learning
KW  - 3D ACTION RECOGNITION
AB  - The use of skeleton data for activity recognition has become prevalent due to its advantages over RGB data. A skeleton video includes frames showing two-or three-dimensional coordinates of human body joints. For recognizing an activity, not all the video frames are informative, and only a few key frames can well represent an activity. Moreover, not all joints participate in every activity; i.e., the key joints may vary across frames and activities. In this paper, we propose a novel framework for finding temporal and spatial attentions in a cooperative manner for activity recognition. The proposed method, which is called STH-DRL, consists of a temporal agent and a spatial agent. The temporal agent is responsible for finding the key frames, i.e., temporal hard attention finding, and the spatial agent attempts to find the key joints, i.e., spatial hard attention finding. We formulate the search problems as Markov decision processes and train both agents through interacting with each other using deep reinforcement learning. Experimental results on three widely used activity recognition benchmark datasets demonstrate the effectiveness of our proposed method. Crown Copyright (c) 2023 Published by Elsevier Ltd. All rights reserved.
AD  - McGill Univ Canada, Dept Elect & Comp Engn, Montreal, PQ, CanadaAD  - Mila Quebec AI Inst, Montreal, PQ, CanadaFU  - Natural Sciences and Engineering Research Council of Canada (NSERC); Calcul Quebec; Compute Canada
FX  - The authors wish to acknowledge the financial support of the Natural Sciences and Engineering Research Council of Canada (NSERC) . This research was enabled in part by support provided by Calcul Quebec and Compute Canada.
PU  - ELSEVIER SCI LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
J9  - PATTERN RECOGN
JI  - Pattern Recognit.
DA  - JUL
PY  - 2023
VL  - 139
C7  - 109428
DO  - 10.1016/j.patcog.2023.109428
C6  - MAR 2023
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000965834800001
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  55
ER  -

TY  - JOUR
AU  - Siatras, V
AU  - Bakopoulos, E
AU  - Mavrothalassitis, P
AU  - Nikolakis, N
AU  - Alexopoulos, K
TI  - On the Use of Asset Administration Shell for Modeling and Deploying Production Scheduling Agents within a Multi-Agent System
T2  - APPLIED SCIENCES-BASEL
LA  - English
KW  - agent-oriented programming
KW  - Asset Administration Shell
KW  - multi-agent system
KW  - scheduling agent
KW  - POWER ENGINEERING APPLICATIONS
KW  - MICROGRIDS
KW  - PLATFORM
KW  - STRATEGY
AB  - Industry 4.0 (I4.0) aims at achieving the interconnectivity of multiple industrial assets from different hierarchical layers within a manufacturing environment. The Asset Administration Shell (AAS) is a pilar component of I4.0 for the digital representation of assets and can be applied in both physical and digital assets, such as enterprise software, artificial intelligence (AI) agents, and databases. Multi-agent systems (MASs), in particular, are useful in the decentralized optimization of complex problems and applicable in various planning or scheduling scenarios that require the system's ability to adapt to any given problem by using different optimization methods. In order to achieve this, a universal model for the agent's information, communication, and behaviors should be provided in a way that is interoperable with the rest of the I4.0 assets and agents. To address these challenges, this work proposes an AAS-based information model for the description of scheduling agents. It allows multiple AI methods for scheduling, such as heuristics, mathematical programming, and deep reinforcement learning, to be encapsulated within a single agent, making it adjustable to different production scenarios. The software implementation of the proposed architecture aims to provide granularity in the deployment of scheduling agents which utilize the underlying AAS metamodel. The agent was implemented using the SARL agent-oriented programming (AOP) language and deployed in an open-source MAS platform. The system evaluation in a real-life bicycle production scenario indicated the agent's ability to adapt and provide fast and accurate scheduling results.
AD  - Univ Patras, Dept Mech Engn & Aeronaut, Lab Mfg Syst & Automat, Patras 26504, GreeceC3  - University of PatrasFU  - European Union [957204 MAS4AI]
FX  - This project has received funding from the European Union's Horizon 2020 research andinno-vation program under grant agreement No 957204 MAS4AI. The dissemination of results herein reflects only the authors' view, and the Commission is not responsible for any use that may be madeof the information it contains.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2076-3417
J9  - APPL SCI-BASEL
JI  - Appl. Sci.-Basel
DA  - SEP
PY  - 2023
VL  - 13
IS  - 17
C7  - 9540
DO  - 10.3390/app13179540
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001062334400001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  68
ER  -

TY  - JOUR
AU  - Chen, HJ
AU  - Liu, WJ
AU  - Goel, R
AU  - Lua, RC
AU  - Mittal, S
AU  - Huang, YZ
AU  - Veeraraghavan, A
AU  - Patel, AB
TI  - Fast Retinomorphic Event-Driven Representations for Video Gameplay and Action Recognition
T2  - IEEE TRANSACTIONS ON COMPUTATIONAL IMAGING
LA  - English
KW  - Smart cameras
KW  - retina
KW  - real-time systems
KW  - streaming media
KW  - cells (biology)
KW  - reinforcement learning
KW  - video signal processing
KW  - video
KW  - ON-CENTER
KW  - CELLS
KW  - CONTRAST
AB  - Good temporal representations are crucial for video understanding, and the state-of-the-art video recognition framework is based on two-stream networks. In such framework, besides the regular ConvNets responsible for RGB frame inputs, a second network is introduced to handle the temporal representation, usually the optical flow (OF). However, OF or other task-oriented flow is computationally costly, and is thus typically pre-computed. Critically, this prevents the two-stream approach from being applied to reinforcement learning (RL) applications such as video game playing, where the next state depends on current state and action choices. Inspired by the early vision systems of mammals and insects, we propose a fast event-driven representation (EDR) that models several major properties of early retinal circuits: (1) log-arithmic input response, (2) multi-timescale temporal smoothing to filter noise, and (3) bipolar (ON/OFF) pathways for primitive event detection. Trading off the directional information for fast speed (>9000 fps), EDR enables fast real-time inference/learning in video applications that require interaction between an agent and the world such as game-playing, virtual robotics, and domain adaptation. In this vein, we use EDR to demonstrate performance improvements over state-of-the-art reinforcement learning algorithms for Atari games, something that has not been possible with pre-computed OF. Moreover, with UCF-101 video action recognition experiments, we show that EDR performs near state-of-the-art in accuracy while achieving a 1,500x speedup in input representation processing, as compared to optical flow.
AD  - Rice Univ, Dept Elect & Comp Engn, Houston, TX 77005 USAAD  - SenseBrain Technol LLC, San Jose, CA 95131 USAAD  - Rice Univ, Dept Comp Sci, Houston, TX 77005 USAAD  - Google Inc, Mountain View, CA 94043 USAAD  - Indian Inst Technol Delhi, New Delhi 110016, IndiaAD  - Borealis AI, Montreal, PQ H2S 3H1, CanadaAD  - Baylor Coll Med, Dept Neurosci, Houston, TX 77030 USAAD  - Indian Inst Technol Kanpur, Kanpur 208016, Uttar Pradesh, IndiaAD  - Quadeye, Gurgaon 122009, IndiaAD  - Olin Coll Engn, Needham, MA 02492 USAAD  - Kensho Technol, Cambridge, MA 02138 USAC3  - Rice UniversityC3  - Rice UniversityC3  - Google IncorporatedC3  - Indian Institute of Technology System (IIT System)C3  - Indian Institute of Technology (IIT) - DelhiC3  - Baylor College of MedicineC3  - Indian Institute of Technology System (IIT System)C3  - Indian Institute of Technology (IIT) - KanpurC3  - Franklin W. Olin College of EngineeringFU  - NSFEAGER [1502875]; NSFCAREER [1652633]; Texas Instruments Distinguished Graduate Student Fellowship; IARPA via DoI/IBC [D16PC00003]; NSF NeuroNex [DBI-1707400]; NSF EAGER [1502875]; NSF CAREER [1652633]; Direct For Computer & Info Scie & Enginr; Division of Computing and Communication Foundations [1502875] Funding Source: National Science Foundation; Div Of Information & Intelligent Systems; Direct For Computer & Info Scie & Enginr [1652633] Funding Source: National Science Foundation
FX  - The work of H. Chen was supported in part by the NSFEAGER underGrant 1502875, in part by the NSFCAREER underGrant 1652633, and in part by the Texas Instruments Distinguished Graduate Student Fellowship. The work of W. Liu, R. C. Lua, and A. B. Patel was supported in part by IARPA via DoI/IBC under Contract D16PC00003 and in part by NSF NeuroNex under Grant DBI-1707400. The work of A. Veeraraghavan was supported in part by the NSF EAGER under Grant 1502875 and in part by the NSF CAREER under Grant 1652633. The associate editor coordinating the review of this manuscript and approving it for publication was O. Cossairt. (Huaijin Chen andWanjia Liu contributed equally to this work.)
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2573-0436
SN  - 2333-9403
J9  - IEEE T COMPUT IMAG
JI  - IEEE Trans. Comput. Imaging
PY  - 2020
VL  - 6
SP  - 276
EP  - 290
DO  - 10.1109/TCI.2019.2948755
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000565812500022
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  75
ER  -

TY  - JOUR
AU  - Tang, T
AU  - Wu, BW
AU  - Hu, GM
TI  - A Hybrid Learning Framework for Service Function Chaining Across Geo-Distributed Data Centers
T2  - IEEE ACCESS
LA  - English
KW  - Routing
KW  - Optimization
KW  - Data centers
KW  - Heuristic algorithms
KW  - Reliability
KW  - Machine learning
KW  - Artificial neural networks
KW  - Service function chaining
KW  - deep reinforcement learning
KW  - game theory
KW  - cloud computing
KW  - ORCHESTRATION
KW  - AWARE
AB  - Service function chaining (SFC) focuses mainly on deploying various network functions in geographically distributed data centers and providing interconnect routing among them. Traditional (convex optimization-based) SFC algorithms exhibit some disadvantages on the scalability and accuracy. Recently, researches have shown the effectiveness of deep reinforcement learning (DRL) in the field of SFC. However, current DRL-based algorithms possess an extremely large action space, which leads to slow convergence and poor scalability. Some researchers relieve this issue by reformulating the SFC problem, which usually results in low utilization and high cost. To address this issue, we develop a hybrid DRL-based framework which decouples the VNF deployment and flow routing into different modules. In the proposed framework, a DRL agent is only responsible for learning the policy of VNF deployment. We customize the structure of the agent base on deep deterministic policy gradient (DDPG) and adopt several techniques to improve the learning efficiency, such as adaptive parameter noise, wolpertinger policy, and prioritized experience replay. The flow routing is conducted in a game-based module (GBM). We design a decentralized routing algorithm for the GBM to address the scalability. The end-to-end latency of flows is minimized while the resource capacity and location constraints are satisfied. During the learning process of the proposed framework, the DRL agent improves its deployment policy with the reward from the GBM (the value of reward depends on flow routing). Thus, the VNF deployment and flow routing are still jointly optimized. Compared to existing DRL-learning algorithms, the proposed hybrid DRL framework can achieve a lower cost since 1) the action space is significantly reduced due to flow routing decoupling; 2) the flow routing procedure is more efficient (the GBM adopts model-based information, e.g., the gradient). Through trace-driven simulations, we show the efficiency of our algorithm compared to existing DRL-based algorithms.
AD  - Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R ChinaAD  - Univ Elect Sci & Technol China, Natl Key Lab Sci & Technol Commun, Chengdu 611731, Peoples R ChinaAD  - Univ Elect Sci & Technol China, Sch Resources & Environm, Chengdu 611731, Peoples R ChinaC3  - University of Electronic Science & Technology of ChinaC3  - University of Electronic Science & Technology of ChinaC3  - University of Electronic Science & Technology of ChinaFU  - National Natural Science Foundation of China [0561701074]
FX  - This work was supported in part by the National Natural Science Foundation of China under Grant 0561701074.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
J9  - IEEE ACCESS
JI  - IEEE Access
PY  - 2020
VL  - 8
SP  - 170225
EP  - 170236
DO  - 10.1109/ACCESS.2020.3024135
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000572943600001
N1  - Times Cited in Web of Science Core Collection:  5
Total Times Cited:  6
Cited Reference Count:  31
ER  -

TY  - CPAPER
AU  - Ammanabrolu, P
AU  - Jiang, LW
AU  - Sap, M
AU  - Hajishirzi, H
AU  - Choi, YJ
A1  - ASSOC COMPUTAT LINGUIST
TI  - Aligning to Social Norms and Values in Interactive Narratives
T2  - NAACL 2022: THE 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES
LA  - English
CP  - Conference of the North-American-Chapter-of-the-Association-for-Computational-Linguistics (NAAACL) - Human Language Technologies
AB  - We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games-environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms-causing harm either to the agent itself or other entities in the environment. Social value alignment refers to creating agents whose behaviors conform to expected moral and social norms for a given context and group of people-in our case, it means agents that behave in a manner that is less harmful and more beneficial for themselves and others.
   We build on the Jiminy Cricket benchmark (Hendrycks et al., 2021b), a set of 25 annotated interactive narratives containing thousands of morally salient scenarios covering everything from theft and bodily harm to altruism. We introduce the GALAD (Game-value Alignment through Action Distillation) agent that uses the social commonsense knowledge present in specially trained language models to contextually restrict its action space to only those actions that are aligned with socially beneficial values. An experimental study shows that the GALAD agent makes decisions efficiently enough to improve state-of-the-art task performance by 4% while reducing the frequency of socially harmful behaviors by 25% compared to strong contemporary value alignment approaches.
AD  - Allen Inst Artificial Intelligence, Seattle, WA 98103 USAAD  - Univ Washington, Seattle, WA 98195 USAC3  - University of WashingtonC3  - University of Washington SeattleFU  - DARPA MCS program through NIWC Pacific [N66001-19-24031]; Google Cloud Compute; Allen Institute for AI
FX  - This work was supported in part by DARPA MCS program through NIWC Pacific (N66001-19-24031), Google Cloud Compute, and the Allen Institute for AI.
PU  - ASSOC COMPUTATIONAL LINGUISTICS-ACL
PI  - STROUDSBURG
PA  - 209 N EIGHTH STREET, STROUDSBURG, PA 18360 USA
SN  - 978-1-955917-71-1
PY  - 2022
SP  - 5994
EP  - 6017
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:000859869506010
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  51
ER  -

TY  - JOUR
AU  - Wang, ZQ
AU  - Menke, JH
AU  - Schäfer, F
AU  - Braun, M
AU  - Scheidler, A
TI  - Approximating multi-purpose AC Optimal Power Flow with reinforcement trained Artificial Neural Network
T2  - ENERGY AND AI
LA  - English
KW  - OPF
KW  - Artificial neural network
KW  - Reinforcement learning
KW  - Grid planning and operation
KW  - Grid congestion management
KW  - OPTIMIZATION
KW  - SYSTEMS
AB  - Solving AC-Optimal Power Flow (OPF) problems is an essential task for grid operators to keep the power system safe for the use cases such as minimization of total generation cost or minimization of infeed curtailment from renewable DERs (Distributed Energy Resource). Mathematical solvers are often able to solve the AC-OPF problem but need significant computation time. Artificial neural networks (ANN) have a good application in function approximation with outstanding computational performance. In this paper, we employ ANN to approximate the solution of AC-OPF for multiple purposes. The novelty of our work is a new training method based on the reinforcement learning concept. A high-performance batched power flow solver is used as the physical environment for training, which evaluates an augmented loss function and the numerical action gradient. The augmented loss function consists of the objective term for each use case and the penalty term for constraints violation. This training method enables training without a reference OPF and the integration of discrete decision variable such as discrete transformer tap changer position in the constrained optimization. To improve the optimality of the approximation, we further combine the reinforcement training approach with supervised training labeled by reference OPF. Various benchmark results show the high approximation quality of our proposed approach while achieving high computational efficiency on multiple use cases.
AD  - Univ Kassel, Dept Energy Management & Power Syst Operat, Wilhelmshoher Allee 73, D-34121 Kassel, GermanyAD  - Fraunhofer Inst Energy Econ & Energy Syst Technol, Konigstor 59, D-34119 Kassel, GermanyC3  - Universitat KasselFU  - Hessian Ministry of Higher Education, Research, Science and the Arts, Germany through the K-ES project [773505]; European Union's Horizon 2020 research and innovation programme within the project EU-SysFlex;  [511/17.001]
FX  - The authors would like to thank Dr.-Ing. Nils Bornhorst for the fruitful discussion. The publication and development of this work was funded by the Hessian Ministry of Higher Education, Research, Science and the Arts, Germany through the K-ES project under reference number: 511/17.001. The development of the work was also funded by the European Union's Horizon 2020 research and innovation programme within the project EU-SysFlex under grant agreement No 773505. The authors are solely responsible for the content of this publication.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 2666-5468
J9  - ENERGY AI
JI  - Energy AI
DA  - JAN
PY  - 2022
VL  - 7
C7  - 100133
DO  - 10.1016/j.egyai.2021.100133
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:001059185900002
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  6
Cited Reference Count:  41
ER  -

TY  - JOUR
AU  - Ognibene, D
AU  - Wilkens, R
AU  - Taibi, D
AU  - Hernandez-Leo, D
AU  - Kruschwitz, U
AU  - Donabauer, G
AU  - Theophilou, E
AU  - Lomonaco, F
AU  - Bursic, S
AU  - Lobo, RA
AU  - Sanchez-Reina, JR
AU  - Scifo, L
AU  - Schwarze, V
AU  - Börsting, J
AU  - Hoppe, U
AU  - Aprin, F
AU  - Malzahn, N
AU  - Eimler, S
TI  - Challenging social media threats using collective well-being-aware recommendation algorithms and an educational virtual companion
T2  - FRONTIERS IN ARTIFICIAL INTELLIGENCE
LA  - English
KW  - collective well-being
KW  - recommender systems
KW  - social media
KW  - virtual companion
KW  - social media threats
KW  - hierarchical reinforcement learning
KW  - NETWORK SITES
KW  - BODY-IMAGE
KW  - SEXUAL OBJECTIFICATION
KW  - INTERNET ADDICTION
KW  - RACIAL HATE
KW  - SYSTEMS
KW  - FACEBOOK
KW  - ADULTS
KW  - DEINDIVIDUATION
KW  - CONNECTEDNESS
AB  - Social media have become an integral part of our lives, expanding our interlinking capabilities to new levels. There is plenty to be said about their positive effects. On the other hand, however, some serious negative implications of social media have been repeatedly highlighted in recent years, pointing at various threats to society and its more vulnerable members, such as teenagers, in particular, ranging from much-discussed problems such as digital addiction and polarization to manipulative influences of algorithms and further to more teenager-specific issues (e.g., body stereotyping). The impact of social media-both at an individual and societal level-is characterized by the complex interplay between the users' interactions and the intelligent components of the platform. Thus, users' understanding of social media mechanisms plays a determinant role. We thus propose a theoretical framework based on an adaptive "Social Media Virtual Companion" for educating and supporting an entire community, teenage students, to interact in social media environments in order to achieve desirable conditions, defined in terms of a community-specific and participatory designed measure of Collective Well-Being (CWB). This Companion combines automatic processing with expert intervention and guidance. The virtual Companion will be powered by a Recommender System (CWB-RS) that will optimize a CWB metric instead of engagement or platform profit, which currently largely drives recommender systems thereby disregarding any societal collateral effect. CWB-RS will optimize CWB both in the short term by balancing the level of social media threats the users are exposed to, and in the long term by adopting an Intelligent Tutor System role and enabling adaptive and personalized sequencing of playful learning activities. We put an emphasis on experts and educators in the educationally managed social media community of the Companion. They play five key roles: (a) use the Companion in classroom-based educational activities; (b) guide the definition of the CWB; (c) provide a hierarchical structure of learning strategies, objectives and activities that will support and contain the adaptive sequencing algorithms of the CWB-RS based on hierarchical reinforcement learning; (d) act as moderators of direct conflicts between the members of the community; and, finally, (e) monitor and address ethical and educational issues that are beyond the intelligent agent's competence and control. This framework offers a possible approach to understanding how to design social media systems and embedded educational interventions that favor a more healthy and positive society. Preliminary results on the performance of the Companion's components and studies of the educational and psychological underlying principles are presented.
AD  - Univ Milano Bicocca, Dept Psychol, Milan, ItalyAD  - Univ Essex, Fac Sci & Hlth, Sch Comp Sci & Elect Engn, Colchester, EnglandAD  - Univ Catholique Louvain UCLouvain, Inst Langage & Commun IL&C, Cental, Ottignies louvain La Neuv, BelgiumAD  - Natl Res Council Italy, Inst Educ Technol, Palermo, ItalyAD  - Pompeu Fabra Univ, Dept Informat & Commun Technol, Barcelona, SpainAD  - Univ Regensburg, Fac Informat Sci, Regensburg, GermanyAD  - Ruhr West Univ Appl Sci, Inst Comp Sci, Bottrop, GermanyAD  - Rhein Ruhr Inst Angew Systeminnovat, Duisburg, GermanyC3  - University of Milano-BicoccaC3  - University of EssexC3  - Consiglio Nazionale delle Ricerche (CNR)C3  - Istituto per le Tecnologie Didattiche (ITD-CNR)C3  - Pompeu Fabra UniversityC3  - University of RegensburgPU  - FRONTIERS MEDIA SA
PI  - LAUSANNE
PA  - AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN  - 2624-8212
J9  - FRONT ARTIF INTELL
JI  - Front. Artif. Intell.
DA  - JAN 9
PY  - 2023
VL  - 5
C7  - 654930
DO  - 10.3389/frai.2022.654930
WE  - Emerging Sources Citation Index (ESCI)AN  - WOS:000913802900001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  302
ER  -

TY  - CPAPER
AU  - Oesterle, M
AU  - Bartelt, C
AU  - Lüdtke, S
AU  - Stuckenschmidt, H
ED  - Ajmeri, N
ED  - Martin, AM
ED  - Savarimuthu, BTR
TI  - Self-learning Governance of Black-Box Multi-Agent Systems
T2  - COORDINATION, ORGANIZATIONS, INSTITUTIONS, NORMS, AND ETHICS FOR GOVERNANCE OF MULTI-AGENT SYSTEMS XV
LA  - English
CP  - International Workshop on Coordination, Organizations, Institutions, and Norms for Governance of Multi-Agent Systems (COINE)
KW  - Multi-Agent System
KW  - Governance
KW  - Self-learning system
KW  - Reinforcement Learning
KW  - Electronic institution
AB  - Agents in Multi-Agent Systems (MAS) are not always built and controlled by the system designer, e.g., on electronic trading platforms. In this case, there is often a system objective which can differ from the agents' own goals (e.g., price stability). While much effort has been put into modeling and optimizing agent behavior, we are concerned in this paper with the platform perspective. Our model extends Stochastic Games (SG) with dynamic restriction of action spaces to a new self-learning governance approach for black-box MAS. This governance learns an optimal restriction policy via Reinforcement Learning.
   As an alternative to the two straight-forward approaches fully centralized control and fully independent learners , this novel method combines a sufficient degree of autonomy for the agents with selective restriction of their action spaces. We demonstrate that the governance, though not explicitly instructed to leave any freedom of decision to the agents, learns that combining the agents' and its own capabilities is better than controlling all actions. As shown experimentally, the self-learning approach outperforms (w.r.t. the system objective) both "full control" where actions are always dictated without any agent autonomy, and "ungoverned MAS" where the agents simply pursue their individual goals.
AD  - Univ Mannheim, Inst Enterprise Syst InES, Mannheim, GermanyAD  - Univ Mannheim, Mannheim, GermanyC3  - University of MannheimC3  - University of MannheimFU  - German Federal Ministry for Economic Affairs and Energy (BMWi)
FX  - This work is supported by the German Federal Ministry for Economic Affairs and Energy (BMWi).
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 2945-9133
SN  - 1611-3349
SN  - 978-3-031-20844-7
SN  - 978-3-031-20845-4
J9  - LECT NOTES ARTIF INT
PY  - 2022
VL  - 13549
SP  - 73
EP  - 91
DO  - 10.1007/978-3-031-20845-4_5
WE  - Conference Proceedings Citation Index - Science (CPCI-S)WE  - Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)AN  - WOS:000896507600005
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  43
ER  -

TY  - CPAPER
AU  - Guo, WB
AU  - Wu, X
AU  - Wang, L
AU  - Xing, XY
AU  - Song, D
A1  - USENIX Association
TI  - PATROL: Provable Defense against Adversarial Policy in Two-player Games
T2  - PROCEEDINGS OF THE 32ND USENIX SECURITY SYMPOSIUM
LA  - English
CP  - 32nd USENIX Security Symposium
KW  - LEVEL
AB  - Recent advances in deep reinforcement learning (DRL) takes artificial intelligence to the next level, from making individual decisions to accomplishing sophisticated tasks via sequential decision makings, such as defeating world-class human players in various games and making real-time trading decisions in stock markets. Following these achievements, we have recently witnessed a new attack specifically designed against DRL. Recent research shows by learning and controlling an adversarial agent/policy, an attacker could quickly discover a victim agent's weaknesses and thus force it to fail its task.
   Due to differences in the threat model, most existing defenses proposed for deep neural networks (DNN) cannot be migrated to train robust policies against adversarial policy attacks. In this work, we draw insights from classical game theory and propose the first provable defense against such attacks in two-player competitive games. Technically, we first model the robust policy training problem as finding the nash equilibrium (NE) point in the entire policy space. Then, we design a novel policy training method to search for the NE point in complicated DRL tasks. Finally, we theoretically prove that our proposed method could guarantee the lower-bound performance of the trained agents against arbitrary adversarial policy attacks. Through extensive evaluations, we demonstrate that our method significantly outperforms existing policy training methods in adversarial robustness and performance in non-adversarial settings.
AD  - Univ Calif Berkeley, Berkeley, CA 94720 USAAD  - Northwestern Univ, Evanston, IL USAC3  - University of California SystemC3  - University of California BerkeleyC3  - Northwestern UniversityFU  - National Science Foundation; Berkeley Center for Responsible, Decentralized Intelligence
FX  - We would like to thank our anonymous shepherd and reviewers for their helpful feedback. This work was supported in part by the National Science Foundation and the Berkeley Center for Responsible, Decentralized Intelligence.
PU  - USENIX ASSOC
PI  - BERKELEY
PA  - SUITE 215, 2560 NINTH ST, BERKELEY, CA 94710 USA
SN  - 978-1-939133-37-3
PY  - 2023
SP  - 3943
EP  - 3960
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001066451504007
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  95
ER  -

TY  - CPAPER
AU  - Wu, QY
AU  - Wang, HZ
AU  - Wang, HN
A1  - ASSOC COMP MACHINERY
TI  - Learning by Exploration: New Challenges in Real-World Environments
T2  - KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING
LA  - English
CP  - 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)
AB  - Learning is a predominant theme for any intelligent system, humans, or machines. Moving beyond the classical paradigm of learning from past experience, e.g., offline supervised learning from given labels, a learner needs to actively collect exploratory feedback to learn from the unknowns, i.e., learning through exploration. This tutorial will introduce the learning by exploration paradigm, which is the key ingredient in many interactive online learning problems, including the multi-armed bandit and, more generally, reinforcement learning problems.
   In this tutorial, we will first motivate the need for exploration in machine learning algorithms and highlight its importance in many real-world problems where online sequential decision making is involved. In real-world application scenarios, considerable challenges arise in such a learning problem, including sample complexity, costly and even outdated feedback, and ethical considerations of exploration (such as fairness and privacy). We will introduce several classical exploration strategies and then highlight the aforementioned three fundamental challenges in the learning from exploration paradigm and introduce the recent research development on addressing them, respectively.
AD  - Univ Virginia, Charlottesville, VA 22903 USAC3  - University of VirginiaFU  - National Science Foundation [IIS-1618948, IIS-1553568]
FX  - This paper is based upon work supported by the National Science Foundation under grant IIS-1618948 and IIS-1553568.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-7998-4
PY  - 2020
SP  - 3575
EP  - 3576
DO  - 10.1145/3394486.3406484
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000749552303093
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  32
ER  -

TY  - CPAPER
AU  - Alves, J
AU  - Lau, N
AU  - Silva, F
ED  - Marreiros, G
ED  - Martins, B
ED  - Paiva, A
ED  - Ribeiro, B
ED  - Sardinha, A
TI  - Skill Learning for Long-Horizon Sequential Tasks
T2  - PROGRESS IN ARTIFICIAL INTELLIGENCE, EPIA 2022
LA  - English
CP  - 21st EPIA Conference on Artificial Intelligence (EPIA)
KW  - Skill learning
KW  - Hierarchical model
KW  - Reinforcement Learning
KW  - Deep learning
KW  - Goal-conditioned policy
KW  - Sequential tasks
AB  - Solving long-horizon problems is a desirable property in autonomous agents. Learning reusable behaviours can equip the agent with this property, allowing it to adapt them when performing various real-world tasks. Our approach for learning these behaviours is composed of three modules, operating in two separate timescales and it uses a hierarchical model with both discrete and continuous variables. This modular structure allows an independent training process for each stage. These stages are organized using a two-level temporal hierarchy. The first level contains the planner, responsible for issuing the skills that should be executed, while the second level executes the skill. In this latter level, to achieve the desired skill behaviour, the discrete skill is converted to a continuous vector that contains information regarding which environment change must occur. With this approach, we aimed to solve long-horizon sequential tasks with delayed rewards. Contrary to existing work, our method uses both variable types to allow an agent to learn high-level behaviours consisting of an interpretable set of skills. This method allows to compose the discrete skills easily, while keeping the flexibility, provided by the continuous representations, to execute them in several different ways. Using a 2D scenario where the agent has to catch a set of objects in a specific order, we demonstrate that our approach is scalable to scenarios with increasingly longer tasks.
AD  - Univ Aveiro, DETI IEETA, Aveiro, PortugalC3  - Universidade de AveiroFU  - FCT - Foundation for Science and Technology [2020.05789.BD]; IEETA -Institute of Electronics and Informatics Engineering of Aveiro - National Funds through the FCT -Foundation for Science and Technology [UIDB/00127/2020]
FX  - This research was developed in the scope of the PhD grant[2020.05789.BD], funded by FCT - Foundation for Science and Technology. This study was also supported by IEETA -Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through the FCT -Foundation for Science and Technology, in the context of the project [UIDB/00127/2020].
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-16474-3
SN  - 978-3-031-16473-6
J9  - LECT NOTES ARTIF INT
PY  - 2022
VL  - 13566
SP  - 713
EP  - 724
DO  - 10.1007/978-3-031-16474-3_58
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000869745400058
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  19
ER  -

TY  - JOUR
AU  - Petousis, P
AU  - Winter, A
AU  - Speier, W
AU  - Aberle, DR
AU  - Hsu, W
AU  - Bui, AAT
TI  - Using Sequential Decision Making to Improve Lung Cancer Screening Performance
T2  - IEEE ACCESS
LA  - English
KW  - Early disease prediction
KW  - dynamic Bayesian networks
KW  - lung cancer screening
KW  - partially observable Markov decision processes
KW  - QMDP algorithm
KW  - TRIAL
KW  - RISK
AB  - Globally, lung cancer is responsible for nearly one in five cancer deaths. The National Lung Screening Trial (NLST) demonstrated the efficacy of low-dose computed tomography (LDCT) to identify early-stage disease, setting the basis for widespread implementation of lung cancer screening programs. However, the specificity of LDCT lung cancer screening is suboptimal, with a significant false positive rate. Representing this imaging-based screening process as a sequential decision making problem, we combined multiple machine learning-based methods to learn a partially-observable Markov decision process that simultaneously optimizes lung cancer detection while enhancing test specificity. Using NLST data, we trained a dynamic Bayesian network as an observational model and used inverse reinforcement learning to discover a rewards function based on experts' decisions. Our resultant predictive model decreased the false positive rate while maintaining a high true positive rate at a level comparable to human experts. Our model also detected a number of lung cancers earlier.
AD  - Univ Calif Los Angeles, Bioengn Dept, Los Angeles, CA 90095 USAAD  - Univ Calif Los Angeles, Med & Imaging Informat, Dept Radiol Sci, Los Angeles, CA 90095 USAC3  - University of California SystemC3  - University of California Los AngelesC3  - University of California SystemC3  - University of California Los AngelesFU  - NIH/NCI [R01 CA226079, R01 CA210360]; NSF [1722516]; UCLA Radiological Sciences Data Driven Diagnostic Decision Support (D4S) Program
FX  - This work was supported in part by the NIH/NCI R01 CA226079, the NIH/NCI R01 CA210360, the NSF #1722516, and in part by the internal support from the UCLA Radiological Sciences Data Driven Diagnostic Decision Support (D4S) Program.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
J9  - IEEE ACCESS
JI  - IEEE Access
PY  - 2019
VL  - 7
SP  - 119403
EP  - 119419
DO  - 10.1109/ACCESS.2019.2935763
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000498555700003
N1  - Times Cited in Web of Science Core Collection:  20
Total Times Cited:  22
Cited Reference Count:  38
ER  -

TY  - JOUR
AU  - Li, T
TI  - Optimizing the configuration of deep learning models for music genre classification
T2  - HELIYON
LA  - English
KW  - Deep reinforcement learning
KW  - Convolutional neural network
KW  - Signal processing
KW  - Music genre classification
AB  - Music genre categorization is a fundamental use of sound processing methods in the realm of music retrieval. Typically, people are responsible for categorizing music genres. Machine learning approaches can automate this procedure. Therefore, in recent years, several approaches have been suggested to achieve this objective. Nevertheless, the given findings indicate that there is still a discrepancy between the observed results and an optimal categorization method. Hence, this paper introduces a novel approach for accurately forecasting music genres by using deep learning methodologies. The proposed approach involves preprocessing the input signals and then representing the characteristics of each signal using a combination of Mel Frequency Cepstral Coefficients (MFCC) and Short-Time Fourier Transform (STFT) features. Subsequently, a convolutional neural network (CNN) is applied to process each group of these characteristics. The proposed technique utilizes two CNN models to analyze MFCC and STFT data. Although the structure of these models is identical, the hyper-parameters of each model are individually adjusted using the black hole optimization (BHO) algorithm. Here, the optimization method finetunes the hyperparameters of each CNN model to minimize their training error. Ultimately, the results of two Convolutional Neural Network (CNN) models are combined to determine the music genre using a classifier based on SoftMax. The efficacy of the suggested methodology in categorizing music genres has been assessed using the GTZAN and Extended-Ballroom datasets. The experimental findings demonstrated that the suggested approach achieved classification accuracies of 95.2 % and 95.7 % in the two datasets, respectively, indicating its superiority over earlier efforts.
AD  - Pingdingshan Polytenchn Coll, Acad Arts, Pingdingshan 467000, Henan, Peoples R ChinaPU  - CELL PRESS
PI  - CAMBRIDGE
PA  - 50 HAMPSHIRE ST, FLOOR 5, CAMBRIDGE, MA 02139 USA
SN  - 2405-8440
J9  - HELIYON
JI  - Heliyon
DA  - JAN 30
PY  - 2024
VL  - 10
IS  - 2
C7  - e24892
DO  - 10.1016/j.heliyon.2024.e24892
C6  - JAN 2024
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001174963900001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  28
ER  -

TY  - JOUR
AU  - Nguyen, T
AU  - Pham, TX
AU  - Zhang, C
AU  - Luu, TM
AU  - Vu, T
AU  - Yoo, CD
TI  - DimCL: Dimensional Contrastive Learning for Improving Self-Supervised Learning
T2  - IEEE ACCESS
LA  - English
KW  - Transfer learning
KW  - Task analysis
KW  - Self-supervised learning
KW  - Learning systems
KW  - Loss measurement
KW  - Computer vision
KW  - Self-supervise learning
KW  - computer vision
KW  - contrastive learning
KW  - deep learning
KW  - transfer learning
AB  - Self-supervised learning (SSL) has gained remarkable success, for which contrastive learning (CL) plays a key role. However, the recent development of new non-CL frameworks has achieved comparable or better performance with high improvement potential, prompting researchers to enhance these frameworks further. Assimilating CL into non-CL frameworks has been thought to be beneficial, but empirical evidence indicates no visible improvements. In view of that, this paper proposes a strategy of performing CL along the dimensional direction instead of along the batch direction as done in conventional contrastive learning, named Dimensional Contrastive Learning (DimCL). DimCL aims to enhance the feature diversity, and it can serve as a regularizer to prior SSL frameworks. DimCL has been found to be effective, and the hardness-aware property is identified as a critical reason for its success. Extensive experimental results reveal that assimilating DimCL into SSL frameworks leads to performance improvement by a non-trivial margin on various datasets and backbone architectures.
AD  - Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South KoreaC3  - Korea Advanced Institute of Science & Technology (KAIST)FU  - Institute for Information and Communications Technology Promotion (IITP) Grant - Korea Government [Ministry of Science and ICT (MSIT)] through the Development of Causal Artificial Intelligence (AI) through Video Understanding and Reinforcement Learning, an [2021-0-01381]; IITP Grant - Korea Government (MSIT) through the Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics [2022-0-00184]
FX  - This work was supported in part by the Institute for Information and Communications Technology Promotion (IITP) Grant funded by the Korea Government [Ministry of Science and ICT (MSIT)] through the Development of Causal Artificial Intelligence (AI) through Video Understanding and Reinforcement Learning, and Its Applications to Real Environments under Grant 2021-0-01381; and in part by the IITP Grant funded by the Korea Government (MSIT) through the Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics under Grant 2022-0-00184.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
J9  - IEEE ACCESS
JI  - IEEE Access
PY  - 2023
VL  - 11
SP  - 21534
EP  - 21545
DO  - 10.1109/ACCESS.2023.3236087
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000946224900001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  73
ER  -

TY  - JOUR
AU  - Ariza-Colpas, PP
AU  - Vicario, E
AU  - Oviedo-Carrascal, AI
AU  - Aziz, SB
AU  - Piñeres-Melo, MA
AU  - Quintero-Linero, A
AU  - Patara, F
TI  - Human Activity Recognition Data Analysis: History, Evolutions, and New Trends
T2  - SENSORS
LA  - English
KW  - ambient assisted living-AAL
KW  - human activity recognition-HAR
KW  - activities of daily living-ADL
KW  - activity recognition systems-ARS
KW  - clustering
KW  - unsupervised activity recognition
KW  - supervised learning
KW  - unsupervised learning
KW  - ensemble learning
KW  - deep learning
KW  - reinforcement learning
KW  - LEARNING ALGORITHMS
KW  - NEURAL-NETWORK
KW  - PHYSICAL-ACTIVITY
KW  - FEATURE-SELECTION
KW  - WEARABLE SENSORS
KW  - CLASSIFICATION
KW  - INTELLIGENCE
KW  - NEIGHBOR
KW  - MOBILE
AB  - The Assisted Living Environments Research Area-AAL (Ambient Assisted Living), focuses on generating innovative technology, products, and services to assist, medical care and rehabilitation to older adults, to increase the time in which these people can live. independently, whether they suffer from neurodegenerative diseases or some disability. This important area is responsible for the development of activity recognition systems-ARS (Activity Recognition Systems), which is a valuable tool when it comes to identifying the type of activity carried out by older adults, to provide them with assistance. that allows you to carry out your daily activities with complete normality. This article aims to show the review of the literature and the evolution of the different techniques for processing this type of data from supervised, unsupervised, ensembled learning, deep learning, reinforcement learning, transfer learning, and metaheuristics approach applied to this sector of science. health, showing the metrics of recent experiments for researchers in this area of knowledge. As a result of this article, it can be identified that models based on reinforcement or transfer learning constitute a good line of work for the processing and analysis of human recognition activities.
AD  - Univ Costa CUC, Dept Comp Sci & Elect, Barranquilla 080002, ColombiaAD  - Univ Pontificia Bolivariana, Fac Engn Informat & Commun Technol, Medellin 050031, ColombiaAD  - Univ Florence, Dept Informat Engn, I-50139 Florence, ItalyAD  - Univ Lahore, Dept Comp Sci & IT, Lahore 44000, PakistanAD  - Univ Norte, Dept Syst Engn, Barranquilla 081001, ColombiaAD  - Univ Popular Cesar, Microbiol Program, Valledupar 200002, ColombiaC3  - Universidad de la CostaC3  - Universidad Pontificia BolivarianaC3  - University of FlorenceC3  - University of LahoreC3  - Universidad del Norte ColombiaFU  - European Union's Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant [734355]
FX  - European Union's Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement No. 734355.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 1424-8220
J9  - SENSORS-BASEL
JI  - Sensors
DA  - MAY
PY  - 2022
VL  - 22
IS  - 9
C7  - 3401
DO  - 10.3390/s22093401
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)AN  - WOS:000796158000001
N1  - Times Cited in Web of Science Core Collection:  13
Total Times Cited:  14
Cited Reference Count:  189
ER  -

TY  - JOUR
AU  - Shu, Y
AU  - Cao, ZJ
AU  - Gao, JH
AU  - Wang, JM
AU  - Yu, PS
AU  - Long, MS
TI  - Omni-Training: Bridging Pre-Training and Meta-Training for Few-Shot Learning
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA  - English
KW  - Task analysis
KW  - Training
KW  - Adaptation models
KW  - Data models
KW  - Feature extraction
KW  - Deep learning
KW  - Bridges
KW  - Few-shot learning
KW  - data efficiency
KW  - transferability
KW  - meta-learning
KW  - pre-training
AB  - Few-shot learning aims to fast adapt a deep model from a few examples. While pre-training and meta-training can create deep models powerful for few-shot generalization, we find that pre-training and meta-training focus respectively on cross-domain transferability and cross-task transferability, which restricts their data efficiency in the entangled settings of domain shift and task shift. We thus propose the Omni-Training framework to seamlessly bridge pre-training and meta-training for data-efficient few-shot learning. Our first contribution is a tri-flow Omni-Net architecture. Besides the joint representation flow, Omni-Net introduces two parallel flows for pre-training and meta-training, responsible for improving domain transferability and task transferability respectively. Omni-Net further coordinates the parallel flows by routing their representations via the joint-flow, enabling knowledge transfer across flows. Our second contribution is the Omni-Loss, which introduces a self-distillation strategy separately on the pre-training and meta-training objectives for boosting knowledge transfer throughout different training stages. Omni-Training is a general framework to accommodate many existing algorithms. Evaluations justify that our single framework consistently and clearly outperforms the individual state-of-the-art methods on both cross-task and cross-domain settings in a variety of classification, regression and reinforcement learning problems.
AD  - Tsinghua Univ, Sch Software, BNRist, Beijing 100084, Peoples R ChinaC3  - Tsinghua UniversityFU  - National Megaproject for New Generation AI [2020AAA0109201]; National Natural Science Foundation of China [62022050, 62021002]; Beijing Nova Program [Z201100006820041]
FX  - This work was supported in part by the National Megaproject for New Generation AI under Grant 2020AAA0109201, in part by the National Natural Science Foundation of China under Grants 62022050 and 62021002, and in part by the Beijing Nova Program under Grant Z201100006820041.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
J9  - IEEE T PATTERN ANAL
JI  - IEEE Trans. Pattern Anal. Mach. Intell.
DA  - DEC
PY  - 2023
VL  - 45
IS  - 12
SP  - 15275
EP  - 15291
DO  - 10.1109/TPAMI.2023.3319517
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001104973300076
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  99
ER  -

TY  - JOUR
AU  - Xu, X
AU  - Xu, Z
AU  - Zhang, R
AU  - Chai, SJ
AU  - Li, JY
TI  - Data-driven-based dynamic pricing method for sharing rooftop photovoltaic energy in a single apartment building
T2  - IET GENERATION TRANSMISSION & DISTRIBUTION
LA  - English
KW  - photovoltaic power systems
KW  - power markets
KW  - building integrated photovoltaics
KW  - pricing
KW  - building management systems
KW  - neural nets
KW  - energy management systems
KW  - learning (artificial intelligence)
KW  - data-driven-based dynamic pricing method
KW  - rooftop photovoltaic energy
KW  - single apartment building
KW  - novel data-driven
KW  - dynamic pricing framework
KW  - load data
KW  - electricity price data
KW  - PV power data
KW  - local PV generations
KW  - building energy management system operator
KW  - internal uniform prices
KW  - rooftop PV productions
KW  - local PV energy
KW  - apartment building users
KW  - uncertain rooftop PV generations
KW  - short-term memory network
KW  - PV predictions
KW  - model-free reinforcement learning method
KW  - near-optimal dynamic pricing strategy
KW  - dynamic pricing strategies
KW  - MANAGEMENT
KW  - ALGORITHM
AB  - In this study, a novel data-driven based dynamic pricing framework is proposed for sharing rooftop photovoltaic (PV) energy in a single apartment building. In this framework, the input includes the load data, electricity price data, and PV power data and the output includes the pricing strategy for local PV generations. Specifically, the building energy management system operator is responsible for setting internal uniform prices of their own rooftop PV productions to facilitate the local PV energy sharing with apartment building users, aiming to maximise the economic profits. To protect the privacy of apartment building users and meanwhile improve the computational efficiency, a neural network is designed for simulating their demand response (DR) behaviours. Besides, the uncertain rooftop PV generations can be duly addressed by using a well-trained long short-term memory network, which can capture the future trends of rooftop PV generations in a rolling-horizon manner. With the information on PV predictions and DR results, a model-free reinforcement learning method is developed for finding the near-optimal dynamic pricing strategy. The simulation results verify the effectiveness of the proposed framework in making dynamic pricing strategies with partial or uncertain information.
AD  - Hong Kong Polytech Univ, Dept Elect Engn, Hong Kong, Peoples R ChinaAD  - Univ New South Wales, Sch Elect Engn & Telecommun, Sydney, NSW, AustraliaAD  - Hunan Univ, Coll Elect & Informat Engn, Changsha, Peoples R ChinaC3  - Hong Kong Polytechnic UniversityC3  - University of New South Wales SydneyC3  - Hunan UniversityFU  - National Natural Science Foundation of China [71971183, 51907056]
FX  - This work was partially supported by the National Natural Science Foundation of China (grant nos. 71971183 and 51907056).
PU  - INST ENGINEERING TECHNOLOGY-IET
PI  - HERTFORD
PA  - MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND
SN  - 1751-8687
SN  - 1751-8695
J9  - IET GENER TRANSM DIS
JI  - IET Gener. Transm. Distrib.
DA  - DEC 18
PY  - 2020
VL  - 14
IS  - 24
SP  - 5720
EP  - 5727
DO  - 10.1049/iet-gtd.2020.0606
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000596021500004
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  4
Cited Reference Count:  40
ER  -

TY  - CPAPER
AU  - Tsourdinis, T
AU  - Makris, N
AU  - Fdida, S
AU  - Korakis, T
ED  - Bernardos, CJ
ED  - Martini, B
ED  - Rojas, E
ED  - Verdi, FL
ED  - Zhu, Z
ED  - Oki, E
ED  - Parzyjegla, H
TI  - DRL-based Service Migration for MEC Cloud-Native 5G and beyond Networks
T2  - 2023 IEEE 9TH INTERNATIONAL CONFERENCE ON NETWORK SOFTWARIZATION, NETSOFT
LA  - English
CP  - 9th IEEE International Conference on Network Softwarization (IEEE NetSoft) - Boosting Future Networks through Advanced Softwarization
KW  - Multi-access Edge Computing
KW  - Beyond 5G
KW  - Cloud-Native network
KW  - AI/ML
KW  - OpenAirInterface
KW  - Kubernetes
KW  - VIRTUAL MACHINE
AB  - Multi-access Edge Computing (MEC) has been considered one of the most prominent enablers for low-latency access to services provided over the telecommunications network. Nevertheless, client mobility, as well as external factors which impact the communication channel can severely deteriorate the eventual user-perceived latency times. Such processes can be averted by migrating the provided services to other edges, while the end-user changes their base station association as they move within the serviced region. In this work, we start from an entirely virtualized cloud-native 5G network based on the OpenAirInterface platform and develop our architecture for providing seamless live migration of edge services. On top of this infrastructure, we employ a Deep Reinforcement Learning (DRL) approach that is able to proactively relocate services to new edges, subject to the user's multi-cell latency measurements and the workload status of the servers. We evaluate our scheme in a testbed setup by emulating mobility using realistic mobility patterns and workloads from real-world clusters. Our results denote that our scheme is capable sustain low-latency values for the end users, based on their mobility within the serviced region.
AD  - Univ Thessaly, Dept Elect & Comp Engn, Volos, GreeceAD  - Sorbonne Univ, CNRS, LIP6, Paris, FranceAD  - CERTH, Ctr Res & Technol Hellas, Thessaloniki, GreeceC3  - University of ThessalyC3  - Centre National de la Recherche Scientifique (CNRS)C3  - Sorbonne UniversiteC3  - Centre for Research & Technology HellasFU  - European Horizon 2020 Programme for research, technological development and demonstration [101008468]
FX  - The research leading to these results has received funding from the European Horizon 2020 Programme for research, technological development and demonstration under Grant Agreement Number No 101008468 (H2020 SLICES-SC). The European Union and its agencies are not liable or otherwise responsible for the contents of this document; its content reflects the view of its authors only.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2693-9770
SN  - 979-8-3503-9980-6
J9  - IEEE Confer on Netwo
PY  - 2023
SP  - 62
EP  - 70
DO  - 10.1109/NetSoft57336.2023.10175417
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001032763600008
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  31
ER  -

TY  - JOUR
AU  - Dixit, P
AU  - Silakari, S
TI  - Deep Learning Algorithms for Cybersecurity Applications: A Technological and Status Review
T2  - COMPUTER SCIENCE REVIEW
LA  - English
KW  - Cybersecurity
KW  - Deep learning
KW  - Attack
KW  - Supervised and unsupervised
KW  - SECURITY
KW  - NETWORK
KW  - INTERNET
KW  - ATTACKS
AB  - Cybersecurity mainly prevents the hardware, software, and data present in the system that has an active internet connection from external attacks. Organizations mainly deploy cybersecurity for their databases and systems to prevent it from unauthorized access. Different forms of attacks like phishing, spear-phishing, a drive-by attack, a password attack, denial of service, etc. are responsible for these security problems In this survey, we analyzed and reviewed the usage of deep learning algorithms for Cybersecurity applications. Deep learning which is also known as Deep Neural Networks includes machine learning techniques that enable the network to learn from unsupervised data and solve complex problems. Here, 80 papers from 2014 to 2019 have been used and successfully analyzed. Deep learning approaches such as Convolutional Neural Network (CNN), Auto Encoder (AE), Deep Belief Network (DBN), Recurrent Neural Network (RNN), Generative Adversal Network (GAN) and Deep Reinforcement Learning (DIL) are used to categorize the papers referred. Each specific technique is effectively discussed with its algorithms, platforms, dataset, and potential benefits. The paper related to deep learning with cybersecurity is mainly published in the year 2018 in a large number and 18% of published articles originate from the UK. In addition, the papers are selected from a variety of journals, and 30% of papers used are from the Elsevier journal. From the experimental analysis, it is clear that the deep learning model improved the accuracy, scalability, reliability, and performance of the cybersecurity applications when applied in realtime. (C) 2020 Elsevier Inc. All rights reserved.
AD  - Rajiv Gandhi Proudyogiki Vishwavidyalaya Bhopal M, Dept Comp Sci & Engg, Univ Inst Technol, Bhopal, IndiaC3  - Rajiv Gandhi Technological UniversityPU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 1574-0137
SN  - 1876-7745
J9  - COMPUT SCI REV
JI  - Comput. Sci. Rev.
DA  - FEB
PY  - 2021
VL  - 39
C7  - 100317
DO  - 10.1016/j.cosrev.2020.100317
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000621106100001
N1  - Times Cited in Web of Science Core Collection:  69
Total Times Cited:  73
Cited Reference Count:  104
ER  -

TY  - CPAPER
AU  - Tiboni, G
AU  - Protopapa, A
AU  - Tommasi, T
AU  - Averta, G
A1  - IEEE
TI  - Domain Randomization for Robust, Affordable and Effective Closed-loop Control of Soft Robots
T2  - 2023 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, IROS
LA  - English
CP  - IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
KW  - SIM-TO-REAL
KW  - DYNAMIC CONTROL
KW  - MODEL
AB  - Soft robots are gaining popularity thanks to their intrinsic safety to contacts and adaptability. However, the potentially infinite number of Degrees of Freedom makes their modeling a daunting task, and in many cases only an approximated description is available. This challenge makes reinforcement learning (RL) based approaches inefficient when deployed on a realistic scenario, due to the large domain gap between models and the real platform. In this work, we demonstrate, for the first time, how Domain Randomization (DR) can solve this problem by enhancing RL policies for soft robots with: i) robustness w.r.t. unknown dynamics parameters; ii) reduced training times by exploiting drastically simpler dynamic models for learning; iii) better environment exploration, which can lead to exploitation of environmental constraints for optimal performance. Moreover, we introduce a novel algorithmic extension to previous adaptive domain randomization methods for the automatic inference of dynamics parameters for deformable objects. We provide an extensive evaluation in simulation on four different tasks and two soft robot designs, opening interesting perspectives for future research on Reinforcement Learning for closed-loop soft robot control.
AD  - Politecn Torino, Turin, ItalyC3  - Polytechnic University of TurinFU  - Politecnico di Torino, CINECA, HPC@POLITO; European Union Next-GenerationEU (PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR) - MISSIONE 4 COMPONENTE 2, INVESTIMENTO 1.3 [1555, PE00000013]
FX  - This work was supported by Politecnico di Torino, CINECA, HPC@POLITO.This study was carried out within the project FAIR - Future Artificial Intelligence Research - and received funding from the European Union Next-GenerationEU (PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR) - MISSIONE 4 COMPONENTE 2, INVESTIMENTO 1.3 - D.D. 1555 11/10/2022, PE00000013). This manuscript reflects only the authors' views and opinions, neither the European Union nor the European Commission can be considered responsible for them.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2153-0858
SN  - 978-1-6654-9190-7
J9  - IEEE INT C INT ROBOT
PY  - 2023
SP  - 612
EP  - 619
DO  - 10.1109/IROS55552.2023.10342537
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001133658800062
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  36
ER  -

TY  - JOUR
AU  - Lourenço, I
AU  - Mattila, R
AU  - Ventura, R
AU  - Wahlberg, B
TI  - A Biologically Inspired Computational Model of Time Perception
T2  - IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS
LA  - English
KW  - Timing
KW  - Biology
KW  - Task analysis
KW  - Decision making
KW  - Animals
KW  - Reinforcement learning
KW  - Robot sensing systems
KW  - Cognitive modeling
KW  - microstimuli
KW  - reinforcement learning
KW  - robotics
KW  - time perception
KW  - DOPAMINE
AB  - Time perception-how humans and animals perceive the passage of time-forms the basis for important cognitive skills, such as decision making, planning, and communication. In this work, we propose a framework for examining the mechanisms responsible for time perception. We first model neural time perception as a combination of two known timing sources: internal neuronal mechanisms and external (environmental) stimuli, and design a decision-making framework to replicate them. We then implement this framework in a simulated robot. We measure the robot's success on a temporal discrimination task originally performed by mice to evaluate their capacity to exploit temporal knowledge. We conclude that the robot is able to perceive time similarly to animals when it comes to their intrinsic mechanisms of interpreting time and performing time-aware actions. Next, by analyzing the behavior of agents equipped with the framework, we propose an estimator to infer characteristics of the timing mechanisms intrinsic to the agents. In particular, we show that from their empirical action probability distribution, we are able to estimate parameters used for perceiving time. Overall, our work shows promising results when it comes to drawing conclusions regarding some of the characteristics present in biological timing mechanisms.
AD  - KTH Royal Inst Technol, Div Decis & Control Syst, S-10044 Stockholm, SwedenAD  - Inst Super Tecn, Inst Syst & Robot, P-1049001 Lisbon, PortugalC3  - Royal Institute of TechnologyC3  - Universidade de CoimbraC3  - Universidade de LisboaFU  - NewLEADS; Swedish Research Council; Wallenberg AI, Autonomous Systems and Software Program (WASP); FCT [UID/EEA/50009/2019]
FX  - This work was supported in part by NewLEADS; in part by the Swedish Research Council; in part by the Wallenberg AI, Autonomous Systems and Software Program (WASP); and in part by the FCT Project under Grant UID/EEA/50009/2019.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2379-8920
SN  - 2379-8939
J9  - IEEE T COGN DEV SYST
JI  - IEEE Trans. Cogn. Dev. Syst.
DA  - JUN
PY  - 2022
VL  - 14
IS  - 2
SP  - 258
EP  - 268
DO  - 10.1109/TCDS.2021.3120301
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000809402600006
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  48
ER  -

TY  - CPAPER
AU  - de Lima, T
AU  - Lorini, E
AU  - Schwarzentruber, F
ED  - Gaggl, S
ED  - Martinez, MV
ED  - Ortiz, M
TI  - Base-Based Model Checking for Multi-agent only Believing
T2  - LOGICS IN ARTIFICIAL INTELLIGENCE, JELIA 2023
LA  - English
CP  - 18th European Conference on Logics in Artificial Intelligence (JELIA)
AB  - We present a novel semantics for the language of multi-agent only believing exploiting belief bases, and show how to use it for automatically checking formulas of this language. We provide a PSPACE algorithm for model checking relying on a reduction to QBF, an implementation and some experimental results on computation time in a concrete example.
AD  - Univ Artois, CRIL, Lens, FranceAD  - CNRS, Lens, FranceAD  - Univ Toulouse, CNRS, IRIT, Toulouse, FranceAD  - ENS Rennes, Bruz, FranceC3  - Universite d'ArtoisC3  - Centre National de la Recherche Scientifique (CNRS)C3  - Universite Federale Toulouse Midi-Pyrenees (ComUE)C3  - Universite de ToulouseC3  - Institut National Polytechnique de ToulouseC3  - Universite Toulouse III - Paul SabatierC3  - Centre National de la Recherche Scientifique (CNRS)C3  - Ecole Normale Superieure de Rennes (ENS Rennes)FU  - French National Agency of Research [ANR-22-CE23-0029, ANR-18-CE33-0012, ANR-19-CHIA-0008]; Natural Intelligence Toulouse Institute (ANITI); Agence Nationale de la Recherche (ANR) [ANR-19-CHIA-0008, ANR-18-CE33-0012] Funding Source: Agence Nationale de la Recherche (ANR)
FX  - This work is partially supported by the project epiRL ("Epistemic Reinforcement Learning") ANR-22-CE23-0029, the project CoPains ("Cognitive Planning in Persuasive Multimodal Communication") ANR-18-CE33-0012 and the AI Chair project Responsible AI (ANR-19-CHIA-0008) both from the French National Agency of Research. Support from the Natural Intelligence Toulouse Institute (ANITI) is also gratefully acknowledged.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 2945-9133
SN  - 1611-3349
SN  - 978-3-031-43618-5
SN  - 978-3-031-43619-2
J9  - LECT NOTES ARTIF INT
PY  - 2023
VL  - 14281
SP  - 437
EP  - 445
DO  - 10.1007/978-3-031-43619-2_30
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001157340700030
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  21
ER  -

TY  - JOUR
AU  - Gutierrez-Franco, E
AU  - Mejia-Argueta, C
AU  - Rabelo, L
TI  - Data-Driven Methodology to Support Long-Lasting Logistics and Decision Making for Urban Last-Mile Operations
T2  - SUSTAINABILITY
LA  - English
KW  - urban logistics
KW  - emerging markets
KW  - nanostores
KW  - customer-centric supply chains
KW  - hybrid methods
KW  - prescriptive analytics
KW  - framework
KW  - digital twin
KW  - VEHICLE-ROUTING PROBLEM
KW  - REVERSE LOGISTICS
KW  - E-COMMERCE
KW  - FREIGHT TRANSPORT
KW  - DELIVERY
KW  - OPTIMIZATION
KW  - ALGORITHMS
KW  - FRAMEWORK
KW  - LOCATION
KW  - DESIGN
AB  - Last-mile operations in forward and reverse logistics are responsible for a large part of the costs, emissions, and times in supply chains. These operations have increased due to the growth of electronic commerce and direct-to-consumer strategies. We propose a novel data- and model-driven framework to support decision making for urban distribution. The methodology is composed of diverse, hybrid, and complementary techniques integrated by a decision support system. This approach focuses on key elements of megacities such as socio-demographic diversity, portfolio mix, logistics fragmentation, high congestion factors, and dense commercial areas. The methodological framework will allow decision makers to create early warning systems and, with the implementation of optimization, machine learning, and simulation models together, make the best utilization of resources. The advantages of the system include flexibility in decision making, social welfare, increased productivity, and reductions in cost and environmental impacts. A real-world illustrative example is presented under conditions in one of the most congested cities: the megacity of Bogota, Colombia. Data come from a retail organization operating in the city. A network of stakeholders is analyzed to understand the complex urban distribution. The execution of the methodology was capable of solving a complex problem reducing the number of vehicles utilized, increasing the resource capacity utilization, and reducing the cost of operations of the fleet, meeting all constraints. These constraints included the window of operations and accomplishing the total number of deliveries. Furthermore, the methodology could accomplish the learning function using deep reinforcement learning in reasonable computational times. This preliminary analysis shows the potential benefits, especially in understudied metropolitan areas from emerging markets, supporting a more effective delivery process, and encouraging proactive, dynamic decision making during the execution stage.
AD  - MIT, Ctr Latin American Logist Innovat, Global SCALE Network, Cambridge, MA 02139 USAAD  - Univ Cent Florida, Dept Ind Engn & Management Syst, Orlando, FL 16299 USAAD  - MIT, Food & Retail Operat Lab, Ctr Transportat & Logist, Cambridge, MA 02139 USAC3  - Massachusetts Institute of Technology (MIT)C3  - State University System of FloridaC3  - University of Central FloridaC3  - Massachusetts Institute of Technology (MIT)FU  - National Science Foundation [2012228]; Fulbright Colombia; Dir for Tech, Innovation, & Partnerships; Translational Impacts [2012228] Funding Source: National Science Foundation
FX  - This material is partially based upon work supported by the National Science Foundation under award no. 2012228 and Scholarship (E.G) Fulbright Colombia.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2071-1050
J9  - SUSTAINABILITY-BASEL
JI  - Sustainability
DA  - JUN
PY  - 2021
VL  - 13
IS  - 11
C7  - 6230
DO  - 10.3390/su13116230
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)AN  - WOS:000660763000001
N1  - Times Cited in Web of Science Core Collection:  22
Total Times Cited:  22
Cited Reference Count:  111
ER  -

TY  - JOUR
AU  - Chen, B
AU  - Zhang, J
AU  - Tang, J
AU  - Cai, LF
AU  - Wang, ZY
AU  - Zhao, S
AU  - Chen, H
AU  - Li, CP
TI  - CONNA: Addressing Name Disambiguation on the Fly
T2  - IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING
LA  - English
KW  - Name disambiguation
KW  - joint model
KW  - multi-field multi-instance
AB  - Name disambiguation is a key and also a very tough problem in many online systems such as social search and academic search. Despite considerable research, a critical issue that has not been systematically studied is disambiguation on the fly - to complete the disambiguation in the real-time. This is very challenging, as the disambiguation algorithm must be accurate, efficient, and error tolerance. In this paper, we propose a novel framework - CONNA - to train a matching component and a decision component jointly via reinforcement learning. The matching component is responsible for finding the top matched candidate for the given paper, and the decision component is responsible for deciding on assigning the top matched person or creating a new person. The two components are intertwined and can be bootstrapped via jointly training. Empirically, we evaluate CONNA on two name disambiguation datasets. Experimental results show that the proposed framework can achieve a 1.21-19.84 percent improvement on F1-score using joint training of the matching and the decision components. The proposed CONNA has been successfully deployed on AMiner - a large online academic search system.
AD  - Zhipu AI Co Ltd, Beijing 100084, Peoples R ChinaAD  - Renmin Univ China, Informat Sch, Beijing 100872, Peoples R ChinaAD  - Tsinghua Univ, Tsinghua Bosch Joint ML Ctr, Dept Comp Sci & Technol, Beijing 100084, Peoples R ChinaAD  - Tsinghua Natl Lab Informat Sci & Technol TNList, Beijing 100084, Peoples R ChinaAD  - Anhui Univ, Sch Comp Sci & Technol, Hefei 230039, Anhui, Peoples R ChinaC3  - Renmin University of ChinaC3  - Tsinghua UniversityC3  - Tsinghua UniversityC3  - Anhui UniversityFU  - National Key R&D Program of China [2018YFB1004401]; NSFC [61532021, 61772537, 61772536, 61702522]; Tsinghua-Bosch Joint ML Center
FX  - This work was supported by the National Key R&D Program of China (No.2018YFB1004401) and NSFC (No.61532021, 61772537, 61772536, 61702522), and Tsinghua-Bosch Joint ML Center. Part of this work has been done when Bo Chen was studying at Renmin University of China.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 1041-4347
SN  - 1558-2191
J9  - IEEE T KNOWL DATA EN
JI  - IEEE Trans. Knowl. Data Eng.
DA  - JUL 1
PY  - 2022
VL  - 34
IS  - 7
SP  - 3139
EP  - 3152
DO  - 10.1109/TKDE.2020.3021256
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000805787100009
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  50
ER  -

TY  - CPAPER
AU  - Risi, S
AU  - Stanley, KO
ED  - LopezIbanez, M
TI  - Deep Neuroevolution of Recurrent and Discrete World Models
T2  - PROCEEDINGS OF THE 2019 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'19)
LA  - English
CP  - Genetic and Evolutionary Computation Conference (GECCO)
AB  - Neural architectures inspired by our own human cognitive system, such as the recently introduced world models, have been shown to outperform traditional deep reinforcement learning (RL) methods in a variety of different domains. Instead of the relatively simple architectures employed in most RL experiments, world models rely on multiple different neural components that are responsible for visual information processing, memory, and decision-making. However, so far the components of these models have to be trained separately and through a variety of specialized training methods. This paper demonstrates the surprising finding that models with the same precise parts can be instead efficiently trained end-to-end through a genetic algorithm (GA), reaching a comparable performance to the original world model by solving a challenging car racing task. An analysis of the evolved visual and memory system indicates that they include a similar effective representation to the system trained through gradient descent. Additionally, in contrast to gradient descent methods that struggle with discrete variables, GAs also work directly with such representations, opening up opportunities for classical planning in latent space. This paper adds additional evidence on the effectiveness of deep neuroevolution for tasks that require the intricate orchestration of multiple components in complex heterogeneous architectures.
AD  - Uber AI, San Francisco, CA 94103 USAPU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-6111-8
PY  - 2019
SP  - 456
EP  - 462
DO  - 10.1145/3321707.3321817
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000523218400055
N1  - Times Cited in Web of Science Core Collection:  26
Total Times Cited:  29
Cited Reference Count:  49
ER  -

TY  - JOUR
AU  - Clempner, JB
TI  - Learning machiavellian strategies for manipulation in Stackelberg security games
T2  - ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE
LA  - English
KW  - City safety
KW  - Manipulation
KW  - Patrolling
KW  - Security games
KW  - Strong Stackelberg equilibrium
KW  - EXTRAPROXIMAL METHOD
KW  - CONVERGENCE ANALYSIS
KW  - NASH EQUILIBRIUM
KW  - LYAPUNOV
AB  - This paper suggests a new approach for repeated Stackelberg security games (SSGs) based on manipulation. Manipulation is a strategy interpreted by the Machiavellianism social behavior theory, which consists on three main concepts: view, tactics, and immorality. The world is conceptualized by manipulators and manipulated (view). Players employ Machiavelli's tactics and Machiavellian intelligence in order to manipulate attacker/defender situations. The immorality plays a fundamental role in these games, defenders are able to not be attached to a conventional moral in order to achieve their goals. We consider a security game model involving manipulating defenders and manipulated attackers engaged cooperatively in a Nash game and at the same time restricted by a Stackelberg game. The resulting game is non-cooperative bargaining game. The cooperation is represented by the Nash bargaining solution. We propose an analytical formula for solving the manipulation game, which arises as the maximum of the quotient of two Nash products. The role of the players in the Stackelberg security game are determined by the weights of the players for the Nash bargaining approach. We consider only a subgame perfect equilibrium where the solution of the manipulation game is a Strong Stackelberg Equilibrium (SSE). We employ a reinforcement learning (RL) approach for the implementation of the immorality. A numerical example related to developing a strategic schedule for the efficient use of resources for patrolling in a smart city is handled using a class of homogeneous, ergodic, controllable, and finite Markov chains for showing the usefulness of the method for security resource allocation.
AD  - Natl Polytech Inst, Sch Phys & Math, Bldg 9,Av Inst Politecn Nacl, Mexico City 07738, DF, MexicoC3  - Instituto Politecnico Nacional - MexicoPU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1012-2443
SN  - 1573-7470
J9  - ANN MATH ARTIF INTEL
JI  - Ann. Math. Artif. Intell.
DA  - APR
PY  - 2022
VL  - 90
IS  - 4
SP  - 373
EP  - 395
DO  - 10.1007/s10472-022-09788-0
C6  - FEB 2022
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000754335300001
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  51
ER  -

TY  - JOUR
AU  - Nazeer, MS
AU  - Laschi, C
AU  - Falotico, E
TI  - Soft DAgger: Sample-Efficient Imitation Learning for Control of Soft Robots
T2  - SENSORS
LA  - English
KW  - DAgger algorithm
KW  - dynamic behavioral mapping
KW  - imitation learning
KW  - online optimization
KW  - Soft DAgger
KW  - soft robotics
KW  - soft robots control
KW  - LOOP DYNAMIC CONTROL
KW  - KINEMATIC CONTROL
KW  - CONTINUUM ROBOTS
KW  - MANIPULATORS
KW  - DESIGN
AB  - This paper presents Soft DAgger, an efficient imitation learning-based approach for training control solutions for soft robots. To demonstrate the effectiveness of the proposed algorithm, we implement it on a two-module soft robotic arm involved in the task of writing letters in 3D space. Soft DAgger uses a dynamic behavioral map of the soft robot, which maps the robot's task space to its actuation space. The map acts as a teacher and is responsible for predicting the optimal actions for the soft robot based on its previous state action history, expert demonstrations, and current position. This algorithm achieves generalization ability without depending on costly exploration techniques or reinforcement learning-based synthetic agents. We propose two variants of the control algorithm and demonstrate that good generalization capabilities and improved task reproducibility can be achieved, along with a consistent decrease in the optimization time and samples. Overall, Soft DAgger provides a practical control solution to perform complex tasks in fewer samples with soft robots. To the best of our knowledge, our study is an initial exploration of imitation learning with online optimization for soft robot control.
AD  - Scuola Super Sant Anna, BioRobot Inst, I-56025 Pontedera, ItalyAD  - Scuola Super Sant Anna, Dept Excellence Robot & AI, I-56125 Pisa, ItalyAD  - Natl Univ Singapore, Dept Mech Engn, Singapore 117575, SingaporeC3  - Scuola Superiore Sant'AnnaC3  - Scuola Superiore Sant'AnnaC3  - National University of SingaporeFU  - This work was conducted at the BRain-Inspired Robotics Lab (BRAIR Lab) of The BioRobotics Institute in collaboration with The Soft Robotics Lab of National University of Singapore. Further acknowledgments are extended to the researchers within the Biorobot
FX  - This work was conducted at the BRain-Inspired Robotics Lab (BRAIR Lab) of The BioRobotics Institute in collaboration with The Soft Robotics Lab of National University of Singapore. Further acknowledgments are extended to the researchers within the Biorobotics Institute, Pontedera, Italy and the BRAIR Lab, Pontedera, Italy for helping collect expert demonstrations.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 1424-8220
J9  - SENSORS-BASEL
JI  - Sensors
DA  - OCT
PY  - 2023
VL  - 23
IS  - 19
C7  - 8278
DO  - 10.3390/s23198278
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001086221800001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  59
ER  -

TY  - CPAPER
AU  - Galimzhanova, E
AU  - Muntean, CI
AU  - Nardini, FM
AU  - Perego, R
AU  - Rocchietti, G
A1  - IEEE
TI  - Rewriting Conversational Utterances with Instructed Large Language Models
T2  - 2023 IEEE INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE AND INTELLIGENT AGENT TECHNOLOGY, WI-IAT
LA  - English
CP  - 22nd IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)
KW  - conversational systems
KW  - query rewriting
KW  - LLMs
KW  - ChatGPT
KW  - information retrieval
AB  - Many recent studies have shown the ability of large language models (LLMs) to achieve state-of-the-art performance on many NLP tasks, such as question answering, text summarization, coding, and translation. In some cases, the results provided by LLMs are on par with those of human experts. These models' most disruptive innovation is their ability to perform tasks via zero-shot or few-shot prompting. This capability has been successfully exploited to train instructed LLMs, where reinforcement learning with human feedback is used to guide the model to follow the user's requests directly. In this paper, we investigate the ability of instructed LLMs to improve conversational search effectiveness by rewriting user questions in a conversational setting. We study which prompts provide the most informative rewritten utterances that lead to the best retrieval performance. Reproducible experiments are conducted on publicly-available TREC CAST datasets. The results show that rewriting conversational utterances with instructed LLMs achieves significant improvements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and 11.5% in Recall@500 over state-of-the-art techniques.
AD  - Univ Pisa, Pisa, ItalyAD  - ISTI CNR, Pisa, ItalyC3  - University of PisaC3  - Consiglio Nazionale delle Ricerche (CNR)C3  - Istituto di Scienza e Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR)FU  - European Union (EU) under the NextGeneration EU programme [PE00000013]; EU's Horizon Europe research and innovation programme EFRA [101093026]; Horizon Europe - Pillar II [101093026] Funding Source: Horizon Europe - Pillar II
FX  - Funding for this research has been provided by: PNRR - M4C2 - Investimento 1.3, Partenariato Esteso PE00000013 - "FAIR - Future Artificial Intelligence Research" Spoke 1 "Human-centered AI" funded by the European Union (EU) under the NextGeneration EU programme; the EU's Horizon Europe research and innovation programme EFRA (Grant Agreement Number 101093026). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the EU or European Commission-EU. Neither the EU nor the granting authority can be held responsible for them.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 979-8-3503-0918-8
PY  - 2023
SP  - 56
EP  - 63
DO  - 10.1109/WI-IAT59888.2023.00014
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001139644800008
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  33
ER  -

TY  - CPAPER
AU  - Bozinis, T
AU  - Passalis, N
AU  - Tefas, A
A1  - IEEE COMP SOC
TI  - Improving Visual Question Answering using Active Perception on Static Images
T2  - 2020 25TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
LA  - English
CP  - 25th International Conference on Pattern Recognition (ICPR)
AB  - Visual Question Answering (VQA) is one of the most challenging emerging applications of deep learning. Providing powerful attention mechanisms is crucial for VQA, since the model must correctly identify the region of an image that is relevant to the question at hand. However, existing models analyze the input images at a fixed and typically small resolution, often leading to discarding valuable fine-grained details. To overcome this limitation, in this work we propose a reinforcement learning-based active perception approach that works by applying a series of transformation operations on the images (translation, zoom) in order to facilitate answering the question at hand. This allows for performing fine-grained analysis, effectively increasing the resolution at which the models process information. The proposed method is orthogonal to existing attention mechanisms and it can be combined with most existing VQA methods. The effectiveness of the proposed method is experimentally demonstrated on a challenging VQA dataset.
AD  - Aristotle Univ Thessaloniki, Fac Sci, Dept Informat, Thessaloniki 54124, GreeceC3  - Aristotle University of ThessalonikiFU  - European Union [871449]
FX  - This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 871449 (OpenDR). This publication reflects the authors' views only. The European Commission is not responsible for any use that may be made of the information it contains.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1051-4651
SN  - 978-1-7281-8808-9
J9  - INT C PATT RECOG
PY  - 2021
SP  - 879
EP  - 884
DO  - 10.1109/ICPR48806.2021.9412885
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000678409200117
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  22
ER  -

TY  - CPAPER
AU  - Nikdel, P
AU  - Vaughan, R
AU  - Chen, M
A1  - IEEE
TI  - LBGP: Learning Based Goal Planning for Autonomous Following in Front
T2  - 2021 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2021)
LA  - English
CP  - IEEE International Conference on Robotics and Automation (ICRA)
KW  - MOBILE ROBOT
KW  - NAVIGATION
KW  - ALGORITHM
KW  - BEHAVIOR
KW  - SYSTEM
AB  - This paper investigates a hybrid solution which combines deep reinforcement learning (RL) and classical trajectory planning 14 the "following in front" application. Here, an autonomous robot aims to stay ahead of a person as the person freely walks around. Following in front is a challenging problem as the user's intended trajectory is unknown and needs to be estimated, explicitly or implicitly, by the robot. In addition, the robot needs to find a feasible way to safely navigate ahead of human trajectory. Our deep RL module makes decisions at a high level by implicitly estimates the human trajectory and produces short-term navigational goals to guide the robot. These goals are used by a trajectory planner, which is responsible for low-level execution, to smoothly navigate the robot to the short-term goals, and eventually in front of the user. We employ curriculum learning in the deep RL module to efficiently achieve a high return. Our system outperforms the state-of-the-art in following ahead and is more reliable compared to end-to-end alternatives in both the simulation and real world experiments. In contrast to a pure deep RL approach, we demonstrate zero-shot transfer of the trained policy from simulation to the real world.
AD  - Simon Fraser Univ, Sch Comp Sci, Burnaby, BC, CanadaC3  - Simon Fraser UniversityFU  - NSERC Discovery Grant
FX  - This work was supported by the NSERC Discovery Grant.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1050-4729
SN  - 2577-087X
SN  - 978-1-7281-9077-8
J9  - IEEE INT CONF ROBOT
PY  - 2021
SP  - 3140
EP  - 3146
DO  - 10.1109/ICRA48506.2021.9560914
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000765738802081
N1  - Times Cited in Web of Science Core Collection:  5
Total Times Cited:  5
Cited Reference Count:  27
ER  -

TY  - JOUR
AU  - Caraglio, M
AU  - Kaur, H
AU  - Fiderer, LJ
AU  - López-Incera, A
AU  - Briegel, HJ
AU  - Franosch, T
AU  - Muñoz-Gil, G
TI  - Learning how to find targets in the micro-world: the case of intermittent active Brownian particles
T2  - SOFT MATTER
LA  - English
AB  - Finding the best strategy to minimize the time needed to find a given target is a crucial task both in nature and in reaching decisive technological advances. By considering learning agents able to switch their dynamics between standard and active Brownian motion, here we focus on developing effective target-search behavioral policies for microswimmers navigating a homogeneous environment and searching for targets of unknown position. We exploit projective simulation, a reinforcement learning algorithm, to acquire an efficient stochastic policy represented by the probability of switching the phase, i.e. the navigation mode, in response to the type and the duration of the current phase. Our findings reveal that the target-search efficiency increases with the particle's self-propulsion during the active phase and that, while the optimal duration of the passive case decreases monotonically with the activity, the optimal duration of the active phase displays a non-monotonic behavior.
   Microswimmers able to switch their dynamics between standard and active Brownian motion can learn how to optimize their odds of finding unknown targets by tuning the probability of switching from the active to the passive phase and vice versa.
AD  - Univ Innsbruck, Inst Theoret Phys, Techn Str 21A, A-6020 Innsbruck, AustriaC3  - University of InnsbruckFU  - Volkswagen Foundation [847476]; European Union's Horizon 2020 research and innovation programme under the Marie Skodowska-Curie [SFB BeyondC F7102]; FWF [Az: 97721]; Volkswagen Foundation [10105529]; European Research Council (ERC, Quant AI); European Union; European Research Council [SFB BeyondC F7102, P 35872-N, P 35580-N]; Austrian Science Fund (FWF)
FX  - H. K. acknowledges funding from the European Union's Horizon 2020 research and innovation programme under the Marie Skodowska-Curie grant agreement No. 847476; M. C. is supported by FWF: P 35872-N; T. F. acknowledges funding by FWF: P 35580-N; A. L. and H. J. B. acknowledge support by the Volkswagen Foundation (Az: 97721); H. J. B. acknowledges funding from FWF through SFB BeyondC F7102, and the European Research Council (ERC, Quant AI, Project No. 10105529). G. M.-G also acknowledges funding from the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union, the European Research Council or the European Research Executive Agency. Neither the European Union nor the granting authorities can be held responsible for them. This research was funded in whole or in part by the Austrian Science Fund (FWF) [P 35872-N; P 35580-N; SFB BeyondC F7102]. For open access purposes, the author has applied a CC BY public copyright license to any author accepted manuscript version arising from this submission.
PU  - ROYAL SOC CHEMISTRY
PI  - CAMBRIDGE
PA  - THOMAS GRAHAM HOUSE, SCIENCE PARK, MILTON RD, CAMBRIDGE CB4 0WF, CAMBS, ENGLAND
SN  - 1744-683X
SN  - 1744-6848
J9  - SOFT MATTER
JI  - Soft Matter
DA  - FEB 28
PY  - 2024
VL  - 20
IS  - 9
SP  - 2008
EP  - 2016
DO  - 10.1039/d3sm01680c
C6  - FEB 2024
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:001157798900001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  71
ER  -

